import os
import json
import uuid
import time
import asyncio
import logging
import torch
import requests
import tiktoken
import numpy as np
import re
import hashlib
from datetime import datetime, timedelta, timezone
from functools import lru_cache
from typing import Optional, Tuple, Dict, List
from pathlib import Path
from threading import Lock
import httpx
import random

import openai
from openai import AsyncOpenAI

from dotenv import load_dotenv
from backend.search_engine import build_web_context
from backend.memory.stwm import update_stwm, get_stwm_snapshot
from backend.memory.turns import (
    add_turn as tb_add_turn,
    maybe_summarize as tb_maybe_summarize,
    get_summaries as tb_get_summaries,
)
from backend.memory.selector import select_summaries
from backend.evidence.builder import build_evidence
from backend.generation.composer import (
    compose_fact_answer,
    apply_style_wrapper,
    ComposeInput,
)
from backend.generation.wrapper import (
    wrap_web_reply,
    wrap_generic_reply,
    wrap_greeting_reply,
)
from backend.rewrite.log import add_rewrite, RewriteRecord
from backend.planner.logger import log_planner, PlannerLog
from backend.policy.state import redact_text

from backend.directives.pipeline import (
    ensure_directive_workers,
    schedule_directive_update,
)
from backend.directives.store import get_compiled as get_compiled_directives

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from backend.rag import (
    retrieve_from_rag,
    ensure_collections,
    embed_query_cached,
    METRIC,
)
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.chat_message_histories.redis import RedisChatMessageHistory
from langchain.memory import ConversationSummaryBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from pymilvus import (
    connections,
    utility,
    Collection,
    CollectionSchema,
    FieldSchema,
    DataType,
    Partition,
)

from transformers import (
    AutoTokenizer as HFTokenizer,
    AutoConfig as HFConfig,
    AutoModelForSequenceClassification as HFForSeq,
)

try:
    # FirestoreëŠ” ì„ íƒì  ì˜ì¡´ì„±. í™˜ê²½/ê¶Œí•œ ì—†ì„ ê²½ìš° None ì²˜ë¦¬
    from google.cloud import firestore as gcf  # type: ignore
except Exception:
    gcf = None

from fastapi import FastAPI, WebSocket, WebSocketDisconnect

# ----------------------------------------------------------------------
# ë¡œê¹… ì„¤ì •
# ----------------------------------------------------------------------
LOG_LEVEL = os.getenv("ROUTER_LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger("router")

# ----------------------------------------------------------------------
# ì…€ 1~3: í™˜ê²½ ë³€ìˆ˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# ----------------------------------------------------------------------
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CLIENT_ID = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
FIRESTORE_ENABLE = bool(int(os.getenv("FIRESTORE_ENABLE", "1")))
TIMEOUT_MOBILE = float(os.getenv("TIMEOUT_MOBILE", "1.0"))
FIRESTORE_USERS_COLL = os.getenv("FIRESTORE_USERS_COLL", "users")
FIRESTORE_EVENTS_SUB = os.getenv("FIRESTORE_EVENTS_SUB", "unified_events")
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://mcp:5000")
DEBUG_META = bool(int(os.getenv("DEBUG_META", "0")))
WS_DEBUG_META = bool(int(os.getenv("WS_DEBUG_META", "0")))

# Firestore ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ ìë™ ì„¤ì • (ê¸°ë³¸: í”„ë¡œì íŠ¸ ë£¨íŠ¸/gcp-service-account-key.json)
try:
    if FIRESTORE_ENABLE:
        default_key_path = os.getenv(
            "GCP_SERVICE_ACCOUNT_PATH",
            str(Path(__file__).resolve().parent / "gcp-service-account-key.json"),
        )
        cur = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        # 1) í˜„ì¬ ì„¤ì •ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ êµì • ì‹œë„
        if cur and not Path(cur).exists() and Path(default_key_path).exists():
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = default_key_path
            logger.info(
                "[fs] GOOGLE_APPLICATION_CREDENTIALS override -> %s", default_key_path
            )
        # 2) ë¯¸ì„¤ì •ì´ë©´ ê¸°ë³¸ ê²½ë¡œë¡œ ì…‹ì—…
        elif (not cur) and Path(default_key_path).exists():
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = default_key_path
            logger.info("[fs] GOOGLE_APPLICATIONS set -> %s", default_key_path)
        elif not Path(default_key_path).exists():
            logger.warning("[fs] service account key not found at %s", default_key_path)
except Exception as _fs_e:
    logger.warning("[fs] failed to set GOOGLE_APPLICATION_CREDENTIALS: %r", _fs_e)

# ì´ì„± ì¶”ë¡ í˜• ëª¨ë¸(ì—ì´ì „íŠ¸/ê·¸ë˜í”„ìš©)
THINKING_MODEL = os.getenv("THINKING_MODEL", "gpt-5-thinking")

# ===== ì¶”ê°€: ì¬ì‘ì„± íƒ€ì„ì•„ì›ƒ/í† í°/ëª¨ë¸ =====
REWRITE_TIMEOUT_S = float(os.getenv("REWRITE_TIMEOUT_S", "1.25"))
REWRITE_MAX_TOKENS = int(os.getenv("REWRITE_MAX_TOKENS", "128"))
REWRITE_MODEL = os.getenv("REWRITE_MODEL", "gpt-4o-mini")

# ===== ë‹´í™” ì•µì»¤/íŒ”ë¡œì—…/ì„ë² ë”© íŒíŠ¸ ì„¤ì • =====
TOPIC_TTL_S = int(os.getenv("TOPIC_TTL_S", "600"))  # ì•µì»¤ TTL(ì´ˆ)
HINT_LOOKBACK = int(
    os.getenv("HINT_LOOKBACK", "8")
)  # íŒíŠ¸ ì¶”ì¶œ ì‹œ ìµœê·¼ ë°œí™” ê°œìˆ˜ (ê¸°ë³¸ 8ë¡œ ì¶•ì†Œ)
HINT_MAX_ITEMS = int(os.getenv("HINT_MAX_ITEMS", "2"))  # íŒíŠ¸ ìµœëŒ€ ë¼ì¸ ìˆ˜
HINT_SIM_THRESHOLD = float(
    os.getenv("HINT_SIM_THRESHOLD", "0.25")
)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„
EXTRACT_TIMEOUT_S = float(os.getenv("EXTRACT_TIMEOUT_S", "0.7"))  # ì•µì»¤ ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ
FOLLOWUP_TIMEOUT_S = float(
    os.getenv("FOLLOWUP_TIMEOUT_S", "0.7")
)  # íŒ”ë¡œì—… íŒë³„ íƒ€ì„ì•„ì›ƒ

# ===== (ê¸°ì¡´ ìƒë‹¨ ì„¤ì •ë¶€ ì•„ë˜ì— ì¶”ê°€) =====
EMBEDDING_DIM_MAP = {
    "text-embedding-3-small": 1536,
    "text-embedding-3-large": 3072,
    "text-embedding-ada-002": 1536,  # ë ˆê±°ì‹œ ì‚¬ìš© ì‹œ
}
EMBEDDING_DIM = int(
    os.getenv("EMBEDDING_DIM", EMBEDDING_DIM_MAP.get(EMBEDDING_MODEL, 1536))
)

# ì°¨ì› ì„ì„ ë°©ì§€: ì»¬ë ‰ì…˜ ì´ë¦„ v3 (ë‚ ì§œ ë©”íƒ€ ì¶”ê°€)
PROFILE_COLLECTION_NAME = f"user_profiles_v3_{EMBEDDING_DIM}d"
LOG_COLLECTION_NAME = f"conversation_logs_v3_{EMBEDDING_DIM}d"

# ë¼ìš°íŒ… ìŠ¤ë ˆìˆ„ë“œ / ì• ë§¤ êµ¬ê°„ / íƒ€ì„ì•„ì›ƒ
TAU_RAG = float(os.getenv("TAU_RAG", 0.55))
TAU_WEB = float(os.getenv("TAU_WEB", 0.55))
AMBIGUITY_BAND = float(os.getenv("AMBIGUITY_BAND", "0.03"))  # ğŸ”§ ê¸°ë³¸ê°’ 0.03ë¡œ í•˜í–¥
TIMEOUT_RAG = float(os.getenv("TIMEOUT_RAG", 2.5))
TIMEOUT_WEB = float(os.getenv("TIMEOUT_WEB", 2.5))

# ===== ì†Œë¶„ë¥˜ê¸° ë³´ì •(Platt/TS) ë° í”„ë¼ì´ì–´ ê°€ì  =====
CAL_WEB = {"a": 1.0, "b": 0.0, "T": 1.00}
CAL_RAG = {"a": 1.0, "b": 0.0, "T": 1.00}

WEB_PRIOR_PAT = r"(ì¶”ì²œ|ê·¼ì²˜|ê°€ê¹Œìš´|ì˜ì—…ì‹œê°„|ê°€ê²©|ë¦¬ë·°|ë­í‚¹|ë‰´ìŠ¤|ìµœì‹ |ì£¼ì†Œ|ì „í™”)"
RAG_PRIOR_PAT = r"(ë‚´ ë¬¸ì„œ|ì‚¬ë‚´|ì •ì±…|ë‚´ ì¼ì •|í”„ë¡œì íŠ¸|ë…¸íŠ¸|ìš”ì•½í–ˆ|íšŒì˜ë¡|RAG|ë‚´ì •ë³´)"


def _apply_calibration(logit: float, cal: dict) -> float:
    # Platt (ê¸°ë³¸). TSë¥¼ ì“°ë ¤ë©´: return 1.0/(1.0+np.exp(-(logit/cal["T"])))
    return 1.0 / (1.0 + np.exp(-(cal.get("a", 1.0) * logit + cal.get("b", 0.0))))


def _heuristic_priors(txt: str) -> tuple[float, float]:
    import re as _re

    web_boost = 0.0
    rag_boost = 0.0
    if _re.search(WEB_PRIOR_PAT, txt):
        web_boost += 0.25
    if _re.search(r"(ì˜¤ëŠ˜|ë‚´ì¼|ì´ë²ˆì£¼|ì§€ê¸ˆ|ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)", txt):
        web_boost += 0.10
    if _re.search(r"(ì—­|ë™|êµ¬|ì‹œ|êµ°|ë„|ê°€|ë¡œ)\b", txt):
        web_boost += 0.05  # ì§€ëª… íŒíŠ¸
    if _re.search(RAG_PRIOR_PAT, txt):
        rag_boost += 0.30
    web_boost = min(web_boost, 0.35)
    rag_boost = min(rag_boost, 0.35)
    return web_boost, rag_boost


# ìˆœìˆ˜ ëŒ€í™” ëª¨ë“œì—ì„œ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œë¡œ ë‹¨ìˆœí™”
SINGLE_CALL_CONV = bool(int(os.getenv("SINGLE_CALL_CONV", "1")))

# LLM ë¼ìš°í„° ì‚¬ìš© ì„¤ì •(íœ´ë¦¬ìŠ¤í‹± ìš°íšŒ)
USE_LLM_ROUTER = bool(int(os.getenv("USE_LLM_ROUTER", "1")))

# ì‚¬ì „ ê²€ì¦ íƒ€ì„ì•„ì›ƒ
PREVALIDATE_TIMEOUT_S = float(os.getenv("PREVALIDATE_TIMEOUT_S", "0.9"))

# ------------------------------
# ê³ ì • ì‹œìŠ¤í…œ ì •ì²´ì„± í”„ë¡¬í”„íŠ¸(3~4ì¤„)
# ------------------------------
IDENTITY_PROMPT = (
    "ë‚˜ëŠ” í•œêµ­ì–´ ê°œì¸ ë¹„ì„œë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë”°ë¼ ëŒ€í™”/ì¥ê¸°ê¸°ì–µ(RAG)/ì›¹ê²€ìƒ‰ì„ ìŠ¤ìŠ¤ë¡œ ì„ íƒí•´ ë‹µí•œë‹¤.\n"
    "ì¦ê±°(RAG/ì›¹)ê°€ ìˆì„ ë•ŒëŠ” ê·¸ ë²”ìœ„ ì•ˆì—ì„œë§Œ ì •í™•íˆ ì¸ìš©í•˜ê³ , ì—†ìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ê±°ë‚˜ í•„ìš” ì‹œ ë˜ë¬»ëŠ”ë‹¤.\n"
    "ì•ˆì „/ê°œì¸ì •ë³´/í—ˆìœ„ëŠ” ê¸ˆì§€í•˜ë©°, ì‚¬ìš©ìì˜ ì·¨í–¥ ì„¤ì •(ë™ì  ì§€ì‹œë¬¸)ì„ ì¡´ì¤‘í•´ ë§íˆ¬ì™€ í˜•ì‹ì„ ë§ì¶˜ë‹¤."
)

# ===== Redis/ìš”ì•½/ìŠ¤ëƒ…ìƒ· ì •ì±… ìƒìˆ˜ =====
MAX_TOKEN_LIMIT = 3000  # Redis ë‹¨ê¸° ë©”ëª¨ë¦¬ í•˜ë“œ í•œë„
RECENT_RAW_TOKENS_BUDGET = 1500  # ìµœê·¼ ì›ë¬¸ ë³´ì¡´ ì˜ˆì‚°
SUMMARY_TARGET_TOKENS = 500  # ì¶•ì•½ ëª©í‘œ(êµ¬ì¡°í™”+ìƒì„± í•©ì‚°)
SYSTEM_TOOL_BUDGET = 200  # ì‹œìŠ¤í…œ/íˆ´ ë©”íƒ€ ì—¬ìœ ë¶„
EDGE_THRESHOLD = MAX_TOKEN_LIMIT  # ì—ì§€ íŠ¸ë¦¬ê±° ì„ê³„ì¹˜ (3000)
DEBOUNCE_TURNS = 5  # ë°°ì¹˜: ìµœì†Œ í„´
DEBOUNCE_SECONDS = 60  # ë°°ì¹˜: ìµœì†Œ ì‹œê°„
SNAPSHOT_QUEUE_MAXSIZE = 128  # ì‘ì—… í
WORKER_CONCURRENCY = 2  # ë™ì‹œ ìŠ¤ëƒ…ìƒ· ì²˜ë¦¬
EMBED_CONCURRENCY = 2  # ì„ë² ë”© ë™ì‹œ ì œí•œ

# ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© ì„¤ì •(ì¡°ì§ ê¶Œí•œ ì—†ì„ ì‹œ ëŸ°íƒ€ì„ ë¹„í™œì„±í™”)
STREAM_ENABLED = bool(int(os.getenv("STREAM_ENABLED", "1")))
_STREAM_RUNTIME_DISABLED = False


def _stream_allowed() -> bool:
    return STREAM_ENABLED and (not _STREAM_RUNTIME_DISABLED)


# ===== ì¤‘ë³µ ì–µì œ/ì‹ ê·œì„± ê²Œì´íŠ¸ìš© ì„¤ì • (ìƒë‹¨ ì„¤ì •ë¶€ì— ì¶”ê°€) =====
SNAPSHOT_EDGE_TOKENS = int(
    os.getenv("SNAPSHOT_EDGE_TOKENS", "4500")
)  # ìŠ¤ëƒ…ìƒ· ì ì¬ íŠ¸ë¦¬ê±°(ë©”ëª¨ë¦¬ 3000ê³¼ ë¶„ë¦¬)
NOVELTY_SIM_THRESHOLD = float(
    os.getenv("NOVELTY_SIM_THRESHOLD", "0.92")
)  # ë¡œê·¸ ê·¼ì‚¬ì¤‘ë³µ ì–µì œ ì„ê³„
NOVELTY_MIN_PROFILE_DELTA = int(
    os.getenv("NOVELTY_MIN_PROFILE_DELTA", "1")
)  # í”„ë¡œí•„ ì‹ ê·œ í•­ëª© ìµœì†Œ ê°œìˆ˜
SNAPSHOT_LOOKBACK_MONTHS = int(
    os.getenv("SNAPSHOT_LOOKBACK_MONTHS", "1")
)  # ê·¼ì‚¬ì¤‘ë³µ íƒìƒ‰ ê¸°ê°„

# ===== ì™¸ë¶€ í˜¸ì¶œ ì¬ì‹œë„ ê³µí†µ ì„¤ì • =====
MAX_RETRIES_OPENAI = int(os.getenv("MAX_RETRIES_OPENAI", "2"))
MAX_RETRIES_HTTP = int(os.getenv("MAX_RETRIES_HTTP", "2"))
RETRY_BASE_DELAY = float(os.getenv("RETRY_BASE_DELAY", "0.25"))

logger.info(
    f"[boot] model={LLM_MODEL} embed={EMBEDDING_MODEL} milvus={MILVUS_HOST}:{MILVUS_PORT} redis={REDIS_URL}"
)
logger.info(
    f"[boot] thresholds tau_rag={TAU_RAG} tau_web={TAU_WEB} ambiguity={AMBIGUITY_BAND} timeouts web={TIMEOUT_WEB}s rag={TIMEOUT_RAG}s"
)

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# Firestore í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ì´ˆê¸°í™”)
_FS_DB = None


def _ensure_fs_db():
    global _FS_DB
    if _FS_DB is not None:
        return _FS_DB
    if not FIRESTORE_ENABLE or gcf is None:
        return None
    try:
        _FS_DB = gcf.Client()
        logger.info("[fs] firestore client initialized")
        return _FS_DB
    except Exception as e:
        logger.warning("[fs] init failed: %r", e)
        return None


# ----------------------------------------------------------------------
# ê³µí†µ: OpenAI/HTTP ì¬ì‹œë„ ë˜í¼ (ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°)
# ----------------------------------------------------------------------


async def _backoff_sleep(attempt: int):
    delay = RETRY_BASE_DELAY * (2**attempt) * (0.5 + random.random())
    await asyncio.sleep(delay)


async def openai_chat_with_retry(**create_kwargs):
    last_err = None
    # ì‚¬ì „ ë§¤í•‘/ì •ë¦¬: Chat CompletionsëŠ” ì¼ê´€ë˜ê²Œ max_tokensë¥¼ ì‚¬ìš©
    # - max_completion_tokensê°€ ë“¤ì–´ì˜¤ë©´ max_tokensë¡œ ë³€í™˜
    # - êµ¬í˜• gpt-4-* ëª¨ë¸ì—ì„œ response_formatì€ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„ ì œ ì œê±°
    try:
        model_name = str(create_kwargs.get("model", LLM_MODEL) or "")
        # 1) max_completion_tokens â†’ max_tokens ì •ê·œí™”
        if "max_completion_tokens" in create_kwargs:
            try:
                mt = int(create_kwargs.pop("max_completion_tokens"))
                create_kwargs["max_tokens"] = mt
            except Exception:
                create_kwargs.pop("max_completion_tokens", None)

        # 2) response_format í˜¸í™˜ì„± ì²´í¬ (gpt-4o/4.1/o3 ë“± ì¼ë¶€ ëª¨ë¸ë§Œ ì•ˆì • ì§€ì›)
        rf = create_kwargs.get("response_format")
        if isinstance(rf, dict):
            rf_type = str(rf.get("type", "")).lower()
            # ì§€ì› ëª¨ë¸ íŒíŠ¸ í‚¤ì›Œë“œ
            supports = any(
                k in model_name for k in ("gpt-4o", "gpt-4.1", "o3", "o4", "4o")
            )
            if not supports:
                # êµ¬í˜• ëª¨ë¸(gpt-4-0613 ë“±)ì—ì„œëŠ” 400ì„ ìœ ë°œí•˜ë¯€ë¡œ ì„ ì œ ì œê±°
                create_kwargs.pop("response_format", None)
            elif rf_type not in ("json_object", "json_schema"):
                # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ì œê±°
                create_kwargs.pop("response_format", None)
    except Exception:
        pass
    for attempt in range(MAX_RETRIES_OPENAI + 1):
        try:
            return await client.chat.completions.create(**create_kwargs)
        except Exception as e:
            last_err = e
            # 400 íŒŒë¼ë¯¸í„° í˜¸í™˜ ì´ìŠˆì— ëŒ€í•œ ìë™ êµì • í›„ ì¬ì‹œë„
            if attempt < MAX_RETRIES_OPENAI:
                msg = str(e)
                try:
                    # ë‚¨ì•„ìˆëŠ” ë¹„í˜¸í™˜ ë§¤ê°œë³€ìˆ˜ ì •ë¦¬
                    if "max_completion_tokens" in create_kwargs:
                        mt = int(create_kwargs.pop("max_completion_tokens"))
                        create_kwargs["max_tokens"] = mt
                    if isinstance(create_kwargs.get("response_format"), dict):
                        create_kwargs.pop("response_format", None)
                    if "stream" in msg and create_kwargs.get("stream") is True:
                        create_kwargs["stream"] = False
                except Exception:
                    pass
                await _backoff_sleep(attempt)
                continue
            break
    raise last_err


async def _rewrite_with_retries(
    messages: list[dict],
    base_timeout_s: float,
    attempts: int = 1,
    delta_s: float = 1.0,
    max_tokens: int | None = None,
    response_format: dict | None = None,
):
    """
    ì¬ì‘ì„± í˜¸ì¶œì— ëŒ€í•´ íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ìµœëŒ€ attempts-1íšŒê¹Œì§€ 1ì´ˆì”© íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ ì¬ì‹œë„.
    - ì„±ê³µ ì‹œ content ë¬¸ìì—´ì„ ë°˜í™˜, ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜.
    """
    for i in range(attempts):
        tout = base_timeout_s + i * delta_s
        try:
            resp = await asyncio.wait_for(
                openai_chat_with_retry(
                    model=REWRITE_MODEL,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=(
                        max_tokens if max_tokens is not None else REWRITE_MAX_TOKENS
                    ),
                    **({"response_format": response_format} if response_format else {}),
                ),
                timeout=tout,
            )
            return (resp.choices[0].message.content or "").strip()
        except asyncio.TimeoutError:
            if i + 1 >= attempts:
                break
            logger.warning("[rewrite] timeout(%.2fs) -> retry %d", tout, i + 1)
        except Exception as e:
            logger.warning("[rewrite] error=%r", e)
            break
    return None


async def http_get_with_retry(
    url: str, headers: dict | None, params: dict | None, timeout: httpx.Timeout | None
):
    last_err = None
    for attempt in range(MAX_RETRIES_HTTP + 1):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client_http:
                return await client_http.get(url, headers=headers, params=params)
        except Exception as e:
            last_err = e
            if attempt >= MAX_RETRIES_HTTP:
                break
            await _backoff_sleep(attempt)
    raise last_err


llm = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY,
    model=LLM_MODEL,
)

# ë©”ëª¨ë¦¬/ìš”ì•½ìš©: ì§„ì§œ LLM ì¸ìŠ¤í„´ìŠ¤(= BaseLanguageModel)
llm_cold = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY,
    model=LLM_MODEL,
)


# ì„ë² ë”© í†µí•©: backend.rag.embeddingsì˜ ë°±ì—”ë“œ ì¶”ìƒí™” ì‚¬ìš©ìœ¼ë¡œ êµì²´
from backend.rag.embeddings import get_embeddings, embed_documents as embed_docs

embeddings = get_embeddings()

PROFILE_DB = {}

# ----------------------------------------------------------------------
# KST & ë‚ ì§œ ìœ í‹¸ (ìƒëŒ€ì‹œì œ ì²˜ë¦¬)
# ----------------------------------------------------------------------
KST = timezone(timedelta(hours=9))


def _now_kst() -> datetime:
    return datetime.now(KST)


def _ym(dt: datetime) -> int:
    return dt.year * 100 + dt.month  # 202508


def _ymd(dt: datetime) -> int:
    return dt.year * 10000 + dt.month * 100 + dt.day  # 20250817


def _week_range(base: datetime, offset_weeks=0) -> Tuple[datetime, datetime]:
    d0 = base + timedelta(weeks=offset_weeks)
    start = d0 - timedelta(days=d0.weekday())  # ì›”ìš”ì¼
    end = start + timedelta(days=6)
    return start.replace(tzinfo=KST), end.replace(tzinfo=KST)


def _month_range(base: datetime, offset_months=0) -> Tuple[datetime, datetime]:
    y, m = base.year, base.month + offset_months
    y += (m - 1) // 12
    m = ((m - 1) % 12) + 1
    start = datetime(y, m, 1, tzinfo=KST)
    y2, m2 = y + (m // 12), (m % 12) + 1
    next_first = datetime(y2, m2, 1, tzinfo=KST)
    end = next_first - timedelta(days=1)
    return start, end


def _ym_minus_months(base: datetime, months: int) -> int:
    y, m = base.year, base.month - months
    while m <= 0:
        y -= 1
        m += 12
    return y * 100 + m


async def _post_verify_answer(
    user_input: str, rag_ctx: str, web_ctx: str, answer: str, websocket: WebSocket
):
    """
    RAG/WEB ì»¨í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•  ë•Œ ë‹µë³€ì˜ ì í•©ì„±ì„ ê²½ëŸ‰ ê²€í† . ì‹¤ì‹œê°„ì„± ìœ„í•´ ë¹„ì°¨ë‹¨ ì „ì†¡.
    - ê·œì¹™: ì§ˆë¬¸ê³¼ ë¬´ê´€/ìƒì¶©/ê³¼ì¥/ì¶”ì¸¡ ì—¬ë¶€ë¥¼ ì ê²€. ë¬¸ì œì‹œ 1ì¤„ ê²½ê³ ì™€ ì¬ê²€í†  ì œì•ˆ.
    """
    if not (rag_ctx.strip() or web_ctx.strip()):
        return
    # f-string í‘œí˜„ì‹ ë‚´ë¶€ì—ëŠ” ë°±ìŠ¬ë˜ì‹œ(ì˜ˆ: '\n')ê°€ ë“¤ì–´ê°ˆ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì‚¬ì „ ê³„ì‚°
    ctx_cut = (f"{rag_ctx}\n{web_ctx}")[:1200]
    ans_cut = (answer or "")[:800]

    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ë‹µë³€ ê²€í† ìë‹¤. [ì»¨í…ìŠ¤íŠ¸]ì™€ [ì§ˆë¬¸] ëŒ€ë¹„ [ë‹µë³€]ì˜ ì í•©ì„±ì„ í™•ì¸í•˜ê³ ,"
                " ë¬´ê´€/ìƒì¶©/ì¶”ì¸¡/ê³¼ì¥ì„ ê°ì§€í•˜ë©´ ê²½ê³  ë©”ì‹œì§€ 1ì¤„ì„ í•œêµ­ì–´ë¡œ ì¶œë ¥í•˜ë¼."
                " ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë§Œ ëŒë ¤ë¼."
            ),
        },
        {
            "role": "user",
            "content": f"[ì§ˆë¬¸]\n{user_input}\n\n[ì»¨í…ìŠ¤íŠ¸]\n{ctx_cut}\n\n[ë‹µë³€]\n{ans_cut}",
        },
    ]
    try:
        resp = await asyncio.wait_for(
            openai_chat_with_retry(
                model=LLM_MODEL, messages=msgs, temperature=0.0, max_tokens=80
            ),
            timeout=0.9,
        )
        tip = (resp.choices[0].message.content or "").strip()
        if tip:
            try:
                await websocket.send_text(f"\n[ê²€í† ] {tip}")
            except Exception:
                pass
    except Exception:
        return


async def _validate_final_answer(
    user_input: str, rag_ctx: str, web_ctx: str, answer: str
) -> bool:
    """
    ìµœì¢… ë‹µë³€ ì í•©ì„± ì‚¬ì „ ê²€ì¦: ì§ˆë¬¸/ì»¨í…ìŠ¤íŠ¸ ëŒ€ë¹„ ë¶€ì í•©í•˜ë©´ False.
    - ì›¹ ë˜í¼ ê²½ë¡œì—ì„œë„ ì‚¬ìš©í•˜ì—¬ ë¶€ì í•©ì‹œ LLM ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í´ë°±.
    """
    try:
        if not answer:
            return False
        schema = {
            "name": "AnswerFit",
            "schema": {
                "type": "object",
                "properties": {"keep": {"type": "boolean"}},
                "required": ["keep"],
                "additionalProperties": False,
            },
        }
        ctx_short = (rag_ctx or "") + "\n" + (web_ctx or "")
        ctx_short = ctx_short[:1200]
        ans_short = (answer or "")[:800]
        msgs = [
            {
                "role": "system",
                "content": (
                    "ë„ˆëŠ” ìµœì¢… ê²€ì¦ìë‹¤. [ì§ˆë¬¸]ê³¼ [ì»¨í…ìŠ¤íŠ¸] ëŒ€ë¹„ [ë‹µë³€]ì´ ì˜ë„ì— ì í•©í•˜ë©´ keep=true,"
                    " ë¶€ì í•©/ë¬´ê´€/ë…¸ì´ì¦ˆ ê³¼ë‹¤ë©´ keep=false. JSONë§Œ."
                ),
            },
            {
                "role": "user",
                "content": f"[ì§ˆë¬¸]\n{user_input}\n\n[ì»¨í…ìŠ¤íŠ¸]\n{ctx_short}\n\n[ë‹µë³€]\n{ans_short}",
            },
        ]
        kwargs = {
            "model": LLM_MODEL,
            "messages": msgs,
            "max_tokens": 20,
            "temperature": 0.0,
        }
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_schema", "json_schema": schema}
        resp = await asyncio.wait_for(openai_chat_with_retry(**kwargs), timeout=0.9)
        txt = (resp.choices[0].message.content or "").strip()
        data = json.loads(txt) if txt.startswith("{") else {}
        return bool(data.get("keep", False))
    except Exception:
        return True  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨í•˜ì§€ ì•ŠìŒ


def _flatten_profile_items(p: dict) -> set[str]:
    # í”„ë¡œí•„ JSONì—ì„œ í•µì‹¬ í•„ë“œë§Œ í‰íƒ„í™”í•˜ì—¬ ë¹„êµ(ì†Œë¬¸ì/ê³µë°±ì •ê·œí™”)
    keys = ("facts", "goals", "tasks", "decisions", "constraints")
    out = set()
    for k in keys:
        v = p.get(k)
        if isinstance(v, list):
            for x in v:
                s = str(x).strip().lower()
                if s:
                    out.add(s)
        elif isinstance(v, str):
            s = v.strip().lower()
            if s:
                out.add(s)
    return out


def _near_duplicate_log(
    session_id: str, log_emb: List[float], ym_min: int
) -> tuple[bool, float]:
    # ìµœê·¼ Nê°œì›” ë²”ìœ„ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¡œê·¸ 1ê°œë¥¼ ì¡°íšŒí•´ ê·¼ì‚¬ì¤‘ë³µ ì—¬ë¶€ë¥¼ ë°˜í™˜
    prof_coll, log_coll = ensure_milvus_collections()
    search_params = {"metric_type": METRIC, "params": {"ef": 32}}  # ë¹„ìš© ë‚®ê²Œ
    expr = f"user_id == '{session_id}' and date_ym >= {ym_min}"
    try:
        res = log_coll.search(
            data=[log_emb],
            anns_field="embedding",
            param=search_params,
            limit=1,
            expr=expr,
            output_fields=["text", "date_ym"],
        )
        if res and res[0]:
            sim = _hit_similarity(res[0][0])
            return (sim >= NOVELTY_SIM_THRESHOLD, sim)
    except Exception as e:
        logger.warning("[novelty] near-dup search error: %r", e)
    return (False, 0.0)


# ----------------------------------------------------------------------
# ìƒëŒ€ì‹œì œ â†’ ì ˆëŒ€ ë‚ ì§œ í† í¬ë‚˜ì´ì € (WEB/RAG)
# ----------------------------------------------------------------------
RELATIVE_PATTERNS_DAY = [
    (r"\bì˜¤ëŠ˜\b", lambda now: (now, now)),
    (r"\bë‚´ì¼\b", lambda now: (now + timedelta(days=1), now + timedelta(days=1))),
    (r"\bëª¨ë ˆ\b", lambda now: (now + timedelta(days=2), now + timedelta(days=2))),
    (r"\bê¸€í”¼\b", lambda now: (now + timedelta(days=3), now + timedelta(days=3))),
    (r"\bë‚´ê¸€í”¼\b", lambda now: (now + timedelta(days=4), now + timedelta(days=4))),
    (r"\bì–´ì œ\b", lambda now: (now - timedelta(days=1), now - timedelta(days=1))),
    (
        r"\bê·¸ì œ\b|\bê·¸ì €ê»˜\b|\bì—Šê·¸ì œ\b",
        lambda now: (now - timedelta(days=2), now - timedelta(days=2)),
    ),
    (r"\bê·¸ë„ì œ\b", lambda now: (now - timedelta(days=3), now - timedelta(days=3))),
]
RELATIVE_PATTERNS_WEEK = [
    (r"\bì´ë²ˆ\s*ì£¼ë§\b", lambda now: _week_range(now, 0)),
    (r"\bì§€ë‚œ\s*ì£¼ë§\b", lambda now: _week_range(now, -1)),
    (r"\bë‹¤ìŒ\s*ì£¼ë§\b", lambda now: _week_range(now, 1)),
    (r"\bì´ë²ˆ\s*ì£¼\b", lambda now: _week_range(now, 0)),
    (r"\bì§€ë‚œ\s*ì£¼\b", lambda now: _week_range(now, -1)),
    (r"\bë‹¤ìŒ\s*ì£¼\b", lambda now: _week_range(now, 1)),
]
RELATIVE_PATTERNS_MONTH_YEAR = [
    (r"\bì´ë²ˆ\s*ë‹¬\b|\bì´ë‹¬\b", lambda now: _month_range(now, 0)),
    (r"\bì§€ë‚œ\s*ë‹¬\b|\bì €ë²ˆ\s*ë‹¬\b", lambda now: _month_range(now, -1)),
    (r"\bë‹¤ìŒ\s*ë‹¬\b", lambda now: _month_range(now, 1)),
    (
        r"\bì˜¬í•´\b",
        lambda now: (
            datetime(now.year, 1, 1, tzinfo=KST),
            datetime(now.year, 12, 31, tzinfo=KST),
        ),
    ),
    (
        r"\bì‘ë…„\b",
        lambda now: (
            datetime(now.year - 1, 1, 1, tzinfo=KST),
            datetime(now.year - 1, 12, 31, tzinfo=KST),
        ),
    ),
    (
        r"\bì¬ì‘ë…„\b",
        lambda now: (
            datetime(now.year - 2, 1, 1, tzinfo=KST),
            datetime(now.year - 2, 12, 31, tzinfo=KST),
        ),
    ),
    (
        r"\bë‚´ë…„\b",
        lambda now: (
            datetime(now.year + 1, 1, 1, tzinfo=KST),
            datetime(now.year + 1, 12, 31, tzinfo=KST),
        ),
    ),
    (
        r"\bë‚´í›„ë…„\b",
        lambda now: (
            datetime(now.year + 2, 1, 1, tzinfo=KST),
            datetime(now.year + 2, 12, 31, tzinfo=KST),
        ),
    ),
]


def _extract_date_range_for_rag(
    text: str, now: Optional[datetime] = None
) -> Optional[Tuple[int, int]]:
    now = now or _now_kst()
    start_dt, end_dt = None, None

    def _apply(pats):
        nonlocal start_dt, end_dt
        for pat, fn in pats:
            m = re.search(pat, text)
            if m:
                s, e = fn(now)
                start_dt = s if start_dt is None else min(start_dt, s)
                end_dt = e if end_dt is None else max(end_dt, e)

    _apply(RELATIVE_PATTERNS_DAY)
    _apply(RELATIVE_PATTERNS_WEEK)
    _apply(RELATIVE_PATTERNS_MONTH_YEAR)
    if start_dt and end_dt:
        return (_ymd(start_dt), _ymd(end_dt))
    return None


def _month_tokens_for_web(
    text: str, now: Optional[datetime] = None
) -> Optional[Tuple[int, int]]:
    now = now or _now_kst()
    start_dt, end_dt = None, None

    def _apply(pats):
        nonlocal start_dt, end_dt
        for pat, fn in pats:
            m = re.search(pat, text)
            if m:
                s, e = fn(now)
                start_dt = s if start_dt is None else min(start_dt, s)
                end_dt = e if end_dt is None else max(end_dt, e)

    _apply(RELATIVE_PATTERNS_DAY)
    _apply(RELATIVE_PATTERNS_WEEK)
    _apply(RELATIVE_PATTERNS_MONTH_YEAR)
    if start_dt and end_dt:
        return (_ym(start_dt), _ym(end_dt))
    return None


# ----------------------------------------------------------------------
# Embedding-based Intent Router ì„¤ì • (ë³´ì¡°)
# ----------------------------------------------------------------------
INTENT_EXAMPLES = {
    "rag": [
        "ë‚´ê°€ ì €ë²ˆì— ì„¤ì •í•œ ëª©í‘œ ë‹¤ì‹œ ì•Œë ¤ì¤˜.",
        "ë‚´ê°€ ë„ˆì—ê²Œ ì•Œë ¤ì¤€ ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€?"
        "ë„ˆê°€ ì–´ì œ ì¶”ì²œí•´ì¤¬ë˜ ë¸Œë Œë“œê°€ ë­ì˜€ì§€?"
        "ìš°ë¦¬ ì§€ë‚œì£¼ì— ë¬´ìŠ¨ ì–˜ê¸°ê¹Œì§€ í–ˆì§€?",
        "ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ì˜ ì§„í–‰ ìƒí™© ìš”ì•½í•´ë´.",
        "ë‚´ê°€ ì—Šê·¸ì œ ë„ˆí•œí…Œ ë§í–ˆë˜ ê³ ë¯¼ ê¸°ì–µí•´?",
        "ì˜¤ëŠ˜ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‚´ ìµœì¢… ëª©í‘œë¥¼ ì—…ë°ì´íŠ¸í•´ì„œ ì •ë¦¬í•´ì¤˜.",
        "ì§€ë‚œ ëŒ€í™”ì—ì„œ OKR ì •ë¦¬í•´ì¤˜.",
    ],
    "web": [
        "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?",
        "ê°€ì¥ ê°€ê¹Œìš´ ìŠ¤íƒ€ë²…ìŠ¤ ì–´ë””ì•¼?",
        "ì—”ë¹„ë””ì•„ì˜ ìµœì‹  GPU ëª¨ë¸ ì´ë¦„ì´ ë­ì•¼?",
        "í•œêµ­ì˜ í˜„ ì •ì¹˜ ìƒí™©",
        "ì €ë… ë¨¹ìœ¼ë ¤ëŠ”ë° ê°•ë‚¨ì—­ ë§›ì§‘ ì¶”ì²œí•´ì¤„ë˜?",
    ],
    "conv": [
        "ê³ ë§ˆì›Œ! ì˜ ì´í•´í–ˆì–´.",
        "ì–‘ìì—­í•™ì€ ì™œ ì´ë ‡ê²Œ ì–´ë ¤ìš¸ê¹Œ?",
        "ì‹¬ì‹¬í•´. ì¬ë°ŒëŠ” ë†ë‹´ í•˜ë‚˜ í•´ì¤˜.",
        "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?",
        "ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ.",
    ],
}

# íŒŒì¼ ê¸°ë°˜ seed í™•ì¥(ì˜µì…˜): My_Business/dataì˜ conv/web/rag ë°ì´í„°ë¡œ ì¦ë¶„
try:
    DATA_DIR = str(Path(__file__).resolve().parents[1] / "data")
    seed_files = {
        "conv": os.path.join(DATA_DIR, "conv_data.txt"),
        "web": os.path.join(DATA_DIR, "web_data.txt"),
        "rag": os.path.join(DATA_DIR, "rag_data.txt"),
    }
except Exception:
    seed_files = {}

_INTENT_READY = False
_INTENT_LOCK = Lock()
INTENT_EMBEDDINGS: dict[str, np.ndarray] = {}
INTENT_EMBEDDINGS_LIST: dict[str, list[np.ndarray]] = {}


def _ensure_intent_embeddings():
    global _INTENT_READY
    if _INTENT_READY:
        return
    with _INTENT_LOCK:
        if _INTENT_READY:
            return
        logger.info("[boot] preparing intent embeddings (lazy) ...")
        # íŒŒì¼ ì‹œë“œ ë³‘í•©
        labels = list(INTENT_EXAMPLES.keys())
        groups = []
        for l in labels:
            texts = list(INTENT_EXAMPLES.get(l, []))
            try:
                p = seed_files.get(l)
                if p and Path(p).exists():
                    with open(p, "r", encoding="utf-8") as f:
                        extra = [ln.strip() for ln in f.readlines() if ln.strip()]
                        # ê³¼ë„í•œ ë…¸ì´ì¦ˆ ë°©ì§€: ìµœëŒ€ 1000 ë¼ì¸ê¹Œì§€ ì‚¬ìš©
                        texts.extend(extra[:1000])
            except Exception:
                pass
            groups.append(texts)
        flat_texts = [t for texts in groups for t in texts]
        # í†µì¼ ì¶”ìƒí™”(embed_docs)
        flat_vecs = embed_docs(flat_texts)
        idx = 0
        for label, texts in zip(labels, groups):
            n = len(texts)
            vecs = flat_vecs[idx : idx + n]
            idx += n
            arr = [np.array(v, dtype=np.float32) for v in vecs]
            avg = np.mean(np.stack(arr, axis=0), axis=0)
            INTENT_EMBEDDINGS[label] = avg
            INTENT_EMBEDDINGS_LIST[label] = arr
        _INTENT_READY = True
        logger.info(
            "[boot] intent embeddings ready: %s", list(INTENT_EMBEDDINGS.keys())
        )


def embedding_router(query: str, threshold: float = 0.7) -> str | None:
    _ensure_intent_embeddings()
    start = time.time()
    q_emb = np.array(embed_query_cached(query))
    sims = {}
    topk = int(os.getenv("ROUTER_SEED_TOPK", "8"))
    qn = float(np.linalg.norm(q_emb) or 1.0)
    for label, emb in INTENT_EMBEDDINGS.items():
        # 1) ì„¼íŠ¸ë¡œì´ë“œ
        s_cent = float(np.dot(q_emb, emb) / (qn * (float(np.linalg.norm(emb)) or 1.0)))
        # 2) ì˜ˆì‹œë³„ top-k í‰ê· 
        lst = INTENT_EMBEDDINGS_LIST.get(label, [])
        if lst:
            per = []
            for v in lst:
                den = float(np.linalg.norm(v) or 1.0) * qn
                per.append(float(np.dot(q_emb, v) / (den or 1.0)))
            per.sort(reverse=True)
            s_top = float(np.mean(per[: max(1, min(topk, len(per)))]))
            sims[label] = 0.5 * s_cent + 0.5 * s_top
        else:
            sims[label] = s_cent
    best = max(sims, key=sims.get)
    took = (time.time() - start) * 1000
    logger.info(
        f"[router:embedding] sims={sims} best={best} thr={threshold} took_ms={took:.1f}"
    )
    return best if sims[best] >= threshold else None


def _prefer_rag_when_recall(user_input: str) -> bool:
    return bool(
        re.search(r"ì§€ë‚œ|ì €ë²ˆ|ì—Šê·¸ì œ|ê·¸ë•Œ|ìš°ë¦¬ ëŒ€í™”|ì–´ì œ ì¶”ì²œ|ì§€ë‚œë²ˆ ì¶”ì²œ", user_input)
    )


# ----------------------------------------------------------------------
# ì…€ 2.5) ì†Œë¶„ë¥˜ê¸° ë¡œë“œ (ë©€í‹°ë¼ë²¨ need_rag/need_web)
# ----------------------------------------------------------------------
# OUTPUT_DIR = "./router_kor_electra_small"
# OUTPUT_DIR = str(Path(__file__).resolve().parent / "router_kor_electra_small")
OUTPUT_DIR = str(
    Path(__file__).resolve().parent.parent / "models" / "router_kor_electra_small"
)
USE_ONNX = bool(int(os.getenv("ROUTER_USE_ONNX", "0")))
ONNX_PATH = os.getenv(
    "ROUTER_ELECTRA_ONNX",
    str(Path(OUTPUT_DIR) / "model.onnx"),
)

MAX_LEN = 192

loaded_tok = HFTokenizer.from_pretrained(OUTPUT_DIR)
loaded_cfg = HFConfig.from_pretrained(OUTPUT_DIR)
_onnx_sess = None
loaded_model = None
if USE_ONNX:
    try:
        import onnxruntime as ort  # type: ignore

        _onnx_sess = ort.InferenceSession(ONNX_PATH, providers=["CPUExecutionProvider"])
        logger.info(
            f"[boot] loaded small classifier(ONNX) from {ONNX_PATH} (multi-label 2 heads)"
        )
    except Exception as _e_onx:
        logger.warning(
            f"[boot] onnx load failed: {repr(_e_onx)} -> fallback to torch weights at {OUTPUT_DIR}"
        )
        loaded_model = HFForSeq.from_pretrained(OUTPUT_DIR, config=loaded_cfg)
        loaded_model.eval()
        logger.info(
            f"[boot] loaded small classifier(Torch) from {OUTPUT_DIR} (multi-label 2 heads)"
        )
else:
    loaded_model = HFForSeq.from_pretrained(OUTPUT_DIR, config=loaded_cfg)
    loaded_model.eval()
    logger.info(
        f"[boot] loaded small classifier(Torch) from {OUTPUT_DIR} (multi-label 2 heads)"
    )


def predict_need_flags(query: str, tau_rag: float = TAU_RAG, tau_web: float = TAU_WEB):
    import numpy as _np

    logits = None
    # ONNX ê²½ë¡œ
    if _onnx_sess is not None:
        try:
            enc_np = loaded_tok(
                query,
                return_tensors="np",
                truncation=True,
                max_length=MAX_LEN,
                padding="max_length",
            )
            inputs = {
                "input_ids": enc_np["input_ids"].astype(_np.int64),
                "attention_mask": enc_np["attention_mask"].astype(_np.int64),
            }
            if "token_type_ids" in enc_np:
                inputs["token_type_ids"] = enc_np["token_type_ids"].astype(_np.int64)
            outs = _onnx_sess.run(None, inputs)
            logits = _np.array(outs[0])[0]
        except Exception as _e_onnx:
            logger.warning(
                f"[clf] onnx inference failed: {repr(_e_onnx)} -> fallback torch"
            )
    # Torch ê²½ë¡œ
    if logits is None:
        with torch.no_grad():
            enc = loaded_tok(
                query,
                return_tensors="pt",
                truncation=True,
                max_length=MAX_LEN,
                padding="max_length",
            )
            out = loaded_model(**enc)
            logits = out.logits[0].detach().cpu().numpy()  # shape (2,) rag, web
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜(Platt) ì ìš© í›„ íœ´ë¦¬ìŠ¤í‹± í”„ë¼ì´ì–´ ê°€ì 
        p_rag = _apply_calibration(float(logits[0]), CAL_RAG)
        p_web = _apply_calibration(float(logits[1]), CAL_WEB)
        w_boost, r_boost = _heuristic_priors(query)
        need_rag_prob = float(np.clip(p_rag + r_boost, 0.0, 1.0))
        need_web_prob = float(np.clip(p_web + w_boost, 0.0, 1.0))
        need_rag = int(need_rag_prob >= tau_rag)
        need_web = int(need_web_prob >= tau_web)
        logger.info(
            f"[clf] q='{query[:80]}' cal_p(rag)={p_rag:.3f}+{r_boost:.2f}->{need_rag_prob:.3f} "
            f"cal_p(web)={p_web:.3f}+{w_boost:.2f}->{need_web_prob:.3f} "
            f"tau_rag={tau_rag} tau_web={tau_web} -> need_rag={need_rag} need_web={need_web}"
        )
        return {
            "need_rag_prob": need_rag_prob,
            "need_web_prob": need_web_prob,
            "need_rag": need_rag,
            "need_web": need_web,
            "p_rag_cal": float(p_rag),
            "p_web_cal": float(p_web),
            "r_boost": float(r_boost),
            "w_boost": float(w_boost),
        }


# ----------------------------------------------------------------------
# LRU ì„ë² ë”© ìºì‹œ: rag.embeddings.embed_query_cachedë¡œ ì¼ì›í™” (ì¤‘ë³µ ì •ì˜ ì œê±°)
# ----------------------------------------------------------------------
# ì£¼ì˜: ê¸°ì¡´ ë™ì¼ í•¨ìˆ˜ëª…ì´ ì¡´ì¬í–ˆìœ¼ë‚˜ ì´ì œëŠ” backend.rag.embeddings ëª¨ë“ˆì˜ êµ¬í˜„ì„ ì‚¬ìš©í•œë‹¤.


# ----------------------------------------------------------------------
# ì…€ 3: Milvus DB í—¬í¼ í•¨ìˆ˜ (HNSW + ë‚ ì§œ ë©”íƒ€ í•„ë“œ + íŒŒí‹°ì…˜)
# ----------------------------------------------------------------------
def ensure_milvus():
    alias = "default"
    if not connections.has_connection(alias):
        logger.info(
            "[milvus] connecting alias=%s host=%s port=%s",
            alias,
            MILVUS_HOST,
            MILVUS_PORT,
        )
        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)


def _ensure_partition(coll: Collection, ym: int):
    part_name = f"ym_{ym}"
    try:
        if part_name not in [p.name for p in coll.partitions]:
            coll.create_partition(partition_name=part_name, description=f"YYYYMM={ym}")
            logger.info("[milvus] created partition %s in %s", part_name, coll.name)
    except Exception as e:
        logger.warning("[milvus] ensure partition error: %r", e)
    return part_name


def create_milvus_collection(name: str, desc: str):
    if utility.has_collection(name):
        coll = Collection(name)
        for f in coll.schema.fields:
            if f.name == "embedding":
                existing_dim = f.params.get("dim")
                if existing_dim != EMBEDDING_DIM:
                    logger.error(
                        f"[milvus] dim mismatch for {name}: existing={existing_dim} expected={EMBEDDING_DIM}"
                    )
                    raise RuntimeError(
                        "Milvus collection dim mismatch. Create a new collection with correct dim."
                    )
        have_dates = set(x.name for x in coll.schema.fields)
        expected = {"date_start", "date_end", "date_ym"}
        missing = expected - have_dates
        if missing:
            logger.warning(
                f"[milvus] collection {name} missing date fields {missing}. Consider migrating to v3."
            )
        logger.info("[milvus] reuse collection=%s", name)
        return coll

    fields = [
        FieldSchema("id", DataType.VARCHAR, is_primary=True, max_length=256),
        FieldSchema("embedding", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),
        FieldSchema("text", DataType.VARCHAR, max_length=65535),
        FieldSchema("user_id", DataType.VARCHAR, max_length=256),
        FieldSchema("type", DataType.VARCHAR, max_length=50),
        FieldSchema("created_at", DataType.INT64),
        FieldSchema("date_start", DataType.INT64),  # YYYYMMDD
        FieldSchema("date_end", DataType.INT64),  # YYYYMMDD
        FieldSchema("date_ym", DataType.INT64),  # YYYYMM
    ]
    schema = CollectionSchema(fields, desc)
    coll = Collection(name, schema)
    coll.create_index(
        "embedding",
        {
            "index_type": "HNSW",
            "metric_type": "COSINE",
            "params": {"M": 16, "efConstruction": 200},
        },
    )
    logger.info(
        "[milvus] created collection=%s index=HNSW(COSINE) M=16 efC=200 dim=%d",
        name,
        EMBEDDING_DIM,
    )
    return coll


# ----------------------------------------------------------------------
# ì…€ 4: ì¥ê¸° ê¸°ì–µ(RAG) ì—…ë°ì´íŠ¸ (ì—…ë°ì´íŠ¸: êµ¬ì¡°í™”+ìš”ì•½ ìŠ¤ëƒ…ìƒ·/ë©±ë“± ì—…ì„œíŠ¸)
# ----------------------------------------------------------------------
_prof_coll = None
_log_coll = None
_milvus_ready = False
_coll_lock = Lock()


def ensure_milvus_collections():
    # ìœ ì§€: ì™¸ë¶€ í˜¸ì¶œì í˜¸í™˜. ë‚´ë¶€ëŠ” rag.ensure_collections ì‚¬ìš©
    return ensure_collections()


# ===== ì•ˆì „í•œ ì „ì—­ í…œí”Œë¦¿ =====
STRUCTURE_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", "ëŒ€í™”ì—ì„œ ì‚¬ì‹¤ì„ ì¶”ì¶œí•˜ë¼. JSONë§Œ ì¶œë ¥. ìŠ¤í‚¤ë§ˆ:\n{schema_json}"),
        ("user", "[í•€ ê³ ì •(ë³€ê²½ ê¸ˆì§€)]:\n{pinned_json}\n\n[ê³¼ê±° ëŒ€í™”]:\n{text_block}"),
    ]
)

SUMMARY_PROMPT = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë„ˆëŠ” ëŒ€í™” ìš”ì•½ ì‹œìŠ¤í…œì´ë‹¤. ì œê³µëœ ê³¼ê±° ëŒ€í™”ë¥¼ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ì¤‘ì ìœ¼ë¡œ ìµœëŒ€ 300~350 í† í°ë‚´ë¡œ ì••ì¶•/ìš”ì•½í•˜ë¼. "
            "ì‚¬ì‹¤ ì¶”ê°€/ë³€ê²½ ê¸ˆì§€, ì• ë§¤í•˜ë©´ ìƒëµ. í•œêµ­ì–´ ìœ ì§€.",
        ),
        (
            "user",
            "[í•€ ê³ ì •(ì°¸ê³ , ë³€ê²½ ê¸ˆì§€)]:\n{pinned_json}\n\n[ê³¼ê±° ëŒ€í™”]:\n{text_block}\n\nê·œì¹™: {verify_rule}",
        ),
    ]
)

STRUCTURE_SCHEMA = {
    "entities": ["ì´ë¦„/ì¥ì†Œ/ì œí’ˆ/ì¡°ì§"],
    "goals": ["ì‚¬ìš©ì ëª©í‘œ"],
    "tasks": ["í• ì¼/ì•¡ì…˜ì•„ì´í…œ"],
    "deadlines": ["YYYY-MM-DD"],
    "facts": ["ì¤‘ìš” ì‚¬ì‹¤"],
    "decisions": ["ê²°ì •ì‚¬í•­"],
    "constraints": ["ì œì•½/ì„ í˜¸"],
    "references": ["ë§í¬/ì‹ë³„ì"],
}
VERIFY_RULE = "ì ˆëŒ€ ìƒˆë¡œìš´ ì‚¬ì‹¤ì„ ì¶”ê°€í•˜ì§€ ë§ê³ , ì›ë¬¸ì— ì—†ëŠ” ìˆ˜ì¹˜ëŠ” ë„£ì§€ ë§ˆë¼."

# ===== í† í° ìœ í‹¸ =====
enc = tiktoken.get_encoding("cl100k_base")


def _count_tokens_text(txt: str) -> int:
    return len(enc.encode(txt))


def _count_tokens_msgs(msgs) -> int:
    total = 0
    for m in msgs:
        total += len(enc.encode((m.type + ": " + (m.content or ""))))
    return total


def _messages_to_text(msgs) -> str:
    return "\n".join(f"{m.type}: {m.content}" for m in msgs)


def _model_supports_response_format(model_name: str) -> bool:
    try:
        m = (model_name or "").lower()
        return any(k in m for k in ("gpt-4o", "gpt-4.1", "4o", "o3", "o4", "gpt-5"))
    except Exception:
        return False


def _split_for_summary(msgs, recent_budget=RECENT_RAW_TOKENS_BUDGET):
    # ë’¤ì—ì„œë¶€í„° recent_budget ì±„ìš°ê³ , ë‚˜ë¨¸ì§€ëŠ” oldë¡œ
    recent, old = [], []
    remain = recent_budget
    for m in reversed(msgs):
        tk = len(enc.encode(m.content or ""))
        if remain > 0:
            recent.append(m)
            remain -= tk
        else:
            old.append(m)
    recent.reverse()
    old.reverse()
    return old, recent  # ì˜¤ë˜ëœê²ƒë“¤, ìµœê·¼ì›ë¬¸


def _pinned_facts_of(session_id: str) -> List[str]:
    pf = PROFILE_DB.get(session_id, {})
    # êµ¬ì¡° ì˜ˆì¸¡: ì£¼ìš” í‚¤ê°€ goals/constraints/facts ë“±ì¼ ê²ƒ
    out = []
    for k in ("goals", "constraints", "facts", "preferences"):
        v = pf.get(k)
        if isinstance(v, list):
            out.extend([str(x) for x in v])
        elif isinstance(v, str):
            out.append(v)
    return out[:50]


def _sha256(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


# ===== ë©±ë“±í‚¤ ìºì‹œ/ì„¸ì…˜ ìƒíƒœ =====
from typing import Any  # ensure Pydantic-friendly typing

SESSION_STATE: Dict[str, Dict[str, Any]] = {}
IDEMPOTENCY_CACHE: Dict[str, str] = {}  # session_id -> last_hash
SNAPSHOT_QUEUE: asyncio.Queue = asyncio.Queue(maxsize=SNAPSHOT_QUEUE_MAXSIZE)
EMBED_SEM = asyncio.Semaphore(EMBED_CONCURRENCY)


async def _ensure_workers():
    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ë¬´ì‹œ
    if getattr(_ensure_workers, "_started", False):
        return
    _ensure_workers._started = True
    for i in range(WORKER_CONCURRENCY):
        asyncio.create_task(_snapshot_worker(i))


async def _snapshot_worker(worker_id: int):
    while True:
        session_id = await SNAPSHOT_QUEUE.get()
        t0 = time.time()
        try:
            await asyncio.to_thread(update_long_term_memory, session_id)
            took = (time.time() - t0) * 1000
            logger.info(
                "[snapshot:worker-%d] done session=%s took_ms=%.1f",
                worker_id,
                session_id,
                took,
            )
        except Exception as e:
            logger.warning(
                "[snapshot:worker-%d] error session=%s err=%r", worker_id, session_id, e
            )
        finally:
            SNAPSHOT_QUEUE.task_done()


def _enqueue_snapshot(session_id: str):
    try:
        SNAPSHOT_QUEUE.put_nowait(session_id)
        logger.info(
            "[snapshot:q] enqueued session=%s qsize=%d",
            session_id,
            SNAPSHOT_QUEUE.qsize(),
        )
    except asyncio.QueueFull:
        logger.warning("[snapshot:q] queue full -> drop session=%s", session_id)


def _edge_and_debounce(session_id: str, tokens_prev: int, tokens_now: int):
    # ì„¸ì…˜ ìƒíƒœê°€ anchor ë“± ë‹¤ë¥¸ í‚¤ë¡œë§Œ ì´ˆê¸°í™”ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°œë³„ í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ë³´ì •
    st = SESSION_STATE.setdefault(session_id, {})
    if "last_flush_at" not in st:
        st["last_flush_at"] = 0.0
    if "turns_since_last" not in st:
        st["turns_since_last"] = 0
    if "prev_tokens" not in st:
        st["prev_tokens"] = 0
    now = time.time()
    # ë©”ëª¨ë¦¬ ì»´íŒ©ì…˜ ì„ê³„(3000)ëŠ” HybridSummaryMemoryì—ì„œ ì²˜ë¦¬. ì—¬ê¸°ì„œëŠ” ìŠ¤ëƒ…ìƒ· ì ì¬ ì„ê³„ë§Œ ë³¸ë‹¤.
    edge = tokens_prev < SNAPSHOT_EDGE_TOKENS and tokens_now >= SNAPSHOT_EDGE_TOKENS
    elapsed = now - st["last_flush_at"]
    turns_ok = st["turns_since_last"] >= DEBOUNCE_TURNS
    time_ok = elapsed >= DEBOUNCE_SECONDS

    if edge and time_ok and turns_ok:
        _enqueue_snapshot(session_id)
        schedule_directive_update(session_id)
        st["last_flush_at"] = now
        st["turns_since_last"] = 0
    else:
        st["turns_since_last"] += 1
    st["prev_tokens"] = tokens_now


# ====== í•µì‹¬: êµ¬ì¡°í™”+ìš”ì•½ ìƒì„± ======
def _build_structured_and_summary(session_id: str, old_msgs) -> Tuple[str, dict]:
    pinned = _pinned_facts_of(session_id)
    old_text = _messages_to_text(old_msgs)

    schema_str = json.dumps(STRUCTURE_SCHEMA, ensure_ascii=False)
    pinned_str = json.dumps(pinned, ensure_ascii=False)
    text_block = old_text

    # 1) ì¶”ì¶œì  êµ¬ì¡°í™”(JSON)
    t0 = time.time()
    try:
        llm_struct = (
            llm_cold.bind(response_format={"type": "json_object"})
            if _model_supports_response_format(LLM_MODEL)
            else llm_cold
        )
        struct = (STRUCTURE_PROMPT | llm_struct | StrOutputParser()).invoke(
            {
                "schema_json": schema_str,
                "pinned_json": pinned_str,
                "text_block": text_block,
            }
        )
    except Exception as e:
        logger.warning("[summary] structure LLM error: %r", e)
        struct = "{}"
    t1 = (time.time() - t0) * 1000
    logger.info("[summary] structure took_ms=%.1f", t1)

    try:
        struct_json = json.loads(struct)
    except Exception:
        struct_json = {
            k: []
            for k in [
                "entities",
                "goals",
                "tasks",
                "deadlines",
                "facts",
                "decisions",
                "constraints",
                "references",
            ]
        }

    # 2) ìƒì„±ì  ìš”ì•½
    t0 = time.time()
    try:
        summ = (SUMMARY_PROMPT | llm_cold | StrOutputParser()).invoke(
            {
                "pinned_json": pinned_str,
                "text_block": text_block,
                "verify_rule": VERIFY_RULE,
            }
        )
    except Exception as e:
        logger.warning("[summary] generative LLM error: %r", e)
        summ = ""
    t1 = (time.time() - t0) * 1000
    logger.info("[summary] generative took_ms=%.1f", t1)

    combo = (
        "[STRUCTURED]\n"
        + json.dumps(struct_json, ensure_ascii=False)
        + "\n\n[SUMMARY]\n"
        + (summ or "").strip()
    )
    tk = _count_tokens_text(combo)
    if tk > SUMMARY_TARGET_TOKENS + 100:
        combo = enc.decode(enc.encode(combo)[:SUMMARY_TARGET_TOKENS])

    meta = {
        "summary_version": "v1_struct+gen",
        "model": LLM_MODEL,
        "pinned_count": len(pinned),
    }
    return combo, meta


# ====== ì»¤ìŠ¤í…€ ë©”ëª¨ë¦¬: 3000 ì´ˆê³¼ ì‹œ 2000â†’500 ìš”ì•½ êµì²´ ======
class HybridSummaryMemory(ConversationSummaryBufferMemory):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # llm, chat_memory ë“± ê·¸ëŒ€ë¡œ
        self.max_token_limit = MAX_TOKEN_LIMIT

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:
        # ê¸°ë³¸ ì €ì¥ (ì›ë¬¸ ìœ ì§€)
        user_msg = inputs.get(self.input_key, inputs.get("input", ""))
        ai_msg = outputs.get(self.output_key, outputs.get("output", ""))

        ts_iso = _now_kst().isoformat()
        if user_msg:
            self.chat_memory.add_message(
                HumanMessage(content=user_msg, additional_kwargs={"ts": ts_iso})
            )
        if ai_msg:
            self.chat_memory.add_message(
                AIMessage(content=ai_msg, additional_kwargs={"ts": ts_iso})
            )

        # 3000 ì´ˆê³¼ ì‹œ: ì˜¤ë˜ëœ êµ¬ê°„ì„ 500 í† í°ìœ¼ë¡œ ìš”ì•½í•˜ì—¬ êµì²´(append ì•„ë‹˜), ìµœê·¼ ì›ë¬¸ì€ ìœ ì§€
        msgs = self.chat_memory.messages
        total_tokens = _count_tokens_msgs(msgs)
        if total_tokens <= self.max_token_limit:
            return

        # ë¶„ë¦¬: ì˜¤ë˜ëœ(old) / ìµœê·¼(recent)
        old_msgs, recent_msgs = _split_for_summary(msgs, RECENT_RAW_TOKENS_BUDGET)
        if not old_msgs:
            return  # ìµœê·¼ë§Œìœ¼ë¡œë„ 3000 ë„˜ëŠ” ê²½ìš°(ë“œë­„) -> ê±´ë„ˆëœ€

        # ì¬ìš”ì•½ ë°©ì§€: oldê°€ ë‹¨ì¼ ìš”ì•½ ë¸”ë¡([SUMMARIZED@...])ë§Œ í¬í•¨í•˜ë©´ ìŠ¤í‚µ
        try:
            old_text_flat = "\n".join(getattr(m, "content", "") for m in old_msgs)
            if (
                old_text_flat.strip().startswith("[SUMMARIZED@")
                and "[SUMMARY]" in old_text_flat
            ):
                logger.info("[redis] skip re-summarization (already summarized block)")
                return
        except Exception:
            pass

        # ìš”ì•½ ìƒì„±/ì¬êµ¬ì„±ì€ ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ì„ í”¼í•˜ê¸° ìœ„í•´ ë°±ê·¸ë¼ìš´ë“œë¡œ ì²˜ë¦¬
        async def _compact_async(
            session_id_local: str, old_msgs_local, recent_msgs_local
        ):
            combo_local, meta_local = _build_structured_and_summary(
                session_id_local, old_msgs_local
            )
            try:
                self.chat_memory.clear()
                stamp = _now_kst().isoformat()
                header = f"[SUMMARIZED@{stamp}] tokens~{SUMMARY_TARGET_TOKENS} | {meta_local['summary_version']} | model={meta_local['model']}"
                self.chat_memory.add_message(
                    AIMessage(
                        content=header + "\n\n" + combo_local,
                        additional_kwargs={"ts": stamp},
                    )
                )
                for m in recent_msgs_local:
                    kwargs = getattr(m, "additional_kwargs", {}) or {}
                    if m.type == "human":
                        self.chat_memory.add_message(
                            HumanMessage(content=m.content, additional_kwargs=kwargs)
                        )
                    else:
                        self.chat_memory.add_message(
                            AIMessage(content=m.content, additional_kwargs=kwargs)
                        )
                logger.info(
                    "[redis] compacted(async): old->summary(â‰ˆ%d tok), kept recent(rawâ‰ˆ%d tok)",
                    SUMMARY_TARGET_TOKENS,
                    RECENT_RAW_TOKENS_BUDGET,
                )
            except Exception as e:
                logger.warning("[redis] compact error(async): %r", e)

            # ìŠ¤ëƒ…ìƒ· íŒŒì´í”„ë¼ì¸ì— ì¦‰ì‹œ ë¶„ê¸°(ë™ì¼ ë©±ë“±/ì¤‘ë³µ ê²Œì´íŠ¸ëŠ” update_long_term_memoryì—ì„œ ìˆ˜í–‰)
            try:
                if session_id_local and isinstance(session_id_local, str):
                    _enqueue_snapshot(session_id_local)
            except Exception:
                pass

        try:
            session_id = getattr(self.chat_memory, "session_id", None) or "unknown"
            asyncio.create_task(
                _compact_async(session_id, list(old_msgs), list(recent_msgs))
            )
        except Exception as e:
            logger.warning("[redis] schedule compact error: %r", e)


# ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€: ë‚´ë¶€ êµ¬í˜„ë§Œ ì»¤ìŠ¤í…€ ë©”ëª¨ë¦¬ë¡œ êµì²´


def get_short_term_memory(session_id: str) -> ConversationSummaryBufferMemory:
    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)
    # pinned facts ë³´í˜¸: ìš”ì•½ LLMì€ llm_cold ì‚¬ìš©, max_token_limit=3000
    return HybridSummaryMemory(
        llm=llm_cold,
        chat_memory=redis_hist,
        max_token_limit=MAX_TOKEN_LIMIT,
        return_messages=True,
        memory_key="chat_history",
    )


# ----------------------------------------------------------------------
# ì…€ 5: RAG ê²€ìƒ‰ ë° ë‹¨ê¸° ê¸°ì–µ ì„¤ì •(ê·¸ëŒ€ë¡œ)
# ----------------------------------------------------------------------

METRIC = os.getenv("MILVUS_METRIC", "COSINE").upper()  # "COSINE" ê¶Œì¥


def _msg_ts_dt(m) -> Optional[datetime]:
    try:
        ts = getattr(m, "additional_kwargs", {}).get("ts")
        if not ts:
            return None
        # ISO ë¬¸ìì—´ â†’ datetime (tz í¬í•¨)
        dt = datetime.fromisoformat(ts)
        # KSTë¡œ ì •ê·œí™”
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        else:
            dt = dt.astimezone(KST)
        return dt
    except Exception:
        return None


def _extract_ts_bounds(
    msgs, fallback_now: Optional[datetime] = None
) -> Tuple[datetime, datetime]:
    fallback_now = fallback_now or _now_kst()
    dts = [_msg_ts_dt(m) for m in msgs]
    dts = [d for d in dts if d is not None]
    if not dts:
        # ê³¼ê±° ë©”ì‹œì§€ì— tsê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•œ í´ë°±
        return fallback_now, fallback_now
    return min(dts), max(dts)


def _hit_similarity(hit) -> float:
    # backend.rag.utils.hit_similarityë¥¼ ì§ì ‘ ì‚¬ìš©(í˜¸í™˜ ìœ ì§€ìš© ë˜í¼)
    try:
        return __import__(
            "backend.rag.utils", fromlist=["hit_similarity"]
        ).hit_similarity(hit)
    except Exception:
        d = getattr(hit, "distance", None)
        s = getattr(hit, "score", None)
        if METRIC == "IP":
            return float(d if d is not None else s)
        elif METRIC == "COSINE":
            dist = float(d if d is not None else 1.0)
            return 1.0 - dist
        else:
            return -float(d if d is not None else 1e9)


def _milvus_hits_to_ctx(hits, score_min: float = 0.45, top_k: int = 3) -> str:
    if not hits:
        return ""
    picked = []
    for hit in hits[:top_k]:
        sim = _hit_similarity(hit)
        logger.info(
            f"[rag] id={getattr(hit,'id',None)} sim={sim:.3f} dist={getattr(hit,'distance',None)}"
        )
        if sim >= score_min:
            picked.append(hit.entity.get("text"))
    return "\n".join(p for p in picked if p)


def retrieve_from_rag(
    session_id: str,
    query: str,
    top_k: int = 2,
    date_filter: Optional[Tuple[int, int]] = None,
) -> str:
    # í˜¸í™˜: rag íŒ¨í‚¤ì§€ í•¨ìˆ˜ë¡œ ìœ„ì„
    return __import__("backend.rag", fromlist=["retrieve_from_rag"]).retrieve_from_rag(
        session_id, query, top_k, date_filter
    )


# ----------------------------------------------------------------------
# ì…€ 6.5: ê·œì¹™ ê¸°ë°˜ ì½”ì–´í”„ + ì¬ì‘ì„± (WEB/RAG)
# ----------------------------------------------------------------------
def _last_user_utterance(hist: str) -> str:
    for line in reversed(hist.splitlines()):
        if line.lower().startswith(("human:", "user:")):
            return line.split(":", 1)[1].strip()
    return ""


def _extract_topic_np(text: str) -> str:
    t = re.sub(r"[\(\)\[\]{}]", " ", text)
    t = re.sub(r"\s+", " ", t).strip()
    chunks = re.split(
        r"(?:ì— ëŒ€í•´ì„œ|ì— ê´€í•´|ì— ëŒ€í•œ|ì˜€ë˜|ì´ì—ˆë˜|ê´€ë ¨|ì´ì•¼ê¸°í•œ|ëŒ€í•´ì„œ|ì—|ì—ì„œ|ìœ¼ë¡œ|ì—ê²Œ|ì™€|ê³¼|ì˜|ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼)",
        t,
    )
    cands = [c.strip() for c in chunks if c and len(c.strip()) >= 2]
    cands = [
        c
        for c in cands
        if not re.fullmatch(r"[0-9\W_]+", c)
        and c not in {"ê·¸ê±°", "ì´ê±°", "ì €ë²ˆ", "ì§€ë‚œë²ˆ", "ê·¸ë‚ ", "ê·¸ë•Œ"}
    ]

    def score(s):
        return (len(re.findall(r"[ê°€-í£]", s)), len(s))

    return max(cands, key=score, default="")


def _shallow_coref(txt: str, hist: str) -> str:
    out = txt
    last = _last_user_utterance(hist)
    topic = _extract_topic_np(last) if last else ""
    if topic:
        out = re.sub(r"\b(ê·¸ê±°|ì´ê±°|ê·¸ë‚ |ê·¸ë•Œ|ì €ë²ˆì—|ì§€ë‚œë²ˆì—)\b", topic, out)
        if re.search(r"\bìš°ë¦¬\s*ëŒ€í™”\b", out):
            out = out.replace("ìš°ë¦¬ ëŒ€í™”", f"ìš°ë¦¬ '{topic}' ëŒ€í™”")
    return re.sub(r"\s+", " ", out).strip()


RAG_REWRITE_SYS = (
    "ë„ˆëŠ” ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë„ìš°ë¯¸ë‹¤. ì¿¼ë¦¬ì—ì„œ ì„ì˜ë¡œ ë‚´ìš©ì„ ì¶”ê°€/ì¶”ì¸¡í•˜ì§€ ë§ˆë¼. "
    "ëª¨í˜¸í•œ ì§€ì‹œì–´ë‚˜ ìƒëŒ€ì‹œì œëŠ” ëª…í™•í•œ ì ˆëŒ€ ë‚ ì§œ ë²”ìœ„(YYYYMMDD~YYYYMMDD)ë¡œ í•´ì„í•˜ë˜, "
    "ìµœì¢… ì¶œë ¥ ë¬¸ì¥ì—ëŠ” ë‚ ì§œ í‘œí˜„ì„ ë„£ì§€ ë§ê³  RAG DB ê²€ìƒ‰ì— íš¨ê³¼ì ì¸ í•µì‹¬ ì£¼ì œ/í‚¤ì›Œë“œë§Œ ë‚¨ê²¨ë¼. "
)

WEB_REWRITE_SYS = (
    "ë„ˆëŠ” ì›¹ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… ë„ìš°ë¯¸ë‹¤. ì¿¼ë¦¬ì—ì„œ ì„ì˜ë¡œ ë‚´ìš©ì„ ì¶”ê°€/ì¶”ì¸¡í•˜ì§€ ë§ˆë¼."
    "ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë³´ê³  ë‰´ìŠ¤/ì›¹ë¬¸ì„œ/ë¡œì»¬ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ë‚¨ê²¨ë¼."
    "ë§ˆì¹¨í‘œ/ì‰¼í‘œ ë“± êµ¬ë‘ì  ê¸ˆì§€."
)


def _rewrite_prompt(task: str, q_rules: str, hints: str | None = None) -> list[dict]:
    sys = RAG_REWRITE_SYS if task == "rag" else WEB_REWRITE_SYS
    hint_block = f"\nì°¸ê³ (ì¶”ê°€ ê¸ˆì§€): {hints}" if hints else ""
    return [
        {"role": "system", "content": sys},
        {
            "role": "user",
            "content": f"ì…ë ¥: {q_rules}{hint_block}\nì¶œë ¥: ì…ë ¥ì˜ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ë©° ì¬ì‘ì„±ëœ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ë¼.",
        },
    ]


# ---- íœ´ë¦¬ìŠ¤í‹±: ì†Œê·œëª¨ ì¸ì‚¬/ì¡ë‹´ ê°ì§€ ë° ì›¹ê²€ìƒ‰ í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬ ----
_SMALL_TALK_PAT = re.compile(
    r"^(ì•ˆë…•|í•˜ì´|í•˜ì´ìš”|í—¬ë¡œ|hello|hi|ë°˜ê°€ì›Œ|ë°˜ê°‘|ê³ ë§ˆì›Œ|ê³ ë§™ë‹¤|ê°ì‚¬|ë•¡í|ã…ã…|ã…‹ã…‹|ã…ã…‡|ì‘|ì›…|ì•¼|í—¬ë¡œìš°)\b",
    re.IGNORECASE,
)
_GENERIC_CHITCHAT_PAT = re.compile(r"(ë­í•˜ë‹ˆ|ë­í•´|ë¨¸í•´|ë­í•¨|ë­í•˜ê³ |ë­í• |ì–´ë•Œ|ì–´ë– ë‹ˆ)\b")


def _is_small_talk(text: str) -> bool:
    t = re.sub(r"\s+", " ", text or "").strip()
    if not t:
        return True
    if _SMALL_TALK_PAT.search(t) is not None:
        return True
    # ì§§ì€ ì¼ë°˜ ëŒ€í™”ë¬¸ì˜ ê²½ìš°(ì§ˆë¬¸í˜•ì´ì§€ë§Œ ëª©ì  ë¶ˆëª…í™•)ë„ ì†Œí†µìœ¼ë¡œ ê°„ì£¼
    if len(t) <= 20 and _GENERIC_CHITCHAT_PAT.search(t):
        return True
    return False


def _is_valid_web_query(q: str) -> bool:
    if not q:
        return False
    # êµ¬ë‘ì /ë¬¼ìŒí‘œ í¬í•¨ ë˜ëŠ” ë‹¨ì–´ ìˆ˜ 2 ë¯¸ë§Œì´ë©´ ê²€ìƒ‰ì„± ë‚®ë‹¤ê³  íŒë‹¨
    if re.search(r"[\.,!?]", q):
        return False
    if len(q.split()) < 2:
        return False
    return True


def _is_local_search_intent(text: str) -> bool:
    # í‚¤ì›Œë“œ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ(í”„ë¡œë•ì…˜ ë¹„ê¶Œì¥).
    # ì›¹/ë¡œì»¬ íƒìƒ‰ ì˜ë„ëŠ” ì†Œë¶„ë¥˜ê¸° ë° LLM ê°€ë“œë¡œ íŒë‹¨.
    return False


def _looks_like_web_intent(text: str) -> bool:
    # í‚¤ì›Œë“œ/ì •ê·œì‹ íœ´ë¦¬ìŠ¤í‹± ì—†ì´ ë¼ìš°í„°/ì†Œë¶„ë¥˜ê¸°/LLMìœ¼ë¡œ íŒë‹¨
    return False


# ----------------------------------------------------------------------
# ì…€ 6.6: ë‹´í™” ì•µì»¤(ì¥ì†Œ/ì£¼ì œ) Â· íŒ”ë¡œì—… íƒì§€ Â· ì„ë² ë”© íŒíŠ¸ ì„ íƒ
# ----------------------------------------------------------------------


def _get_anchor_state(session_id: str) -> dict:
    st = SESSION_STATE.setdefault(session_id, {})
    anchor = st.setdefault("anchor", {"place": "", "topic": "", "ts": 0.0})
    # TTL ì ìš©
    now_ts = time.time()
    try:
        last_ts = float(anchor.get("ts", 0.0))
    except Exception:
        last_ts = 0.0
    if last_ts and (now_ts - last_ts) > TOPIC_TTL_S:
        anchor = {"place": "", "topic": "", "ts": 0.0}
        st["anchor"] = anchor
    return anchor


def _update_anchor_state(session_id: str, place: str | None, topic: str | None):
    try:
        anchor = _get_anchor_state(session_id)
        changed = False
        if isinstance(place, str) and place.strip():
            anchor["place"] = place.strip()
            changed = True
        if isinstance(topic, str) and topic.strip():
            anchor["topic"] = topic.strip()
            changed = True
        if changed:
            anchor["ts"] = time.time()
    except Exception:
        pass


async def _extract_anchors(user_input: str, hist_tail: str) -> dict:
    """LLMìœ¼ë¡œ í˜„ì¬ ì…ë ¥/ìµœê·¼ ë§¥ë½ì—ì„œ ì¥ì†Œ/ì£¼ì œ ì•µì»¤ë¥¼ ê²½ëŸ‰ ì¶”ì¶œí•œë‹¤."""
    schema = {
        "type": "object",
        "properties": {
            "place": {"type": "string"},
            "topic": {"type": "string"},
        },
        "required": [],
        "additionalProperties": False,
    }
    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ë‹´í™” ì•µì»¤ ì¶”ì¶œê¸°ë‹¤. ì…ë ¥ê³¼ ìµœê·¼ ë§¥ë½ì—ì„œ ì¥ì†Œëª…(ì˜ˆ: ìƒë´‰ì—­)ê³¼ í•µì‹¬ ì£¼ì œ(ì˜ˆ: ë””ì €íŠ¸ ì¹´í˜)ë¥¼ ì¶”ì¶œí•˜ë¼."
                " ëª¨í˜¸í•˜ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒë ¤ë¼. JSONë§Œ: {place?:string, topic?:string}."
            ),
        },
        {"role": "user", "content": f"[ì…ë ¥]\n{user_input}\n\n[ìµœê·¼]\n{hist_tail}"},
    ]
    try:
        kwargs = {
            "model": LLM_MODEL,
            "messages": msgs,
            "max_tokens": 80,
            "temperature": 0.0,
        }
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_object"}
        resp = await asyncio.wait_for(
            openai_chat_with_retry(**kwargs), timeout=EXTRACT_TIMEOUT_S
        )
        content = (resp.choices[0].message.content or "").strip()
        data = json.loads(content) if content.startswith("{") else {}
        place = (data.get("place") or "").strip()
        topic = (data.get("topic") or "").strip()
        return {"place": place, "topic": topic}
    except Exception:
        return {"place": "", "topic": ""}


def _build_hints_by_embedding(
    session_id: str,
    hist_msgs,
    query_text: str,
    lookback: int = HINT_LOOKBACK,
    max_items: int = HINT_MAX_ITEMS,
    sim_thr: float = HINT_SIM_THRESHOLD,
) -> str:
    """ìµœê·¼ ì‚¬ìš©ì ë°œí™”ì—ì„œ ì„ë² ë”© ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ 1~2ê°œë§Œ ì„ íƒí•˜ì—¬ íŒíŠ¸ë¡œ ì‚¬ìš©í•œë‹¤.
    - API í˜¸ì¶œ ìµœì í™”: OpenAIEmbeddings.embed_documentsë¡œ ë°°ì¹˜ ì„ë² ë”©
    - ì„¸ì…˜ ë‹¨ìœ„ ìºì‹œ: SESSION_STATE[session_id]["hint_vecs"] (í…ìŠ¤íŠ¸ í•´ì‹œâ†’ë²¡í„°)
    """
    try:
        # ìµœê·¼ì—ì„œ ì‚¬ìš©ì(human) ë°œí™”ë§Œ ìˆ˜ì§‘
        lines = []
        for m in reversed(hist_msgs):
            if getattr(m, "type", "") == "human":
                txt = (getattr(m, "content", "") or "").strip()
                if txt:
                    lines.append(txt)
            if len(lines) >= lookback:
                break
        lines = list(reversed(lines))
        if not lines:
            return ""

        # ì„¸ì…˜ ìºì‹œ ì¤€ë¹„
        st = SESSION_STATE.setdefault(session_id, {})
        hint_cache: dict = st.setdefault("hint_vecs", {})

        # ì¿¼ë¦¬ ì„ë² ë”© (ë‹¨ì¼, ìºì‹œ ì‚¬ìš©)
        qv = embed_query_cached(query_text)

        # ìºì‹œì— ì—†ëŠ” ì¤„ë§Œ ë°°ì¹˜ ì„ë² ë”© ì‹œë„
        missing = []
        idx_map = []
        for i, ln in enumerate(lines):
            key = _sha256(ln)
            if key not in hint_cache:
                missing.append(ln)
                idx_map.append((i, key))

        if missing:
            try:
                vecs = embed_docs(missing)
                for (i, key), vec in zip(idx_map, vecs):
                    hint_cache[key] = np.array(vec, dtype=np.float32)
            except Exception as _e:
                # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ë¼ì¸ ë‹¨ìœ„ í´ë°± (ì„ë² ë”© ìºì‹œ ì¬ì‚¬ìš©)
                for i, key in idx_map:
                    hint_cache[key] = embed_query_cached(lines[i])

        # ìµœì‹  lookback ìœˆë„ìš° ë‚´ í‚¤ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ í†µì œ)
        keep_keys = set(_sha256(ln) for ln in lines)
        if len(hint_cache) > len(keep_keys):
            for k in list(hint_cache.keys()):
                if k not in keep_keys:
                    hint_cache.pop(k, None)

        # ìœ ì‚¬ë„ ìŠ¤ì½”ì–´ë§
        scored: list[tuple[float, str]] = []
        for ln in lines:
            lv = hint_cache.get(_sha256(ln))
            if lv is None:
                lv = embed_query_cached(ln)
            denom = float(np.linalg.norm(qv) * np.linalg.norm(lv)) or 1.0
            sim = float(np.dot(qv, lv) / denom)
            if sim >= sim_thr:
                scored.append((sim, ln))

        if not scored:
            return ""
        scored.sort(key=lambda x: -x[0])
        picked = [s for _, s in scored[:max_items]]
        hint = " ".join(picked)
        return hint[:200]
    except Exception:
        return ""


async def _clarify_for_anchors(
    session_id: str, user_input: str, hist_tail: str, anchors: dict
) -> str:
    """ì¥ì†Œ/ì£¼ì œ ì•µì»¤ê°€ ë¹„ê±°ë‚˜ ë¶ˆëª…í™•í•  ë•Œ LLMìœ¼ë¡œ ì§§ì€ ëª…í™•í™” ì§ˆë¬¸ì„ ìƒì„±í•œë‹¤.
    - ë‘˜ ë‹¤ ì¶©ë¶„í•˜ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    - JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ
    """
    place = (anchors.get("place") or "").strip()
    topic = (anchors.get("topic") or "").strip()
    if place or topic:
        return ""
    # ì„¸ì…˜ ìƒíƒœ ê¸°ë°˜ íŒíŠ¸ êµ¬ì„±: ì €ì¥ ì•µì»¤ + STWM ìŠ¤ëƒ…ìƒ·(êµ¬ì¡°í™”ëœ ìƒíƒœë§Œ)
    try:
        persisted = _get_anchor_state(session_id)
    except Exception:
        persisted = {"place": "", "topic": "", "ts": 0.0}
    try:
        stwm_now = get_stwm_snapshot(session_id)
    except Exception:
        stwm_now = {
            "last_loc": "",
            "last_topic": "",
        }
    hint_place = (persisted.get("place") or stwm_now.get("last_loc") or "").strip()
    hint_topic = (persisted.get("topic") or stwm_now.get("last_topic") or "").strip()
    hint = (f"ì¥ì†Œ={hint_place} | ì£¼ì œ={hint_topic}").strip(" | ")

    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ëª…í™•í™” ì—ì´ì „íŠ¸ë‹¤. ì•„ë˜ íŒíŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ ë§¥ë½ì„ ìœ ì§€í•œ êµ¬ì²´ ì§ˆë¬¸ 1ë¬¸ì¥ë§Œ í•œêµ­ì–´ë¡œ ìƒì„±í•˜ê³ ,"
                " íŒíŠ¸ê°€ ì¶©ë¶„í•˜ê±°ë‚˜ ëª¨í˜¸í•˜ì§€ ì•Šë‹¤ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ë¼."
                " ì§ˆë¬¸ì€ í•œ ë¬¸ì¥, ê³µì†ì²´, ë©”íƒ€/ê´‘ê³ /ë§í¬ ê¸ˆì§€. JSONë§Œ: {clarify:string}."
                f" [íŒíŠ¸] {hint}"
            ),
        },
        {
            "role": "user",
            "content": f"[ì…ë ¥]\n{user_input}\n\n[ìµœê·¼]\n{hist_tail}",
        },
    ]
    try:
        kwargs = {
            "model": LLM_MODEL,
            "messages": msgs,
            "max_tokens": 60,
            "temperature": 0.0,
        }
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_object"}
        resp = await asyncio.wait_for(
            openai_chat_with_retry(**kwargs), timeout=FOLLOWUP_TIMEOUT_S
        )
        content = (resp.choices[0].message.content or "").strip()
        data = json.loads(content) if content.startswith("{") else {}
        clarify = (data.get("clarify") or "").strip()
        return clarify
    except Exception:
        return ""


async def rewrite_query(
    task: str,
    user_input: str,
    hist: str,
    anchor_hint: str | None = None,
    session_id: str | None = None,
) -> Dict[str, str | Tuple[int, int] | None]:
    """
    ë°˜í™˜:
      - RAG: {"query_text": <ë‚ ì§œ ì œê±° í…ìŠ¤íŠ¸>, "date_filter": (start_ymd,end_ymd)}
      - WEB: {"web_query": <í‚¤ì›Œë“œ ë‚˜ì—´(2~6ê°œ) + (ì˜µì…˜)YYYYë…„ Mì›”>}
    """
    base = _shallow_coref(user_input, hist)

    if task == "rag":
        date_range = _extract_date_range_for_rag(base)
        q_rules = base
        # íŒíŠ¸: ìµœê·¼ ëŒ€í™” ì¤‘ í•´ë‹¹ ì£¼ì œ ê´€ë ¨ í•µì‹¬ ë¼ì¸ ì¼ë¶€ë§Œ ìš”ì•½í•´ ì‚¬ìš© (ì˜¤ì—¼ ìµœì†Œí™”)
        hints = ""
        try:
            msgs = hist.splitlines()[-20:]
            topic = _extract_topic_np(base)
            hints = " ".join([m for m in msgs if topic and topic in m])[:200]
        except Exception:
            hints = ""

        t0 = time.time()
        out = await _rewrite_with_retries(
            _rewrite_prompt("rag", q_rules, hints if hints else None),
            base_timeout_s=REWRITE_TIMEOUT_S,
            attempts=1,
            delta_s=1.0,
            max_tokens=REWRITE_MAX_TOKENS,
        )
        query_text = out or q_rules
        logger.info(
            "[rewrite:RAG] base='%s' -> out='%s' took_ms=%.1f",
            q_rules[:80],
            query_text[:80],
            (time.time() - t0) * 1000,
        )
        return {"query_text": query_text, "date_filter": date_range}

    # ---- WEB ì¼€ì´ìŠ¤: LLMë§Œìœ¼ë¡œ í‚¤ì›Œë“œ ê°•ì œ ----
    ym_range = _month_tokens_for_web(base)
    q_rules = base
    web_query = q_rules

    # ë™ì  ì¬ì‘ì„± íƒ€ì„ì•„ì›ƒ(ì›¹ íƒ€ì„ì•„ì›ƒì— ë¹„ë¡€)
    RW_TIMEOUT = min(max(REWRITE_TIMEOUT_S, TIMEOUT_WEB * 0.6), 1.8)

    # 1ì°¨ ì‹œë„: JSON ìŠ¤í‚¤ë§ˆë¡œ í˜•ì‹ ê°•ì œ
    schema = {
        "name": "KeywordQuery",
        "schema": {
            "type": "object",
            "properties": {
                "q": {
                    "type": "string",
                    "description": "ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ 2~6ê°œì˜ í•œêµ­ì–´ í•µì‹¬ í‚¤ì›Œë“œ (ë¬¸ì¥/êµ¬ë‘ì  ê¸ˆì§€)",
                }
            },
            "required": ["q"],
            "additionalProperties": False,
        },
    }
    # íŒíŠ¸: ìµœê·¼ ëŒ€í™”ì—ì„œ ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œ ë¼ì¸ë§Œ ì„ íƒ(ì˜¤ì—¼ ë°©ì§€) + ì•µì»¤ íŒíŠ¸ ë³‘í•©
    hints = ""
    try:
        # histëŠ” í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ì„ì‹œ ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ êµ¬ì„±
        class _Msg:
            def __init__(self, t, c):
                self.type = t
                self.content = c

        hist_msgs = []
        for ln in hist.splitlines()[-HINT_LOOKBACK:]:
            if ln.startswith("human:") or ln.startswith("Human:"):
                hist_msgs.append(_Msg("human", ln.split(":", 1)[1].strip()))
        # ì„¸ì…˜ ìºì‹œ ìµœì í™”ë¥¼ ìœ„í•´ ì „ë‹¬ë°›ì€ session_id ì‚¬ìš© (ì—†ìœ¼ë©´ default)
        try:
            emb_hints = _build_hints_by_embedding(
                session_id or "default", hist_msgs, base
            )
        except Exception:
            emb_hints = _build_hints_by_embedding("default", hist_msgs, base)
        hints = ((anchor_hint or "") + " " + (emb_hints or "")).strip()
    except Exception:
        hints = ""

    # 1) JSON ìŠ¤í‚¤ë§ˆ ë°©ì‹ ì¬ì‘ì„± + íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ (response_format ê°•ì œ)
    t0 = time.time()
    hint_suffix = ("\nì°¸ê³ (ì¶”ê°€ ê¸ˆì§€): " + hints) if hints else ""
    user_content_json = f"ì…ë ¥: {q_rules}{hint_suffix}\nì¶œë ¥: ë¬¸ì¥ ê¸ˆì§€, 2~6ê°œ í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ."
    out_json = await _rewrite_with_retries(
        [
            {"role": "system", "content": WEB_REWRITE_SYS},
            {"role": "user", "content": user_content_json},
        ],
        base_timeout_s=RW_TIMEOUT,
        attempts=1,
        delta_s=1.0,
        max_tokens=REWRITE_MAX_TOKENS,
        response_format={"type": "json_schema", "json_schema": schema},
    )
    if out_json:
        try:
            data = json.loads(out_json)
            cand = (data.get("q") or "").strip()
            if 2 <= len(cand.split()) <= 6 and not re.search(r"[.,!?]", cand):
                web_query = cand
        except Exception:
            pass
    if not web_query:
        # 2) í”„ë¦¬í¼ ë°±ì—… + ì¬ì‹œë„
        user_content_free = f"ì…ë ¥: {q_rules}{hint_suffix}\nì¶œë ¥: ë¬¸ì¥ ê¸ˆì§€, 2~6ê°œ í‚¤ì›Œë“œë§Œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ í•œ ì¤„ë¡œ."
        out2 = await _rewrite_with_retries(
            [
                {"role": "system", "content": WEB_REWRITE_SYS},
                {"role": "user", "content": user_content_free},
            ],
            base_timeout_s=min(TIMEOUT_WEB * 0.8, 2.2),
            attempts=1,
            delta_s=1.0,
            max_tokens=REWRITE_MAX_TOKENS,
        )
        if out2 and 2 <= len(out2.split()) <= 6 and not re.search(r"[.,!?]", out2):
            web_query = out2
    logger.info(
        "[rewrite:WEB] base='%s' -> out='%s' took_ms=%.1f",
        q_rules[:80],
        (web_query or q_rules)[:80],
        (time.time() - t0) * 1000,
    )

    # ì—°-ì›” í† í° ì£¼ì…(ìˆìœ¼ë©´) â€” ë¡œì»¬ê²€ìƒ‰/ë‰´ìŠ¤ í˜¼ìš©ì„ ê³ ë ¤í•˜ì—¬ ìœ ì§€
    # ë¡œì»¬ ê²€ìƒ‰ì€ ì—°-ì›” í† í°ì„ ë¶™ì´ë©´ ê²€ìƒ‰ í’ˆì§ˆì´ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆì–´ ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
    # í•„ìš” ì‹œ ë‰´ìŠ¤/ì‹œí™©ì„± ì§ˆì˜ì— í•œí•´ ë³„ë„ ê·œì¹™ìœ¼ë¡œ ì¬ë„ì… ê°€ëŠ¥

    # í›„ì²˜ë¦¬: ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    web_query = re.sub(r"\s+", " ", web_query).strip()
    return {"web_query": web_query}


# ----------------------------------------------------------------------
# ì…€ 7: Naver Search Chain
# ----------------------------------------------------------------------
async def naver_search(query: str, display: int = 5) -> dict:
    """(í˜¸í™˜) MCP ì„œë²„ë¥¼ í†µí•´ ë„¤ì´ë²„ ê²€ìƒ‰ í˜¸ì¶œ. service ëª¨ë“ˆ ì‚¬ìš© ê¶Œì¥."""
    kind, ctx = await build_web_context(MCP_SERVER_URL, query, display, TIMEOUT_WEB)
    return {"kind": kind, "data": {"items": []}, "ctx": ctx}


async def search_web(query: str) -> str:
    kind, ctx = await build_web_context(MCP_SERVER_URL, query, 2, TIMEOUT_WEB)
    logger.info(f"[web:ctx] kind={kind} ctx_len={len(ctx)}")
    return ctx


# ----------------------------------------------------------------------
# ì…€ 6.8: ëª¨ë°”ì¼ ì»¨í…ìŠ¤íŠ¸(Firestore) ì¡°íšŒ ë° ìš”ì•½
# ----------------------------------------------------------------------
def _kst_day_bounds(now: Optional[datetime] = None) -> tuple[datetime, datetime]:
    now = now or _now_kst()
    start = datetime(now.year, now.month, now.day, tzinfo=KST)
    end = start + timedelta(days=1)
    return start, end


def _to_utc(dt: datetime) -> datetime:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST)
    return dt.astimezone(timezone.utc)


def _safe_parse_iso(dt_str: str) -> Optional[datetime]:
    if not dt_str:
        return None
    s = dt_str.strip()
    try:
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


def _fmt_hm_kst(dt: datetime) -> str:
    try:
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        dt_kst = dt.astimezone(KST)
        return dt_kst.strftime("%H:%M")
    except Exception:
        return ""


def _build_mobile_ctx_sync(user_id: str) -> str:
    db = _ensure_fs_db()
    if not db:
        return ""
    try:
        kst_start, kst_end = _kst_day_bounds()
        start_utc = _to_utc(kst_start)
        end_utc = _to_utc(kst_end)

        q = (
            db.collection(FIRESTORE_USERS_COLL)
            .document(user_id)
            .collection(FIRESTORE_EVENTS_SUB)
            .where("recordTimestamp", ">=", start_utc)
            .where("recordTimestamp", "<", end_utc)
            .order_by("recordTimestamp", direction=gcf.Query.DESCENDING)
            .limit(200)
        )
        docs = list(q.stream())
        if not docs:
            return ""
        events = [d.to_dict() for d in docs]

        # ìµœê·¼ ìœ„ì¹˜ 1ê±´
        latest_loc = None
        for e in events:
            if (e.get("dataType") or "").upper() == "LOCATION":
                latest_loc = e
                break

        # ì˜¤ëŠ˜ ì¼ì • ì¶”ì¶œ
        today_events: list[dict] = []
        for e in events:
            if (e.get("dataType") or "").upper() != "CALENDAR_UPDATE":
                continue
            payload = e.get("payload", {}) or {}
            for ev in payload.get("events", []) or []:
                st_raw = ev.get("startTime")
                if not st_raw:
                    continue
                st_dt = _safe_parse_iso(st_raw)
                if not st_dt:
                    continue
                # KST ê¸°ì¤€ ì˜¤ëŠ˜ ë²”ìœ„ ë‚´ ì—¬ë¶€
                st_kst = st_dt if st_dt.tzinfo else st_dt.replace(tzinfo=timezone.utc)
                st_kst = st_kst.astimezone(KST)
                if kst_start <= st_kst < kst_end:
                    today_events.append(ev)

        # ì¼ì • í¬ë§· (ìµœëŒ€ 5ê°œ)
        lines_cal = []
        if today_events:
            # ê°€ì¥ ì´ë¥¸ ì‹œì‘ì‹œê°„ ìˆœ ì •ë ¬
            def _key(ev):
                dt = _safe_parse_iso(ev.get("startTime") or "") or _now_kst()
                return dt

            today_events.sort(key=_key)
            for ev in today_events[:5]:
                st = _safe_parse_iso(ev.get("startTime") or "")
                hm = _fmt_hm_kst(st) if st else ""
                title = (ev.get("title") or "").strip() or "(ì œëª© ì—†ìŒ)"
                loc = (ev.get("location") or "").strip()
                if loc:
                    lines_cal.append(f"- {hm} {title} @ {loc}")
                else:
                    lines_cal.append(f"- {hm} {title}")

        # ìœ„ì¹˜ í¬ë§·
        line_loc = ""
        if latest_loc:
            p = latest_loc.get("payload", {}) or {}
            addr = (p.get("address") or "").strip()
            if addr:
                line_loc = f"í˜„ì¬ ìœ„ì¹˜: {addr}"
            else:
                lat = p.get("latitude")
                lng = p.get("longitude")
                if lat is not None and lng is not None:
                    line_loc = f"í˜„ì¬ ìœ„ì¹˜: ({lat:.5f}, {lng:.5f})"

        blocks = []
        if lines_cal:
            blocks.append("[ì˜¤ëŠ˜ ì¼ì •]\n" + "\n".join(lines_cal))
        if line_loc:
            blocks.append("[í˜„ì¬ ìœ„ì¹˜]\n" + line_loc)
        return "\n\n".join(blocks)
    except Exception as e:
        logger.warning("[mobile] fetch error: %r", e)
        return ""


async def build_mobile_ctx(user_id: str) -> str:
    # Firestore SDKëŠ” ë™ê¸° í´ë¼ì´ì–¸íŠ¸ì´ë¯€ë¡œ ìŠ¤ë ˆë“œë¡œ ì˜¤í”„ë¡œë”©
    return await asyncio.to_thread(_build_mobile_ctx_sync, user_id)


# ----------------------------------------------------------------------
# ì…€ 6.9: RAG ì˜ë¯¸ ë¶ˆì¼ì¹˜(semantic mismatch) í•„í„°
# ----------------------------------------------------------------------
async def filter_semantic_mismatch(user_input: str, rag_ctx: str) -> str:
    """ì‚¬ìš©ì ì§ˆì˜ì™€ RAG ì»¨í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ ë¶ˆì¼ì¹˜ë¥¼ LLMìœ¼ë¡œ ë¹ ë¥´ê²Œ ê°ì§€í•´ í•„í„°ë§í•œë‹¤.
    - rag_ctxê°€ ë¹„ê±°ë‚˜ ë§¤ìš° ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - JSON ìŠ¤í‚¤ë§ˆ: {"keep": bool, "filtered": string}
    - íƒ€ì„ì•„ì›ƒ ì§§ê²Œ(â‰¤0.9s) ìš´ì˜
    """
    if not rag_ctx or len(rag_ctx) < 60:
        return rag_ctx
    schema = {
        "name": "RagFilter",
        "schema": {
            "type": "object",
            "properties": {
                "keep": {"type": "boolean"},
                "filtered": {"type": "string"},
            },
            "required": ["keep"],
            "additionalProperties": False,
        },
    }
    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” RAG ì»¨í…ìŠ¤íŠ¸ í•„í„°ë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ê±°ë‚˜ ì£¼ì œì ìœ¼ë¡œ ìƒì¶©í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ì œê±°í•œë‹¤. "
                "íŠ¹íˆ ì¥ì†Œ/ì—…ì¢…/ì¹´í…Œê³ ë¦¬ê°€ ë‹¤ë¥´ë©´(ì˜ˆ: ë°” vs í•œì‹ì§‘) ì œê±°í•˜ë¼."
            ),
        },
        {
            "role": "user",
            "content": (
                f"[ì§ˆë¬¸]\n{user_input}\n\n[ì»¨í…ìŠ¤íŠ¸]\n{rag_ctx}\n\n"
                "ê·œì¹™: 1) ì§ˆë¬¸ê³¼ ë¬´ê´€/ìƒì¶© ë¶€ë¶„ì€ ì‚­ì œí•œë‹¤. 2) ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ë‚¨ê¸´ë‹¤. 3) ê²°ê³¼ëŠ” JSONë§Œ.\n"
                "ìŠ¤í‚¤ë§ˆ: {keep:boolean, filtered:string}"
            ),
        },
    ]
    try:
        kwargs = {
            "model": LLM_MODEL,
            "messages": msgs,
            "temperature": 0.0,
            "max_tokens": 220,
        }
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_schema", "json_schema": schema}
        resp = await asyncio.wait_for(
            openai_chat_with_retry(**kwargs), timeout=min(0.9, TIMEOUT_RAG)
        )
        content = (resp.choices[0].message.content or "").strip()
        data = json.loads(content) if content.startswith("{") else {}
        keep = bool(data.get("keep", False))
        if not keep:
            return ""
        filtered = (data.get("filtered") or "").strip()
        return filtered or rag_ctx
    except Exception:
        return rag_ctx


# ----------------------------------------------------------------------
# ì…€ 6.95: WEB ì»¨í…ìŠ¤íŠ¸ í•„í„° (í˜„ ì‚¬ìš©ì ì¿¼ë¦¬ ê¸°ì¤€)
# ----------------------------------------------------------------------
async def filter_web_ctx(user_input: str, web_ctx: str) -> str:
    """ì‚¬ìš©ì ì§ˆì˜ì™€ WEB ì»¨í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ ë¶ˆì¼ì¹˜ë¥¼ LLMìœ¼ë¡œ ê°ì§€í•˜ì—¬ ê´€ë ¨ ì—†ëŠ” ë¸”ë¡ì„ ì œê±°í•œë‹¤.
    - web_ctxê°€ ë¹„ê±°ë‚˜ ë§¤ìš° ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ì†Œê·œëª¨ ì¸ì‚¬/ì¡ë‹´ì´ë©´ ë¬´ì¡°ê±´ ì œê±°(ì›¹ê²€ìƒ‰ ë¶ˆí•„ìš”)
    - JSON ìŠ¤í‚¤ë§ˆ: {keep:boolean, filtered:string}
    - íƒ€ì„ì•„ì›ƒ ì§§ê²Œ(â‰¤0.9s) ìš´ì˜
    """
    if not web_ctx or len(web_ctx) < 30:
        return web_ctx
    if _is_small_talk(user_input):
        return ""
    schema = {
        "name": "WebFilter",
        "schema": {
            "type": "object",
            "properties": {
                "keep": {"type": "boolean"},
                "filtered": {"type": "string"},
            },
            "required": ["keep"],
            "additionalProperties": False,
        },
    }
    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” WEB ì»¨í…ìŠ¤íŠ¸ í•„í„°ë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ê±°ë‚˜ ì£¼ì œì ìœ¼ë¡œ ìƒì¶©í•˜ëŠ” ì›¹ ê²°ê³¼ ë¸”ë¡ì€ ì œê±°í•œë‹¤. "
                "ë¸”ë¡ í˜•ì‹(ì´ë¦„/ê°„ë‹¨ ì„¤ëª…/ë§í¬)ê³¼ ë§í¬ëŠ” ìœ ì§€í•˜ë¼."
            ),
        },
        {
            "role": "user",
            "content": (
                f"[ì§ˆë¬¸]\n{user_input}\n\n[WEB ì»¨í…ìŠ¤íŠ¸]\n{web_ctx}\n\n"
                "ê·œì¹™: 1) ì§ˆë¬¸ê³¼ ë¬´ê´€/ìƒì¶© ë¸”ë¡ì€ ì‚­ì œ. 2) ê´€ë ¨ ë¸”ë¡ë§Œ ìœ ì§€. 3) ê²°ê³¼ëŠ” JSONë§Œ.\n"
                "ìŠ¤í‚¤ë§ˆ: {keep:boolean, filtered:string}"
            ),
        },
    ]
    try:
        kwargs = {
            "model": LLM_MODEL,
            "messages": msgs,
            "temperature": 0.0,
            "max_tokens": 220,
        }
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_schema", "json_schema": schema}
        resp = await asyncio.wait_for(
            openai_chat_with_retry(**kwargs), timeout=min(0.9, TIMEOUT_WEB)
        )
        content = (resp.choices[0].message.content or "").strip()
        data = json.loads(content) if content.startswith("{") else {}
        keep = bool(data.get("keep", False))
        if not keep:
            return ""
        filtered = (data.get("filtered") or "").strip()
        return filtered or web_ctx
    except Exception:
        return web_ctx


# ----------------------------------------------------------------------
# ì…€ 8: Conversation Chain
# ----------------------------------------------------------------------
async def conversation_chain(
    session_id: str, user_input: str, stm: ConversationSummaryBufferMemory
) -> str:
    # ë¡œì»¬ ì„í¬íŠ¸ë¡œ ì˜ì¡´ì„± ìµœì†Œí™” (ìƒë‹¨ import ìˆ˜ì • ë¶ˆí•„ìš”)
    from backend.directives.store import get_compiled as get_compiled_directives

    # 1) ì‚¬ìš©ì ê³ ì • ì·¨í–¥ JSONì´ ì»´íŒŒì¼ëœ system í”„ë¡¬í”„íŠ¸(ìºì‹œ) ë¡œë“œ
    slot_sys, _ = get_compiled_directives(session_id)

    # 2) íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì…ë ¥ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    hist = "\n".join(f"{m.type}: {m.content}" for m in stm.chat_memory.messages)
    prompt = (
        "ë„ˆëŠ” í•œêµ­ì–´ ë¹„ì„œì•±ì´ë‹¤. ì‚¬ìš©ìì˜ ì§€ì†ì  ì·¨í–¥(JSON ì§€ì‹œë¬¸)ì´ ìˆë‹¤ë©´ ìš°ì„  ì ìš©í•˜ë¼.\n"
        "ë¹ˆ RAG/Web ì»¨í…ìŠ¤íŠ¸ë¥¼ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ì‹¤ì‹œê°„ì„±/ì‹œìŠ¤í…œ ë©”íƒ€ ë°œì–¸ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ë¼.\n"
        "ë‘ê´„ì‹ìœ¼ë¡œ í•µì‹¬ì„ ë¨¼ì € ë§í•˜ê³ , í•„ìš”í•˜ë©´ ìµœì†Œí•œìœ¼ë¡œë§Œ ë§ë¶™ì—¬ë¼.\n"
        f"[ëŒ€í™” íˆìŠ¤í† ë¦¬]\n{hist}\n"
        f"[ìµœì‹  ì…ë ¥]\n{user_input}"
    )

    # 3) LLM í˜¸ì¶œ (ì§€ì‹œë¬¸ì´ ìˆìœ¼ë©´ system ìµœì „ë‹¨ì— ì£¼ì…)
    messages = ([{"role": "system", "content": slot_sys}] if slot_sys else []) + [
        {"role": "system", "content": "ëŒ€í™” ì „ê°œ ì§€ì¹¨ì„ ë”°ë¥´ë¼."},
        {"role": "user", "content": prompt},
    ]

    # ìˆœìˆ˜ ëŒ€í™” ëª¨ë“œëŠ” main_responseì—ì„œ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œë¡œ ëŒ€ì²´í•˜ë¯€ë¡œ,
    # ì—¬ê¸°ì„œëŠ” í’ˆì§ˆ ë³´ì™„ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©. ê¸°ë³¸ì€ ê²½ëŸ‰ ìš”ì•½ ìˆ˜ì¤€ë§Œ ë°˜í™˜.
    if SINGLE_CALL_CONV:
        return ""
    t0 = time.time()
    resp = await openai_chat_with_retry(
        model=LLM_MODEL,
        messages=messages,
        temperature=1.0,
    )
    took = (time.time() - t0) * 1000
    content = (resp.choices[0].message.content or "").strip()
    logger.info(
        f"[conv] model_used={resp.model} took_ms={took:.1f} out_len={len(content)}"
    )
    return content


# ----------------------------------------------------------------------
# ì…€ 10: Main LLM ìµœì¢… ì‘ë‹µ í…œí”Œë¦¿
# ----------------------------------------------------------------------
FINAL_PROMPT = PromptTemplate(
    input_variables=[
        "rag_ctx",
        "web_ctx",
        "mobile_ctx",
        "conv_ctx",
        "aux_ctx",
        "question",
        "web_summary",
    ],
    template=(
        "ë„ˆëŠ” ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•œ ê°œì¸ ë¹„ì„œ AIì´ë‹¤.\n\n"
        "{rag_ctx}\n\n"
        "{web_summary}"
        "{web_ctx}\n\n"
        "[ëª¨ë°”ì¼ ì»¨í…ìŠ¤íŠ¸]\n{mobile_ctx}\n\n"
        "[ì†Œí†µ ì²´ì¸ ê²°ê³¼]\n{conv_ctx}\n\n"
        "[ë§¥ë½ ë³´ì¡°]\n{aux_ctx}\n\n"
        "ì‚¬ìš©ì ì§ˆë¬¸: {question}\n"
        "ê·œì¹™: 1) web_ctxê°€ ë¹„ì–´ ìˆì§€ ì•Šë‹¤ë©´ web_ctxì˜ ê° ë¸”ë¡ì„ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ë¼. ê° ë¸”ë¡ì€ 3ì¤„(ì´ë¦„, ê°„ë‹¨í•œ ì„¤ëª…, ë§í¬)ì´ë©° ë§í¬ëŠ” ë°˜ë“œì‹œ ìœ ì§€í•œë‹¤. í•„ìš”í•˜ë©´ ë§¨ ìœ„ì— í•œ ì¤„ ìš”ì•½ë§Œ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. ê·¸ í•œ ì¤„ ìš”ì•½ì—ëŠ” mobile_ctxë¥¼ ì°¸ê³ í•œ ë§¥ë½ì„ í¬í•¨í•´ë„ ëœë‹¤. "
        "2) rag_ctx/web_ctxì— 'ìˆëŠ” ë‚´ìš©ë§Œ' ì¸ìš©Â·ìš”ì•½í•˜ê³ , ë¸”ë¡ì˜ ì›ë¬¸ í˜•ì‹ì€ ë³´ì¡´í•œë‹¤. "
        "3) rag_ctx/web_ctxê°€ ì¡´ì¬í•˜ë©´ conv_ctxëŠ” ë¬´ì‹œ. ë‹¨, mobile_ctx/aux_ctxëŠ” í†¤/ë§¥ë½ shaping ë³´ì¡°ë¡œë§Œ ì‚¬ìš©í•˜ë©° ì‚¬ì‹¤ ì¸ìš© ê¸ˆì§€. "
        "4) ì£¼ì†Œ/ë§í¬ëŠ” ê·¸ëŒ€ë¡œ. "
        "5) mobile_ctxëŠ” ê¶Œí•œ/ì •í™•ë„ ì´ìŠˆë¡œ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ , í•„ìš”í•  ë•Œë§Œ ì¡°ì‹¬ìŠ¤ë ˆ ë³´ì¡°ë¡œ ë°˜ì˜í•˜ë¼."
    ),
)


# ----------------------------------------------------------------------
# ì—…ë°ì´íŠ¸: ìŠ¤ëƒ…ìƒ·/í”„ë¡œí•„ ì—…ì„œíŠ¸ íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€)
# ----------------------------------------------------------------------
def update_long_term_memory(session_id: str):
    """
    - ê³¼ê±° ëŒ€í™”(old) êµ¬ì¡°í™”+ìš”ì•½(â‰ˆ500tok) ìŠ¤ëƒ…ìƒ· ìƒì„±
    - ë©±ë“± í‚¤(sha256)ë¡œ ë¡œê·¸ ì—…ì„œíŠ¸ ë°©ì§€
    - í”„ë¡œí•„ ê°±ì‹  í›„ ì—…ì„œíŠ¸
    """
    logger.info("[rag:update] start session_id=%s", session_id)
    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)
    messages = history.messages
    if not messages:
        logger.info("[rag:update] no messages -> skip")
        return

    # ë¶„ë¦¬
    old_msgs, recent_msgs = _split_for_summary(messages, RECENT_RAW_TOKENS_BUDGET)
    snap_start_dt, snap_end_dt = _extract_ts_bounds(old_msgs, _now_kst())
    ymd_start = _ymd(snap_start_dt)
    ymd_end = _ymd(snap_end_dt)
    ym_end = _ym(snap_end_dt)
    logger.info(
        "[rag:update] ts_bounds start=%s end=%s (ymd_start=%d ymd_end=%d ym_end=%d)",
        snap_start_dt.isoformat(),
        snap_end_dt.isoformat(),
        ymd_start,
        ymd_end,
        ym_end,
    )

    if not old_msgs:
        logger.info("[rag:update] nothing to summarize(old empty) -> skip")
        return
    old_text = _messages_to_text(old_msgs)
    snap_text, meta = _build_structured_and_summary(session_id, old_msgs)
    snap_hash = _sha256(old_text)

    # ë©±ë“±ì„± ì²´í¬
    if IDEMPOTENCY_CACHE.get(session_id) == snap_hash:
        logger.info("[rag:update] idempotent skip (same hash)")
        return
    IDEMPOTENCY_CACHE[session_id] = snap_hash

    # í”„ë¡œí•„ ìš”ì•½/í†µí•©
    conv_all = _messages_to_text(messages)
    summary_chain = (
        ChatPromptTemplate.from_template(
            "ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìì˜ íŠ¹ì§•ê³¼ ê´€ê³„ì—†ëŠ” ì¸ì‚¬ë§ ë“± ë¶ˆí•„ìš”í•œ ì¡ë‹´ê³¼ ë‚´ìš©ì€ ëª¨ë‘ ì œê±°í•˜ê³ , "
            "ì‚¬ìš©ì í”„ë¡œí•„ì— ìœ ì˜ë¯¸í•œ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½í•´ë¼.\n{conversation}"
        )
        | llm_cold
        | StrOutputParser()
    )
    summary_text = summary_chain.invoke({"conversation": conv_all})
    logger.info("[rag:update] profile_summary_len=%d", len(summary_text or ""))

    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)
    llm_profile = (
        llm_cold.bind(response_format={"type": "json_object"})
        if _model_supports_response_format(LLM_MODEL)
        else llm_cold
    )
    profile_chain = (
        ChatPromptTemplate.from_template(
            "[ê¸°ì¡´ í”„ë¡œí•„]\n{old}\n[ìš”ì•½ëœ ìµœì‹  ëŒ€í™”]\n{sum}\n"
            "ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ì‚¬ìš©ì í•µì‹¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™”ë¥¼ ìœ„í•œ JSON í”„ë¡œí•„ë¡œ ë°˜í™˜í•´ì¤˜."
        )
        | llm_profile
        | StrOutputParser()
    )
    new_prof_str = profile_chain.invoke({"old": old_prof, "sum": summary_text})

    try:
        new_prof = json.loads(new_prof_str)
        PROFILE_DB[session_id] = new_prof
        logger.info("[rag:update] profile_json_ok keys=%s", list(new_prof.keys()))
    except json.JSONDecodeError:
        logger.warning("[rag:update] profile json decode failed -> skip profile update")
        new_prof = PROFILE_DB.get(session_id, {})

    # RAG ëª¨ë“ˆì˜ ë³´ì¥ í•¨ìˆ˜ë¡œ ë‹¨ì¼í™” (dim/ìŠ¤í‚¤ë§ˆ ì¼ì¹˜ ë³´ì¥)
    try:
        from backend.rag import ensure_collections as _ensure_cols

        prof_coll, log_coll = _ensure_cols()
    except Exception as _ec_e:
        logger.warning("[rag:update] ensure_collections error: %r", _ec_e)
        # í˜¸í™˜ í´ë°±: ê¸°ì¡´ ê²½ë¡œ ìœ ì§€ ì‹œë„
        ensure_milvus()
        prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, "User Profiles")
        log_coll = create_milvus_collection(LOG_COLLECTION_NAME, "Conversation Logs")

    now = _now_kst()
    ym = ym_end
    ymd = ymd_end
    try:
        part_prof = _ensure_partition(prof_coll, ym)
    except Exception:
        part_prof = None
    try:
        part_log = _ensure_partition(log_coll, ym)
    except Exception:
        part_log = None

    # ì—…ì„œíŠ¸: í”„ë¡œí•„
    prof_emb = embed_query_cached(json.dumps(new_prof, ensure_ascii=False))
    try:
        prof_coll.upsert(
            [
                {
                    "id": session_id,
                    "embedding": prof_emb,
                    "text": json.dumps(new_prof, ensure_ascii=False),
                    "user_id": session_id,
                    "type": "profile",
                    "created_at": int(time.time_ns()),
                    "date_start": ymd_start,  # â¬…ï¸ old ë²”ìœ„ ì‹œì‘
                    "date_end": ymd_end,  # â¬…ï¸ old ë²”ìœ„ ë(ìµœì‹ )
                    "date_ym": ym,  # â¬…ï¸ ëì›”
                }
            ],
            partition_name=part_prof if part_prof else None,
        )
        logger.info("[rag:update] upsert profile session_id=%s", session_id)
    except Exception as e:
        logger.warning("[rag:update] profile upsert error: %r", e)

    # ===== ì‹ ê·œì„± í‰ê°€: ì§ì „ í”„ë¡œí•„ vs ì‹ ê·œ í”„ë¡œí•„ì˜ í•µì‹¬ í•­ëª© ì°¨ì´ =====
    try:
        old_prof_obj = json.loads(old_prof) if old_prof else {}
    except Exception:
        old_prof_obj = {}
    new_items = _flatten_profile_items(new_prof if isinstance(new_prof, dict) else {})
    old_items = _flatten_profile_items(
        old_prof_obj if isinstance(old_prof_obj, dict) else {}
    )
    profile_delta_cnt = len(new_items - old_items)
    logger.info(
        "[novelty] profile_delta_cnt=%d (min=%d)",
        profile_delta_cnt,
        NOVELTY_MIN_PROFILE_DELTA,
    )

    # ===== ìŠ¤ëƒ…ìƒ· í…ìŠ¤íŠ¸ ì •ê·œí™” í•´ì‹œ/SimHash/ê·¼ì‚¬ì¤‘ë³µ ì²´í¬ =====
    # 1) ì •ê·œí™” í…ìŠ¤íŠ¸ í•´ì‹œ (ì™„ì „ì¤‘ë³µ ì¦‰ì‹œ ë°°ì œ)
    try:
        _norm = " ".join((snap_text or "").strip().lower().split())
        norm_hash = _sha256(_norm)
    except Exception:
        norm_hash = _sha256(snap_text or "")

    # 2) SimHash ì„œëª…(ê²½ëŸ‰): ì¤‘ë³µ í›„ë³´ ë¹ ë¥¸ ë°°ì œì— ì‚¬ìš© (Redis ì €ì¥)
    try:
        simhash64 = __import__("backend.rag.refs", fromlist=["_simhash64"])._simhash64
        sig64 = simhash64(snap_text or "")
        # Redisì— ìµœê·¼ ì„œëª… ì €ì¥ í›„ ê·¼ì ‘ ì„œëª… ì¡´ì¬ ì‹œ ë¹ ë¥¸ ë°°ì œ íŒíŠ¸ë¡œ í™œìš©
        try:
            import redis as _redis

            _r = _redis.Redis.from_url(REDIS_URL, decode_responses=True)
            sigkey = f"snap:sig:{session_id}"
            # ê·¼ì ‘ íƒìƒ‰ì€ ê°„ë‹¨íˆ ë™ì¼ ë¸”ë¡ ì„œëª…ë§Œ í™•ì¸(í™•ì¥ ì—¬ì§€)
            if _r.sismember(sigkey, str(sig64)):
                logger.info("[novelty] simhash immediate-hit -> likely duplicate")
        except Exception:
            pass
    except Exception:
        sig64 = 0

    # íšŒì°¨ ìš”ì•½ì˜ ì—”í‹°í‹°/í‚¤í”„ë ˆì´ì¦ˆë¡œ TAGS í—¤ë” ì£¼ì…(ê²€ìƒ‰ ê°€ì¤‘ì¹˜ìš©)
    try:
        struct_try = json.loads(
            (STRUCTURE_PROMPT | llm_cold | StrOutputParser()).invoke(
                {
                    "schema_json": json.dumps(STRUCTURE_SCHEMA, ensure_ascii=False),
                    "pinned_json": json.dumps(
                        _pinned_facts_of(session_id), ensure_ascii=False
                    ),
                    "text_block": old_text,
                }
            )
        )
    except Exception:
        struct_try = {}
    tags_line = ""
    try:
        ents = struct_try.get("entities", []) if isinstance(struct_try, dict) else []
        kps = struct_try.get("facts", []) if isinstance(struct_try, dict) else []
        tags = list(dict.fromkeys([str(x) for x in (ents + kps) if x]))[:10]
        if tags:
            tags_line = "[TAGS] " + ",".join(tags) + "\n"
    except Exception:
        pass

    text_blob = (
        f"[SNAPSHOT meta:turn_range=?, token_count=?, ver={meta['summary_version']}, model={meta['model']}] \n"
        + tags_line
        + snap_text
    )
    log_emb = embed_query_cached(text_blob)

    ym_min = _ym_minus_months(_now_kst(), SNAPSHOT_LOOKBACK_MONTHS)
    is_dup, dup_sim = _near_duplicate_log(session_id, log_emb, ym_min)
    logger.info(
        "[novelty] near_dup=%s sim=%.3f thr=%.2f ym_min=%d",
        is_dup,
        dup_sim,
        NOVELTY_SIM_THRESHOLD,
        ym_min,
    )

    # ===== ê²Œì´íŠ¸: ê·¼ì‚¬ì¤‘ë³µì´ê±°ë‚˜, í”„ë¡œí•„ ì‹ ê·œ í•­ëª©ì´ ê±°ì˜ ì—†ìœ¼ë©´ ë¡œê·¸ ì ì¬ ìŠ¤í‚µ =====
    # ì™„ì „ì¤‘ë³µ(ì •ê·œí™” í•´ì‹œ) ë° ê·¼ì‚¬ì¤‘ë³µ/ì‹ ê·œì„± ë¶€ì¡± ì»·
    prev_hash = SESSION_STATE.get(session_id, {}).get("last_norm_hash")
    if prev_hash == norm_hash:
        is_dup = True
    if is_dup or profile_delta_cnt < NOVELTY_MIN_PROFILE_DELTA:
        logger.info(
            "[rag:update] skip log upsert due to %s",
            "near-duplicate" if is_dup else "low-novelty(profile)",
        )
    else:
        # (ê¸°ì¡´) log_coll.upsert(...) ê·¸ëŒ€ë¡œ ì‹¤í–‰
        try:
            log_coll.upsert(
                [
                    {
                        "id": f"{session_id}:{snap_hash}",
                        "embedding": log_emb,
                        "text": text_blob,
                        "user_id": session_id,
                        "type": "log",
                        "created_at": int(time.time_ns()),
                        "date_start": ymd_start,
                        "date_end": ymd_end,
                        "date_ym": ym,
                    }
                ],
                partition_name=part_log if part_log else None,
            )
            logger.info("[rag:update] upsert log id=%s", f"{session_id}:{snap_hash}")
        except Exception as e:
            logger.warning("[rag:update] log upsert error: %r", e)

    # ì„œëª…/ì •ê·œí™” í•´ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
    try:
        st = SESSION_STATE.setdefault(session_id, {})
        st["last_norm_hash"] = norm_hash
        # Redisì— simhash ì§‘í•© ì—…ë°ì´íŠ¸
        try:
            import redis as _redis

            _r = _redis.Redis.from_url(REDIS_URL, decode_responses=True)
            sigkey = f"snap:sig:{session_id}"
            _r.sadd(sigkey, str(sig64))
            _r.expire(sigkey, int(os.getenv("SNAP_SIG_TTL_SEC", "15552000")))  # 180d
        except Exception:
            pass
    except Exception:
        pass


# ----------------------------------------------------------------------
# ì…€ 11: FastAPI + WebSocket ì„œë²„
# ----------------------------------------------------------------------
app = FastAPI()


@app.on_event("startup")
async def _on_startup():
    await _ensure_workers()
    await ensure_directive_workers()
    # ì¼ì¼ 03:00(KST) ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë™
    try:
        from backend.directives.scheduler import ensure_daily_scheduler

        await ensure_daily_scheduler()
    except Exception as e:
        logger.warning("[startup] directive scheduler init error: %r", e)
    await asyncio.to_thread(_ensure_intent_embeddings)  # â† ì¶”ê°€
    try:
        ensure_milvus_collections()
    except Exception as e:
        logger.warning("[startup] milvus warm error : %r", e)
    logger.info("[startup] workers ready")

    # í”„ë¡œì•¡í‹°ë¸Œ ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë™
    try:
        from backend.proactive.scheduler import ensure_proactive_scheduler

        await ensure_proactive_scheduler()
        logger.info("[startup] proactive scheduler ready")
    except Exception as e:
        logger.warning("[startup] proactive scheduler init error: %r", e)


@app.get("/")
def health():
    logger.info("[health] ok")
    return {"status": "ok"}


# ----------------------------------------------------------------------
# ë‚´ë¶€ ì—”ë“œí¬ì¸íŠ¸ (MCP í”„ë¡ì‹œìš©)
# ----------------------------------------------------------------------


@app.post("/internal/rag/retrieve")
async def internal_rag_retrieve(payload: Dict[str, Any]):
    """
    ì…ë ¥: {"session_id": str, "query": str, "top_k": int|None, "date_filter": [int,int]|null}
    ì¶œë ¥: {"blocks": string}
    """
    try:
        sid = str(payload.get("session_id") or "").strip()
        q = str(payload.get("query") or "").strip()
        top_k = int(payload.get("top_k") or 2)
        df = payload.get("date_filter")
        date_filter = (
            (int(df[0]), int(df[1]))
            if isinstance(df, (list, tuple)) and len(df) == 2
            else None
        )
        blocks = retrieve_from_rag(sid, q, top_k=top_k, date_filter=date_filter)
        return {"blocks": blocks or ""}
    except Exception as e:
        logger.warning("[/internal/rag/retrieve] error: %r", e)
        return {"blocks": ""}


@app.post("/internal/mobile/context")
async def internal_mobile_context(payload: Dict[str, Any]):
    """
    ì…ë ¥: {"session_id": str}
    ì¶œë ¥: {"blocks": string}
    """
    try:
        sid = str(payload.get("session_id") or "").strip()
        blocks = await build_mobile_ctx(sid)
        return {"blocks": blocks or ""}
    except Exception as e:
        logger.warning("[/internal/mobile/context] error: %r", e)
        return {"blocks": ""}


@app.post("/internal/evidence/bundle")
async def internal_evidence_bundle(payload: Dict[str, Any]):
    """
    ì…ë ¥: {"session_id": str, "query": str, "web_on": bool, "rag_on": bool, "timeout_s": float|None}
    ì¶œë ¥: {"web": {"blocks": string}, "rag": {"blocks": string}}
    """
    try:
        from backend.evidence.builder import build_evidence as _build_evidence

        sid = str(payload.get("session_id") or "").strip()
        q2 = str(payload.get("query") or "").strip()
        web_on = bool(payload.get("web_on", True))
        rag_on = bool(payload.get("rag_on", True))
        timeout_s = float(payload.get("timeout_s") or max(TIMEOUT_WEB, TIMEOUT_RAG))
        mcp_url = os.getenv("MCP_SERVER_URL", "http://mcp:5000")
        _, web_ctx, rag_ctx = await _build_evidence(
            mcp_url, sid, q2, web_on, rag_on, timeout_s
        )
        return {"web": {"blocks": web_ctx or ""}, "rag": {"blocks": rag_ctx or ""}}
    except Exception as e:
        logger.warning("[/internal/evidence/bundle] error: %r", e)
        return {"web": {"blocks": ""}, "rag": {"blocks": ""}}


@app.get("/internal/directives/{session_id}/compiled")
async def internal_directives_compiled(session_id: str):
    try:
        from backend.directives.store import get_compiled as _get_compiled

        prompt, ver = _get_compiled(session_id)
        return {"prompt": prompt or "", "version": ver or ""}
    except Exception as e:
        logger.warning("[/internal/directives/compiled] error: %r", e)
        return {"prompt": "", "version": ""}


async def background_rag_update(session_id: str):
    logger.info("[bg] schedule rag update session=%s", session_id)
    _enqueue_snapshot(session_id)


# ---- app.py: main_response (ê·¸ëŒ€ë¡œ) ----


async def main_response(
    session_id: str,
    user_input: str,
    websocket: WebSocket,
    mobile_ctx: str,
    rag_ctx: str,
    web_ctx: str,
    conv_ctx: str,
) -> str:
    # ë¡œì»¬ ì„í¬íŠ¸ë¡œ ì˜ì¡´ì„± ìµœì†Œí™”
    from backend.directives.store import get_compiled as get_compiled_directives

    # web_ctxê°€ ì¥í™©í•  ê²½ìš°ì—ë§Œ í•œ ì¤„ ìš”ì•½ í”„ë¦¬ì•°ë¸” êµ¬ì„±
    web_summary = ""
    if web_ctx and len(web_ctx) > 400:
        web_summary = "[ìš”ì•½] ì•„ë˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½: "

    # aux_ctxëŠ” í†¤/ë§¥ë½ shaping ì „ìš©: STWM/íšŒì°¨ìš”ì•½ì„ ë³„ë„ ìŠ¬ë¡¯ìœ¼ë¡œ ì „ë‹¬
    aux_ctx = SESSION_STATE.get(session_id, {}).get("aux_ctx", "")
    prompt = FINAL_PROMPT.format(
        rag_ctx=rag_ctx,
        web_ctx=web_ctx,
        mobile_ctx=mobile_ctx,
        conv_ctx=conv_ctx,
        aux_ctx=aux_ctx,
        question=user_input,
        web_summary=web_summary,
    )
    logger.info(
        f"[final] prompt_sizes rag={len(rag_ctx)} web={len(web_ctx)} conv={len(conv_ctx)} q_len={len(user_input)}"
    )

    # 1) ê³ ì • ì·¨í–¥ JSON ì§€ì‹œë¬¸(ë°°ì¹˜ ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼)ì„ ìºì‹œì—ì„œ ë¡œë“œ
    slot_sys, _ = get_compiled_directives(session_id)

    evidence_mode = bool(rag_ctx.strip() or web_ctx.strip())

    if evidence_mode:
        sys_rule = (
            "ê°œì¸ ë¹„ì„œ AIì´ë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ ë‹µí•˜ë¼. "
            "1) rag_ctx/web_ctxê°€ ì¡´ì¬í•˜ë©´ í•´ë‹¹ ë²”ìœ„ ë‚´ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , ë¹ˆ ì„¹ì…˜ì€ ì–¸ê¸‰í•˜ì§€ ë§ë¼. "
            "2) web_ctxê°€ ìˆìœ¼ë©´ ê° ë¸”ë¡(ì´ë¦„/ê°„ë‹¨ ì„¤ëª…/ë§í¬)ì„ ì¬ì§ˆë¬¸ ì—†ì´ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ë˜, ë§¨ ìœ„ì— í•œ ì¤„ ìš”ì•½ë§Œ ë§ë¶™ì¼ ìˆ˜ ìˆë‹¤. "
            "3) ì¶”ê°€ ì§ˆë¬¸(clarify)ì´ë‚˜ ì„ í˜¸ íƒìƒ‰ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ë§ë¼. "
            "4) mobile_ctx/aux_ctxëŠ” í†¤/ì—°ê²°ê°ì„ ìœ„í•œ ë³´ì¡°ë¡œë§Œ í™œìš©í•˜ê³ , ì‚¬ì‹¤ ì¸ìš©ì€ ê¸ˆì§€í•œë‹¤. "
            "5) ë¶ˆí•„ìš”í•œ ì‚¬ì¡± ì—†ì´ ê°„ê²°í•˜ê²Œ."
        )
    else:
        sys_rule = (
            "ê°œì¸ ë¹„ì„œ AIì´ë‹¤. rag_ctx/web_ctxê°€ ë¹„ì–´ ìˆìœ¼ë¯€ë¡œ conv_ctxì™€ aux_ctx(íŠ¹íˆ STWM ì•µì»¤)ë¥¼ ìš°ì„  ì ìš©í•´ ë‹µí•˜ë¼. "
            "ì¬ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ë§ê³ , ì‚¬ìš©ìê°€ ì œê³µí•œ ì•µì»¤(ì¥ì†Œ/ì£¼ì œ/ì‹œê°„ ë“±)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆì„ ê°„ê²°íˆ ì œì‹œí•˜ë¼. "
            "ì‚¬ì‹¤ ë‹¨ì •ì´ë‚˜ ì¶”ì¸¡ì€ ê¸ˆì§€í•˜ë©°, í•„ìš”í•œ ê²½ìš° ë§ˆì§€ë§‰ í•œ ì¤„ì—ë§Œ ì„ íƒì§€ë¥¼ ì œì•ˆí•˜ë¼(ì˜ˆ: 'ë£¨í”„íƒ‘/ì¡°ìš©/ì¹µí…Œì¼ ì¤‘ì‹¬ ì¤‘ ê³¨ë¼ì£¼ì„¸ìš”'). "
            "mobile_ctxëŠ” í†¤ê³¼ ì—°ê²°ê° ë³´ì¡°ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì‚¬ì‹¤ ì¸ìš©ì€ ê¸ˆì§€í•œë‹¤."
        )

    # ê³ ì • ì •ì²´ì„± í”„ë¡¬í”„íŠ¸ + ë™ì  ì‚¬ìš©ì ì§€ì‹œë¬¸(slot_sys) ëª¨ë‘ ì£¼ì…
    messages = (
        ([{"role": "system", "content": IDENTITY_PROMPT}])
        + ([{"role": "system", "content": slot_sys}] if slot_sys else [])
        + [
            {"role": "system", "content": sys_rule},
            {"role": "user", "content": prompt},
        ]
    )

    # Evidence ëª¨ë“œì—ì„œë„ LLM ë‹¨ê³„ë¥¼ ë°˜ë“œì‹œ ê±°ì¹œë‹¤(ì§ì ‘ ì¶œë ¥ ê¸ˆì§€).
    # í•„ìš” ì‹œ ë˜í¼ ê²°ê³¼ë¥¼ íŒíŠ¸ë¡œë§Œ ì‚¬ìš©í•˜ì—¬ í¬ë§· ì•ˆì •ì„±ì„ ë†’ì¸ë‹¤.
    evidence_hint = ""
    try:
        if evidence_mode and web_ctx.strip():
            draft = wrap_web_reply(user_input, web_ctx, "")
            ok = await _validate_final_answer(user_input, rag_ctx, web_ctx, draft)
            evidence_hint = draft if ok else ""
    except Exception:
        evidence_hint = ""

    # 4) ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ(LLM)
    t0 = time.time()
    full_answer = ""
    model_logged = False
    try:
        # ìˆœìˆ˜ ëŒ€í™” ëª¨ë“œë©´(conv_ctxë§Œ ì‚¬ìš©) conv ì²´ì¸ ì—†ì´ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        # ì›¹/RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•ŒëŠ” í¬ë§· ì•ˆì •ì„±ê³¼ ì‘ë‹µ ì¼ê´€ì„±ì„ ìœ„í•´ ë” ë‚®ì€ temperature ì‚¬ìš©
        if _stream_allowed():
            try:
                create_kwargs = {
                    "model": LLM_MODEL,
                    "messages": messages,
                    "stream": True,
                }
                # ì¦ê±° ëª¨ë“œì—ì„œëŠ” ì°½ì˜ì„± ì–µì œ(ì§ˆë¬¸/ì¼íƒˆ ë°©ì§€)
                if evidence_mode:
                    create_kwargs["temperature"] = 0.0
                stream = await openai_chat_with_retry(**create_kwargs)
            except Exception as e_stream_flag:
                # ì¡°ì§ ìŠ¤íŠ¸ë¦¬ë° ê¶Œí•œ ë¯¸ë³´ìœ  ì‹œ ë¹„í™œì„±í™”í•˜ê³  ë…¼ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í´ë°±
                _STREAM_RUNTIME_DISABLED = True
                raise e_stream_flag
        else:
            raise RuntimeError("stream_disabled")

        async for chunk in stream:
            if not model_logged:
                mid = getattr(chunk, "model", None)
                if mid:
                    logger.info(f"[final] model_used={mid}")
                    model_logged = True
            delta = chunk.choices[0].delta
            token = getattr(delta, "content", None) or ""
            if token:
                full_answer += token
                await websocket.send_text(token)
    except Exception as e:
        logger.warning(f"[final] stream error: {repr(e)}")
        # í´ë°±: ë…¼ìŠ¤íŠ¸ë¦¬ë° (ê¶Œí•œ ë¶ˆê°€/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ í¬í•¨)
        create_kwargs = {"model": LLM_MODEL, "messages": messages, "stream": False}
        if evidence_mode:
            create_kwargs["temperature"] = 0.0
        resp = await openai_chat_with_retry(**create_kwargs)
        logger.info(f"[final] model_used={resp.model}")
        text = (resp.choices[0].message.content or "").strip()
        full_answer = text
        if text:
            await websocket.send_text(text)

    took = (time.time() - t0) * 1000
    logger.info(f"[final] streamed out_len={len(full_answer)} took_ms={took:.1f}")
    # ê²½ëŸ‰ ì‚¬í›„ ê²€í† (ì¦ê±° ëª¨ë“œì¼ ë•Œë§Œ). ì‹¤ì‹œê°„ì„± ìœ„í•´ ë¹„ì°¨ë‹¨ìœ¼ë¡œ ìˆ˜í–‰
    try:
        if evidence_mode and (rag_ctx.strip() or web_ctx.strip()):
            asyncio.create_task(
                _post_verify_answer(
                    user_input, rag_ctx, web_ctx, full_answer, websocket
                )
            )
    except Exception:
        pass
    return full_answer


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    logger.info("[ws] accepted session=%s", session_id)

    try:
        while True:
            user_input = await websocket.receive_text()
            turn_id = str(uuid.uuid4())[:8]
            logger.info(
                f"[turn:{turn_id}] recv q_len={len(user_input)} q='{user_input}'"
            )

            # ì„¸ì…˜ IDë¥¼ ì„¸ì…˜ ìƒíƒœì— ìµœê·¼ ì‚¬ìš© ì„¸ì…˜ìœ¼ë¡œ ê¸°ë¡ (ì„ë² ë”© íŒíŠ¸ ìºì‹œ í‚¤ë¡œ í™œìš©)
            try:
                SESSION_STATE["__last_session__"] = session_id
            except Exception:
                pass
            stm = get_short_term_memory(session_id)
            hist_msgs = stm.chat_memory.messages
            hist = "\n".join(f"{m.type}: {m.content}" for m in hist_msgs)

            # ---- STWM ì—…ë°ì´íŠ¸ + í„´ ë²„í¼ ê¸°ë¡(ìœ ì €) ----
            try:
                stwm_snap = update_stwm(session_id, user_input)
                try:
                    logger.info(
                        f"[turn:{turn_id}] stwm latest extras=%s entities=%d",
                        (stwm_snap.extras or {}),
                        len(stwm_snap.entities or []),
                    )
                except Exception:
                    pass
            except Exception:
                stwm_snap = None
            try:
                tb_add_turn(session_id, "user", user_input)
            except Exception:
                pass

            # ---- ë‹´í™” ì•µì»¤ ì¶”ì¶œ ë° ê°±ì‹ (LLM ê¸°ë°˜) ----
            try:
                hist_tail = "\n".join(
                    f"{m.type}: {m.content}" for m in hist_msgs[-HINT_LOOKBACK:]
                )
                anchors = await _extract_anchors(user_input, hist_tail)
                _update_anchor_state(
                    session_id, anchors.get("place"), anchors.get("topic")
                )
            except Exception as _anc_e:
                logger.warning(f"[turn:{turn_id}] anchor extract error: {repr(_anc_e)}")

            tokens_prev = SESSION_STATE.get(session_id, {}).get("prev_tokens", 0)
            tokens_now = len(enc.encode(hist))
            logger.info(f"[turn:{turn_id}] hist_tokens={tokens_now}")

            # ì—ì§€-íŠ¸ë¦¬ê±° + ë””ë°”ìš´ìŠ¤: RAG ìŠ¤ëƒ…ìƒ· ì˜ˆì•½
            _edge_and_debounce(session_id, tokens_prev, tokens_now)

            # 1) ë¼ìš°íŒ…: í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ ì„ë² ë”© ìš°ì„  ë˜ëŠ” ì†Œë¶„ë¥˜ê¸° ìš°ì„ 
            ROUTE_EMBEDDING_FIRST = bool(int(os.getenv("ROUTE_EMBEDDING_FIRST", "1")))
            need_rag_prob = 0.0
            need_web_prob = 0.0
            need_rag = False
            need_web = False
            # ìŠ¬ë¡¯ ì¶©ì¡± ë¸Œë¦¿ì§•: ì§ì „ í„´ì—ì„œ ì§€ì—­ ëª…í™•í™”ë¥¼ ìš”ì²­í–ˆê³ , ì´ë²ˆ í„´ì— ì¥ì†Œê°€ ì…ë ¥ë˜ë©´ ì›¹ìœ¼ë¡œ ìŠ¹ê²©
            try:
                st = SESSION_STATE.get(session_id, {})
                awaiting = bool(st.get("await_place"))
                place_now = bool((anchors.get("place") or "").strip())
                if awaiting and place_now:
                    need_web = True
                    need_web_prob = max(need_web_prob, max(TAU_WEB, 0.6))
                    # ì¼íšŒì„±ìœ¼ë¡œ í•´ì œ
                    st["await_place"] = False
            except Exception:
                pass
            if ROUTE_EMBEDDING_FIRST:
                # (ì„ë² ë”© ìš°ì„ ) ëª¨ë“  í„´ì—ì„œ ì„ë² ë”© ë¼ìš°í„° 1ì°¨ â†’ ì• ë§¤í•˜ë©´ LLM one-call ë³´ì¡°
                aux_label = await asyncio.to_thread(embedding_router, user_input, 0.4)
                if aux_label == "rag":
                    need_rag = True
                    need_rag_prob = max(need_rag_prob, TAU_RAG + 0.1)
                elif aux_label == "web":
                    need_web = True
                    need_web_prob = max(need_web_prob, TAU_WEB + 0.1)
                if not (need_rag or need_web):
                    try:
                        data = await router_one_call(user_input, hist)
                        if (data.get("route") or "") == "rag":
                            need_rag = True
                        elif (data.get("route") or "") == "web":
                            need_web = True
                    except Exception:
                        pass
            else:
                # (ë ˆê±°ì‹œ) ì†Œë¶„ë¥˜ê¸° ìš°ì„  â†’ ì• ë§¤êµ¬ê°„ì—ì„œë§Œ ì„ë² ë”© ë¼ìš°í„° ë³´ì¡°
                pred = predict_need_flags(user_input, tau_rag=TAU_RAG, tau_web=TAU_WEB)
                need_rag_prob = float(pred["need_rag_prob"])
                need_web_prob = float(pred["need_web_prob"])
                need_rag = bool(pred["need_rag"])  # 1/0 â†’ bool
                need_web = bool(pred["need_web"])  # 1/0 â†’ bool

            logger.info(
                f"[turn:{turn_id}] route rag_prob={need_rag_prob:.3f} web_prob={need_web_prob:.3f} -> need_rag={need_rag} need_web={need_web}"
            )

            # 2) ì €ì‹ ë¢° ì»· + ì• ë§¤ë°´ë“œ ì²˜ë¦¬ + íƒ€ì„ë°•ìŠ¤ í‰í–‰ ê³„íš
            rag_timeout = TIMEOUT_RAG
            web_timeout = TIMEOUT_WEB
            LOW_CONF_MARGIN = 0.14
            TIMEBOX_SEC = 1.2

            max_prob = max(need_rag_prob, need_web_prob)
            if max_prob < max(TAU_RAG, TAU_WEB) - LOW_CONF_MARGIN:
                # ë¹„ìš©ì ˆê°: ê²€ìƒ‰/RAG ìŠ¤í‚µ, ë³´ê°•ì§ˆë¬¸ 1ë¬¸ìœ¼ë¡œ íšŒí”¼
                allow_conv = True
                try:
                    # ì•µì»¤/ìƒíƒœê°€ ì´ë¯¸ ì¶©ë¶„í•˜ë‹¤ë©´ clarify ìƒëµ
                    persisted = _get_anchor_state(session_id)
                    stwm_now = get_stwm_snapshot(session_id)
                    has_any_anchor = any(
                        [
                            str(anchors.get("place") or "").strip(),
                            str(anchors.get("topic") or "").strip(),
                            str(persisted.get("place") or "").strip(),
                            str(persisted.get("topic") or "").strip(),
                            str(stwm_now.get("last_loc") or "").strip(),
                            str(stwm_now.get("last_topic") or "").strip(),
                        ]
                    )
                    clarify_q = ""
                    if not has_any_anchor:
                        clarify_q = await _clarify_for_anchors(
                            session_id, user_input, hist_tail, anchors
                        )
                except Exception:
                    clarify_q = ""
                # ì»¨í…ìŠ¤íŠ¸-ë¯¼ê° ë‹¨ì„œ: ì´ì „ í„´ì´ ìŒì‹/ì¶”ì²œ ë§¥ë½ì´ë©´ ë‹¨ì¼ ëª…ì‚¬ëŠ” ì„¸ë¶€ì¡°ê±´ìœ¼ë¡œ í•´ì„í•˜ì—¬ ì›¹ ìŠ¹ê²© ì‹œë„
                try:
                    stwm_q = get_stwm_snapshot(session_id)
                    topic_hint = (stwm_q.get("last_topic") or "") if stwm_q else ""
                except Exception:
                    topic_hint = ""
                single_noun = bool(
                    re.fullmatch(r"[ê°€-í£A-Za-z]{2,10}", user_input.strip())
                )
                food_context = bool(
                    re.search(r"ë§›ì§‘|ì¹´í˜|ë©”ë‰´|ì‹ë‹¹|ë ˆìŠ¤í† ë‘|ìŒì‹|ë¨¹|ì¶”ì²œ", topic_hint)
                )
                if single_noun and food_context:
                    need_web = True
                    allow_conv = False
                    logger.info(
                        f"[turn:{turn_id}] single-noun in food context -> escalate web"
                    )
                elif clarify_q:
                    await websocket.send_text(clarify_q)
                    continue
                else:
                    logger.info(
                        f"[turn:{turn_id}] low_conf no-clarify -> allow conv only"
                    )
            else:
                rag_delta = abs(need_rag_prob - TAU_RAG)
                web_delta = abs(need_web_prob - TAU_WEB)
                rag_amb = rag_delta <= AMBIGUITY_BAND
                web_amb = web_delta <= AMBIGUITY_BAND

                # ìƒíƒœ ê¸°ë°˜ ë¼ìš°íŠ¸ íŒíŠ¸(ìµœê·¼ ë¼ìš°íŠ¸/ì•µì»¤/STWM): ëª¨í˜¸í•˜ê³  ì•„ì§ ê²°ì • ì•ˆ ë‚¬ì„ ë•Œë§Œ ì‚¬ìš©
                try:
                    st_now = SESSION_STATE.get(session_id, {})
                    last_route = (st_now.get("last_route") or "").strip()
                    last_route_at = float(st_now.get("last_route_at") or 0.0)
                except Exception:
                    last_route, last_route_at = "", 0.0
                try:
                    persisted = _get_anchor_state(session_id)
                    stwm_now = get_stwm_snapshot(session_id)
                    has_any_anchor = any(
                        [
                            str(persisted.get("place") or "").strip(),
                            str(persisted.get("topic") or "").strip(),
                            str(stwm_now.get("last_loc") or "").strip(),
                            str(stwm_now.get("last_topic") or "").strip(),
                        ]
                    )
                except Exception:
                    has_any_anchor = False
                within_ttl = False
                try:
                    within_ttl = (time.time() - last_route_at) <= max(
                        300.0, float(globals().get("TOPIC_TTL_S", 600.0))
                    )
                except Exception:
                    within_ttl = False

                if (
                    (rag_amb or web_amb)
                    and (not need_rag and not need_web)
                    and has_any_anchor
                    and within_ttl
                ):
                    if last_route == "web":
                        need_web = True
                        logger.info(
                            f"[turn:{turn_id}] state-bridge -> prefer web (last_route ttl+anchors)"
                        )
                    elif last_route == "rag":
                        need_rag = True
                        logger.info(
                            f"[turn:{turn_id}] state-bridge -> prefer rag (last_route ttl+anchors)"
                        )

                if rag_amb or web_amb:
                    aux_label = await asyncio.to_thread(
                        embedding_router, user_input, 0.4
                    )
                    logger.info(f"[turn:{turn_id}] embedding_router label={aux_label}")
                    if aux_label == "rag":
                        need_rag = True
                    elif aux_label == "web":
                        need_web = True
                    elif aux_label == "both":
                        need_rag, need_web = True, True

                # ìµœí›„ìˆ˜ë‹¨: ì—¬ì „íˆ ì• ë§¤ â†’ ì €ë¹„ìš© ë³‘ë ¬(íƒ€ì„ë°•ìŠ¤)
                if not need_rag and not need_web and (rag_amb or web_amb):
                    need_rag, need_web = True, True
                    rag_timeout = TIMEBOX_SEC
                    web_timeout = TIMEBOX_SEC
                    logger.info(
                        f"[turn:{turn_id}] fallback -> cheap parallel(web2,rag2,{TIMEBOX_SEC}s)"
                    )

            # ê²°ì • ë¡œê·¸

            logger.info(
                f"[turn:{turn_id}] decision need_rag={need_rag} need_web={need_web}"
            )

            # ìµœê·¼ ë¼ìš°íŠ¸ ì €ì¥(ìƒíƒœ ê¸°ë°˜ ë¼ìš°íŒ… íŒíŠ¸ë¥¼ ìœ„í•´)
            try:
                st = SESSION_STATE.setdefault(session_id, {})
                st["last_route_at"] = time.time()
                st["last_route"] = (
                    "web" if need_web else ("rag" if need_rag else "conv")
                )
            except Exception:
                pass

            # === í•˜ë“œ ê²Œì´íŠ¸: ì™¸ë¶€ ê·¼ê±° í•„ìš” ì‹œ conv ë¹„í™œì„±í™” ===
            # ì´ˆê¸° íŒë‹¨ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë˜, ì›¹/RAG ìŠ¤ì¼€ì¤„ ê²°ì • ì´í›„ ìµœì¢… ì¬ê³„ì‚°í•œë‹¤.
            allow_conv = not (need_rag or need_web)
            logger.info(f"[turn:{turn_id}] allow_conv(initial)={allow_conv}")

            tasks = {}

            # 2.5) ì¬ì‘ì„± íƒœìŠ¤í¬ ì˜ˆì•½
            rew_rag_task = None
            rew_web_task = None

            # ğŸ”§ confident fast-path: í™•ì‹  í¬ë©´ ë¦¬ë¼ì´íŠ¸ ìƒëµ
            fastpath_margin = 0.15
            prob_max = max(need_rag_prob, need_web_prob)
            tau_max = max(TAU_RAG, TAU_WEB)
            fastpath = prob_max >= (tau_max + fastpath_margin)
            logger.info(
                f"[turn:{turn_id}] fastpath={fastpath} prob_max={prob_max:.3f} tau_max={tau_max:.3f}"
            )

            # pronoun/time ì§€ì‹œì–´ ê°ì§€: ë¦¬ë¼ì´íŠ¸ íŠ¸ë¦¬ê±° ì¡°ê±´
            def detect_pronoun_or_time(t: str) -> bool:
                return bool(
                    re.search(r"(ê·¸ê±°|ì´ê±°|ê±°ê¸°|ê·¸ë•Œ|ì˜¤ëŠ˜|ë‚´ì¼|ì´ë²ˆì£¼|ì§€ë‚œì£¼)", t)
                )

            pronoun_hit = detect_pronoun_or_time(user_input)

            # conv ê²Œì´íŠ¸: ì™¸ë¶€ ê·¼ê±°ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ convëŠ” 'ìŠ¤íƒ€ì¼/ì—°ê²°'ë§Œ (ì‚¬ì‹¤ ìƒì„± ê¸ˆì§€)
            allow_conv = not (need_rag or need_web)
            conv_mode = "style_only" if not allow_conv else "full"

            # 3) ì „ë¬¸ê°€ íŒ€ ë³‘ë ¬ ì‹¤í–‰
            if allow_conv and not SINGLE_CALL_CONV:
                tasks["conv"] = asyncio.create_task(
                    conversation_chain(session_id, user_input, stm)
                )
                logger.info(f"[turn:{turn_id}] schedule conv=on (2-call)")
            else:
                logger.info(
                    f"[turn:{turn_id}] schedule conv=off (single-call mode or blocked)"
                )

            # 3.0) ëª¨ë°”ì¼ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (í•­ìƒ ì‹œë„, ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’)
            tasks["mobile"] = asyncio.create_task(build_mobile_ctx(session_id))

            # 3.1) RAG
            rag_query_text = None
            rag_date_filter = None
            if need_rag:
                if fastpath:
                    # ğŸ”§ ë¦¬ë¼ì´íŠ¸ ìƒëµ: ì–•ì€ ì½”ë¦¬í¼ëŸ°ìŠ¤ + ë‚ ì§œí•„í„°ë§Œ
                    rag_query_text = _shallow_coref(user_input, hist)
                    rag_date_filter = _extract_date_range_for_rag(rag_query_text)
                    asyncio.create_task(
                        asyncio.to_thread(embed_query_cached, rag_query_text)
                    )  # ğŸ”§ í”„ë¦¬ì›Œë°
                    logger.info(
                        f"[turn:{turn_id}] schedule rag=FAST q='{rag_query_text[:80]}' date_filter={rag_date_filter}"
                    )
                    tasks["rag"] = asyncio.wait_for(
                        asyncio.to_thread(
                            retrieve_from_rag,
                            session_id,
                            rag_query_text,
                            2,
                            rag_date_filter,
                        ),
                        timeout=rag_timeout,
                    )
                    # ë¦¬ë¼ì´íŠ¸ ê¸°ë¡
                    try:
                        add_rewrite(
                            session_id,
                            RewriteRecord(
                                raw_query=user_input,
                                query_rewritten=rag_query_text,
                                applied_slots=["coref", "date"],
                            ),
                        )
                    except Exception:
                        pass
                else:
                    # ì•µì»¤ íŒíŠ¸ë¥¼ êµ¬ì„±(ìˆìœ¼ë©´)
                    anchor = SESSION_STATE.get(session_id, {}).get("anchor", {})
                    anchor_hint = (
                        " ".join(
                            s
                            for s in [anchor.get("place", ""), anchor.get("topic", "")]
                            if s
                        ).strip()
                        or None
                    )
                    # ë¦¬ë¼ì´íŠ¸: ëŒ€ëª…ì‚¬/ì‹œì /ì§€ëª… ê°ì§€ ì‹œì—ë§Œ ì‹¤í–‰(ë˜ëŠ” fastpath=Falseì¼ ë•Œë§Œ)
                    if pronoun_hit or not fastpath:
                        rew_rag_task = asyncio.create_task(
                            rewrite_query(
                                "rag", user_input, hist, anchor_hint, session_id
                            )
                        )
                    try:
                        if rew_rag_task is not None:
                            rag_rw = await asyncio.wait_for(
                                rew_rag_task, timeout=REWRITE_TIMEOUT_S
                            )
                        else:
                            rag_rw = {"query_text": user_input, "date_filter": None}
                    except Exception as e:
                        logger.warning(
                            f"[turn:{turn_id}] rewrite RAG exception={repr(e)} -> use user_input"
                        )
                        rag_rw = {"query_text": user_input, "date_filter": None}
                    rag_query_text = rag_rw.get("query_text") or user_input
                    rag_date_filter = rag_rw.get("date_filter", None)
                    asyncio.create_task(
                        asyncio.to_thread(embed_query_cached, rag_query_text)
                    )  # ğŸ”§ í”„ë¦¬ì›Œë°
                    logger.info(
                        f"[turn:{turn_id}] schedule rag=on timeout={rag_timeout}s q='{rag_query_text[:80]}' date_filter={rag_date_filter}"
                    )
                    tasks["rag"] = asyncio.wait_for(
                        asyncio.to_thread(
                            retrieve_from_rag,
                            session_id,
                            rag_query_text,
                            2,
                            rag_date_filter,
                        ),
                        timeout=rag_timeout,
                    )
                    try:
                        add_rewrite(
                            session_id,
                            RewriteRecord(
                                raw_query=user_input,
                                query_rewritten=rag_query_text,
                                applied_slots=["rewrite_llm"],
                            ),
                        )
                    except Exception:
                        pass
            # ì‚¬ìš©ìê°€ 'ê²€ìƒ‰í•˜ì§€ ë§ê³  ì§€ë‚œ íšŒì˜ ë‚´ìš©' ë“± ë³´ì • ì‹ í˜¸ë¥¼ ì¤„ ê²½ìš° ê°•ì œ RAG ì„ íšŒ
            if not need_rag and re.search(
                r"ê²€ìƒ‰.*í•˜ì§€ ë§|ê²€ìƒ‰.*ë§ê³ |íšŒì˜ ë‚´ìš©|ì§€ë‚œ ëŒ€í™”|ì—Šê·¸ì œ", user_input
            ):
                logger.info(f"[turn:{turn_id}] user correction -> force RAG retry")
                need_rag = True
                rq = _shallow_coref(user_input, hist)
                df = _extract_date_range_for_rag(rq)
                tasks["rag"] = asyncio.wait_for(
                    asyncio.to_thread(retrieve_from_rag, session_id, rq, 2, df),
                    timeout=rag_timeout,
                )
                try:
                    add_rewrite(
                        session_id,
                        RewriteRecord(
                            raw_query=user_input,
                            query_rewritten=rq,
                            applied_slots=["coref", "date", "user_correction"],
                        ),
                    )
                except Exception:
                    pass
            else:
                logger.info(f"[turn:{turn_id}] schedule rag=off")

            # 3.2) WEB
            web_query = None
            if need_web:
                # 1) ë°˜ë“œì‹œ LLM ë¦¬ë¼ì´íŒ… â†’ ê²€ìƒ‰ ìˆœì„œ ê°•ì œ
                anchor = SESSION_STATE.get(session_id, {}).get("anchor", {})
                anchor_hint = (
                    " ".join(
                        s
                        for s in [anchor.get("place", ""), anchor.get("topic", "")]
                        if s
                    ).strip()
                    or None
                )
                # composite ì¿¼ë¦¬(ì§ˆì˜+STWM ì•µì»¤)
                stwm_q = get_stwm_snapshot(session_id)
                bits = [user_input]
                for k in (
                    "last_loc",
                    "last_time",
                    "last_act",
                    "last_target",
                    "last_topic",
                ):
                    v = str(stwm_q.get(k) or "").strip() if stwm_q else ""
                    if v:
                        bits.append(v)
                composite = " ".join(bits)
                try:
                    rew_web_task = asyncio.create_task(
                        rewrite_query("web", composite, hist, anchor_hint, session_id)
                    )
                    web_rw = await asyncio.wait_for(
                        rew_web_task, timeout=min(REWRITE_TIMEOUT_S, 0.8)
                    )
                    web_query = (web_rw.get("web_query") or composite).strip()
                except Exception as e:
                    logger.warning(
                        f"[turn:{turn_id}] rewrite WEB exception={repr(e)} -> embedding fallback"
                    )
                    # ì„ë² ë”© í´ë°±(ê°„ë‹¨ ì••ì¶•): í† í° ìƒìœ„ 8ê°œ
                    try:
                        toks = [t for t in re.split(r"\W+", composite) if len(t) >= 2]
                        web_query = " ".join(toks[:8]).lower()
                    except Exception:
                        web_query = composite.lower()
                logger.info(
                    f"[turn:{turn_id}] schedule web=on timeout={web_timeout}s q='{web_query[:80]}'"
                )
                # ì›¹ íƒìƒ‰ ì˜ë„ê°€ ëšœë ·í•˜ì§€ ì•Šìœ¼ë©´ ì°¨ë‹¨ (ì‚¬ìš©ì ì¿¼ë¦¬ ì¤‘ì‹¬)
                # ì˜ë„ íŒì •ì€ ì†Œë¶„ë¥˜ê¸° ë° ë¼ìš°í„°/ê°€ë“œì— ìœ„ì„. íœ´ë¦¬ìŠ¤í‹± ì²´í¬ ì œê±°.
                # 2) ê²€ìƒ‰ ì‹¤í–‰

                if need_web:
                    # ì—”ë“œí¬ì¸íŠ¸ ì„ íƒ ì „ë‹¬(í˜„ì¬ search_web ì‹œê·¸ë‹ˆì²˜ê°€ ì§€ì›í•  ë•Œ ë°˜ì˜)
                    tasks["web"] = asyncio.wait_for(
                        search_web(web_query), timeout=web_timeout
                    )
                try:
                    add_rewrite(
                        session_id,
                        RewriteRecord(
                            raw_query=user_input,
                            query_rewritten=web_query,
                            applied_slots=["rewrite_llm"],
                        ),
                    )
                except Exception:
                    pass
            # ì‚¬ìš©ì ê°•ì œ ì§€ì‹œ íœ´ë¦¬ìŠ¤í‹± ì œê±°: ë¼ìš°í„°/ê°€ë“œ ê²°ê³¼ì— ë”°ë¦„
            else:
                logger.info(f"[turn:{turn_id}] schedule web=off")

            # === ë¼ìš°íŒ… ìµœì¢… í™•ì • í›„ conv í—ˆìš© ì—¬ë¶€ ì¬ê³„ì‚° ===
            allow_conv = not (need_rag or need_web)
            logger.info(f"[turn:{turn_id}] allow_conv(final)={allow_conv}")
            # ì•ˆì „ ë¶„ê¸°: ì„ë² ë”© ìš°ì„  ê²½ë¡œì—ì„œëŠ” predê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©
            p_rag_cal = float(locals().get("pred", {}).get("p_rag_cal", need_rag_prob))
            p_web_cal = float(locals().get("pred", {}).get("p_web_cal", need_web_prob))
            r_boost = float(locals().get("pred", {}).get("r_boost", 0.0))
            w_boost = float(locals().get("pred", {}).get("w_boost", 0.0))
            logger.info(
                f"[turn:{turn_id}] cal_p(rag)={p_rag_cal:.3f}+{r_boost:.2f}->{need_rag_prob:.3f} "
                f"cal_p(web)={p_web_cal:.3f}+{w_boost:.2f}->{need_web_prob:.3f} "
                f"fastpath={fastpath} allow_conv={allow_conv}"
            )

            # í”Œë˜ë„ˆ ë¡œê·¸ (ê´€ì¸¡ìš©)
            try:
                aux_label_str = locals().get("aux_label", None)
            except Exception:
                aux_label_str = None
            try:
                # pred ë¯¸ì •ì˜ ì‹œ í˜„ì¬ ê²°ì •ê°’ì„ ì‚¬ìš©
                raw_decision = {
                    "need_rag": bool(
                        locals().get("pred", {}).get("need_rag", need_rag)
                    ),
                    "need_web": bool(
                        locals().get("pred", {}).get("need_web", need_web)
                    ),
                }
                amb = {
                    "rag": bool(locals().get("rag_amb", False)),
                    "web": bool(locals().get("web_amb", False)),
                }
                reason = "rules_hit"
                if max_prob < max(TAU_RAG, TAU_WEB) - LOW_CONF_MARGIN:
                    reason = "low_conf"
                elif not (need_rag or need_web) and (
                    locals().get("rag_amb", False) or locals().get("web_amb", False)
                ):
                    reason = "cheap_parallel"
                elif fastpath:
                    reason = "fastpath"
                pl = PlannerLog(
                    p_rag_calib=float(p_rag_cal),
                    p_web_calib=float(p_web_cal),
                    prior_rag=float(r_boost),
                    prior_web=float(w_boost),
                    tau=float(max(TAU_RAG, TAU_WEB)),
                    delta=float(AMBIGUITY_BAND),
                    low_conf=float(max(TAU_RAG, TAU_WEB) - LOW_CONF_MARGIN),
                    fast_margin=float(fastpath_margin),
                    raw_decision=raw_decision,
                    amb=amb,
                    aux_router=str(aux_label_str or "none"),
                    final_decision={
                        "need_rag": bool(need_rag),
                        "need_web": bool(need_web),
                        "allow_conv": bool(allow_conv),
                    },
                    reason=reason,
                    time_budget_ms=int(max(rag_timeout, web_timeout) * 1000),
                )
                log_planner(pl)
            except Exception as _pl_e:
                logger.warning(f"[turn:{turn_id}] planner log error={repr(_pl_e)}")

            key_list = list(tasks.keys())
            t1 = time.time()
            results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            gather_ms = (time.time() - t1) * 1000

            out_map = {}
            for k, r in zip(key_list, results):
                if isinstance(r, Exception):
                    logger.warning(f"[turn:{turn_id}] task={k} exception={repr(r)}")
                    out_map[k] = ""
                else:
                    out_map[k] = r or ""
                logger.info(f"[turn:{turn_id}] task={k} ctx_len={len(out_map[k])}")

            rag_ctx = out_map.get("rag", "")
            web_ctx = out_map.get("web", "")
            mobile_ctx = out_map.get("mobile", "")
            conv_ctx = out_map.get("conv", "") if allow_conv else ""

            # ì¦ê±° ë²ˆë“¤ ìˆ˜ì§‘(3ë¶„ ìºì‹œ): í•„ìš” ì‹œ ì›¹/ë¼ê·¸ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•
            try:
                if need_rag or need_web:
                    q2 = (
                        locals().get("rag_query_text")
                        or locals().get("web_query")
                        or user_input
                    ) or user_input
                    timeout_bw = max(rag_timeout, web_timeout)
                    bundle, web_ctx2, rag_ctx2 = await asyncio.wait_for(
                        build_evidence(
                            MCP_SERVER_URL,
                            session_id,
                            q2,
                            need_web,
                            need_rag,
                            timeout_bw,
                        ),
                        timeout=min(timeout_bw + 0.3, 3.0),
                    )
                    if web_ctx2:
                        web_ctx = web_ctx2
                    if rag_ctx2:
                        rag_ctx = rag_ctx2
            except Exception as _ev_e:
                logger.warning(f"[turn:{turn_id}] evidence build error={repr(_ev_e)}")
            # SINGLE_CALL_CONV ëª¨ë“œì—ì„œëŠ” conv ì²´ì¸ì„ ë¹„í™œì„±í™”í•˜ë¯€ë¡œ, ìµœì¢… ì‘ë‹µì˜ ë§¥ë½ ì†ì‹¤ì„ ë§‰ê¸° ìœ„í•´
            # conv_ctxê°€ ë¹„ì–´ ìˆê³  ìˆœìˆ˜ ëŒ€í™” ëª¨ë“œ(allow_conv=True)ë¼ë©´ ìµœê·¼ íˆìŠ¤í† ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•œë‹¤.
            if allow_conv and SINGLE_CALL_CONV and not conv_ctx:
                # ê³¼ë„í•œ ê¸¸ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœê·¼ íˆìŠ¤í† ë¦¬ë§Œ ì‚¬ìš©
                conv_ctx = hist[-2000:]
            logger.info(
                f"[turn:{turn_id}] gather_ms={gather_ms:.1f} rag_len={len(rag_ctx)} web_len={len(web_ctx)} mobile_len={len(mobile_ctx)} conv_len={len(conv_ctx)}"
            )

            # RAG ì˜ë¯¸ ë¶ˆì¼ì¹˜ í•„í„° ì ìš© (ì§§ì€ íƒ€ì„ì•„ì›ƒ)
            if rag_ctx:
                try:
                    _rag_ctx_prev_len = len(rag_ctx)
                    rag_ctx = await filter_semantic_mismatch(user_input, rag_ctx)
                    logger.info(
                        f"[turn:{turn_id}] rag_filter len_in={_rag_ctx_prev_len} len_out={len(rag_ctx)}"
                    )
                except Exception as e:
                    logger.warning(
                        f"[turn:{turn_id}] rag_mismatch_filter error={repr(e)}"
                    )

            # WEB ì»¨í…ìŠ¤íŠ¸ í•„í„° ì ìš© (í˜„ ì‚¬ìš©ì ì¿¼ë¦¬ ê¸°ì¤€)
            if web_ctx:
                try:
                    _web_ctx_prev_len = len(web_ctx)
                    web_ctx = await filter_web_ctx(user_input, web_ctx)
                    logger.info(
                        f"[turn:{turn_id}] web_filter len_in={_web_ctx_prev_len} len_out={len(web_ctx)}"
                    )
                except Exception as e:
                    logger.warning(f"[turn:{turn_id}] web_filter error={repr(e)}")

            # ğŸ”§ warm-up stream: ì™¸ë¶€ ê·¼ê±° ëª¨ë“œì¸ë° ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆì„ ë•Œ ì¦‰ì‹œ í”„ë¦¬ì•°ë¸” 1~2ë¬¸ì¥
            # â€˜íƒìƒ‰ ì¤‘' í”„ë¦¬ì•°ë¸”ì€ ì—„ê²©í•œ ì›¹ ì˜ë„ì—ì„œë§Œ, ê·¸ë¦¬ê³  ì‹¤ì œ ê²€ìƒ‰ì„ ìš”ì²­í–ˆìœ¼ë‚˜ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆì„ ë•Œë§Œ ë…¸ì¶œ
            if (need_web and not need_rag) and (not rag_ctx and not web_ctx) and True:
                try:
                    await websocket.send_text(
                        "[íƒìƒ‰ ì¤‘] ê´€ë ¨ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤. ê·¼ê±° í™•ë³´ í›„ ë‹µë³€ì„ ì´ì–´ê°€ê² ìŠµë‹ˆë‹¤.\n"
                    )
                    logger.info(f"[turn:{turn_id}] warmup sent")
                except Exception as e:
                    logger.warning(f"[turn:{turn_id}] warmup send error={repr(e)}")

            # 4) ì‚¬ì „ ê²€ì¦: ì§ˆë¬¸-ì»¨í…ìŠ¤íŠ¸ ì í•©ì„±. ë¬¸ì œì‹œ ì¬ì‹œë„/ëª…í™•í™”
            # ì†Œê·œëª¨ ì¸ì‚¬/ì¡ë‹´ì´ë©´ ëª…ì‹œì ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì œê±° í›„ ì¸ì‚¬ ë˜í¼ ì ìš©
            if _is_small_talk(user_input):
                rag_ctx = ""
                web_ctx = ""
                conv_ctx = ""
                try:
                    greet = wrap_greeting_reply(user_input)
                    await websocket.send_text(greet)
                    continue
                except Exception:
                    pass

            if USE_LLM_ROUTER:
                try:
                    pre_msgs = [
                        {
                            "role": "system",
                            "content": (
                                "ë„ˆëŠ” ì‚¬ì „ ê²€ì¦ê¸°ë‹¤. ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ë¬´ê´€/ìƒì¶©/ì¶”ì¸¡ì´ë©´ 'clarify'ì— ì§§ì€ ì§ˆë¬¸,"
                                " ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´. JSONë§Œ: {clarify:string}."
                            ),
                        },
                        {
                            "role": "user",
                            "content": f"[ì§ˆë¬¸]\n{user_input}\n\n[RAG]\n{rag_ctx[:800]}\n\n[WEB]\n{web_ctx[:800]}",
                        },
                    ]
                    kwargs_pv = {
                        "model": LLM_MODEL,
                        "messages": pre_msgs,
                        "max_tokens": 60,
                    }
                    if _model_supports_response_format(LLM_MODEL):
                        kwargs_pv["response_format"] = {"type": "json_object"}
                    pv = await asyncio.wait_for(
                        openai_chat_with_retry(**kwargs_pv),
                        timeout=PREVALIDATE_TIMEOUT_S,
                    )
                    pv_text = (pv.choices[0].message.content or "").strip()
                    try:
                        pv_json = json.loads(pv_text)
                        clarify_q = (pv_json.get("clarify") or "").strip()
                    except Exception:
                        clarify_q = ""
                    if clarify_q:
                        await websocket.send_text(clarify_q)
                        continue  # ì‚¬ìš©ìì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í›„ ë‹¤ìŒ í„´ì—ì„œ ì¬ì‹œë„
                except Exception:
                    pass

            # ì„ íƒ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±: TurnSummary Top-3 + STWM ìµœì‹  ìŠ¤ëƒ…ìƒ· â†’ aux_ctxë¡œ ë¶„ë¦¬
            try:
                q_for_select = (rag_query_text or web_query or user_input) or user_input
                sums_all = tb_get_summaries(session_id)
                picked = select_summaries(
                    q_for_select, sums_all, topk_bm25=10, topk_final=3
                )
                picked_texts = [s.answer_summary for s in picked]
            except Exception:
                picked_texts = []
            sum_block = (
                ("[íšŒì°¨ ìš”ì•½]\n" + "\n".join(f"- {t}" for t in picked_texts))
                if picked_texts
                else ""
            )
            # STWM ìµœì‹  ìŠ¤ëƒ…ìƒ· ë³‘í•©(ì •í˜• ìŠ¬ë¡¯ë§Œ): ê°œì¸í™” íšŒìƒ ê°•í™”
            try:
                stwm_dict = get_stwm_snapshot(session_id)
                stwm_lines = []
                if stwm_dict:
                    stwm_lines.append(
                        "ì´ë¦„: " + str(stwm_dict.get("last_person") or "")
                    )
                    stwm_lines.append("ì¥ì†Œ: " + str(stwm_dict.get("last_loc") or ""))
                    stwm_lines.append("ì‹œê°„: " + str(stwm_dict.get("last_time") or ""))
                    stwm_lines.append("í–‰ìœ„: " + str(stwm_dict.get("last_act") or ""))
                    stwm_lines.append(
                        "ëŒ€ìƒ: " + str(stwm_dict.get("last_target") or "")
                    )
                    stwm_lines.append(
                        "ê°ì •: " + str(stwm_dict.get("last_emotion") or "")
                    )
                    stwm_lines.append("ì£¼ì œ: " + str(stwm_dict.get("last_topic") or ""))
                    stwm_lines.append(
                        "ì•„ì´í…œ: " + str(stwm_dict.get("last_item") or "")
                    )
                stwm_block = "[STWM]\n" + "\n".join(stwm_lines)
            except Exception:
                stwm_block = ""
            aug_blocks = "\n\n".join(b for b in [stwm_block, sum_block] if b)
            if aug_blocks:
                # aux_ctx ìŠ¬ë¡¯ì— ì €ì¥í•˜ì—¬ FINAL_PROMPTì—ì„œ í†¤/ë§¥ë½ shaping ì „ìš©ìœ¼ë¡œ ì‚¬ìš©
                st = SESSION_STATE.setdefault(session_id, {})
                st["aux_ctx"] = aug_blocks

            # conv-only ê²½ë¡œ: ì „ì²´ íˆìŠ¤í† ë¦¬ ëŒ€ì‹  ì„ë² ë”© ìœ ì‚¬ë„ Top-k ì„ íƒ ë¬¸ì¥ë§Œ conv_ctxì— íˆ¬ì…
            try:
                if allow_conv and not (rag_ctx.strip() or web_ctx.strip()):
                    # ì§ˆì˜+ì•µì»¤ í•©ì„± ì¿¼ë¦¬ë¡œ ìœ ì‚¬ë„ ê³„ì‚° â†’ ìœ„ì¹˜/ì‹œê°„/í–‰ìœ„/ëŒ€ìƒ ë¬¸ì¥ íƒˆë½ ë°©ì§€
                    stwm_dict_for_q = get_stwm_snapshot(session_id)
                    anchor_bits = []
                    for k in (
                        "last_loc",
                        "last_time",
                        "last_act",
                        "last_target",
                        "last_topic",
                    ):
                        v = (
                            str(stwm_dict_for_q.get(k) or "").strip()
                            if stwm_dict_for_q
                            else ""
                        )
                        if v:
                            anchor_bits.append(v)
                    composite_q = (user_input + " " + " ".join(anchor_bits)).strip()
                    conv_sel = _build_hints_by_embedding(
                        session_id, hist_msgs, composite_q
                    )
                    if conv_sel:
                        conv_ctx = "[ì„ íƒ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸]\n" + conv_sel
            except Exception:
                pass

            # ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ë©´ ëŒ€í™” í—ˆìš©(íšŒìƒ ê²½ë¡œ ë³´ì¥)
            if not allow_conv and not (rag_ctx.strip() or web_ctx.strip()):
                allow_conv = True
                conv_ctx = hist[-2000:]
                logger.info(
                    f"[turn:{turn_id}] conv_rescue enabled (no external context)"
                )

            # 5) ìµœì¢… ë©”ì¸ LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì¦ê±° ëª¨ë“œì—ì„œëŠ” ë˜í¼ ìš°ì„ )
            full_answer = await main_response(
                session_id,
                user_input,
                websocket,
                mobile_ctx,
                rag_ctx,
                web_ctx,
                conv_ctx,
            )

            # ëŒ€í™” ì €ì¥ (ìŠ¤íŠ¸ë¦¬ë° ëˆ„ì ë³¸) - ì €ì¥ ì „ ë ˆë“œì•¡ì…˜ ì ìš©
            try:
                safe_user = redact_text(user_input)
                safe_ai = redact_text(full_answer)
            except Exception:
                safe_user, safe_ai = user_input, full_answer
            stm.save_context({"input": safe_user}, {"output": safe_ai})
            logger.info(f"[turn:{turn_id}] saved to STM out_len={len(full_answer)}")

            # Evidence í¬ì¸í„° ì €ì¥(ì›¹/RAG ì»¨í…ìŠ¤íŠ¸ ì¡´ì¬ ì‹œ)
            try:
                if web_ctx.strip() or rag_ctx.strip():
                    __store_refs = __import__(
                        "backend.rag.refs", fromlist=["store_refs_from_contexts"]
                    ).store_refs_from_contexts
                    refs = __store_refs(session_id, web_ctx, rag_ctx)
                    logger.info(
                        f"[turn:{turn_id}] stored evidence refs count={len(refs)}"
                    )
            except Exception as _ref_e:
                logger.warning(
                    f"[turn:{turn_id}] evidence refs store error={repr(_ref_e)}"
                )

            # í„´ ë²„í¼ ë° ìš”ì•½ íŠ¸ë¦¬ê±°
            try:
                tb_add_turn(session_id, "assistant", full_answer)
            except Exception:
                pass
            try:
                await tb_maybe_summarize(session_id, stwm_snap)
            except Exception as _sum_e:
                logger.warning(f"[turn:{turn_id}] turn summary error={repr(_sum_e)}")

            # ë””ë²„ê·¸ ë©”íƒ€(ì˜µì…˜): JSONì€ UI ë¹„ë…¸ì¶œ, ë¡œê·¸/ì˜µì…˜ íƒ­ìš©
            if WS_DEBUG_META:
                try:
                    meta = {
                        "planner": {
                            "rag": bool(need_rag),
                            "web": bool(need_web),
                            "prob": {
                                "rag": need_rag_prob,
                                "web": need_web_prob,
                            },
                        },
                        "stwm": get_stwm_snapshot(session_id),
                    }
                    await websocket.send_text(
                        "\n[debug_meta] " + json.dumps(meta, ensure_ascii=False)
                    )
                except Exception:
                    pass

    except WebSocketDisconnect:
        logger.info(
            "[ws] disconnected session=%s, scheduling final rag update.", session_id
        )
        _enqueue_snapshot(session_id)
        schedule_directive_update(session_id, force=True)  # directive
        pass


# ì• ë§¤í•  ë•Œë§Œ í˜¸ì¶œ: ë¼ë²¨+ì¬ì‘ì„±+ëª…í™•í™” ì§ˆë¬¸ì„ í•œ ë²ˆì— ë°›ëŠ”ë‹¤.
async def router_one_call(user_input: str, hist: str) -> dict:
    # í”„ë¦¬í¼ JSON ì§€ì¹¨ìœ¼ë¡œ ë‹¨ì¼ í˜¸ì¶œ ë¼ìš°íŒ…: conv|rag|web + í•„ìš”í•œ ì¿¼ë¦¬/ëª…í™•í™”
    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ë¼ìš°íŒ… ì—ì´ì „íŠ¸ë‹¤. ì‚¬ìš©ì ì…ë ¥ê³¼ ìµœê·¼ íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³ , conv|rag|web ì¤‘ í•˜ë‚˜ë§Œ ê³ ë¥´ë¼. "
                "í•„ìš”ì‹œ rag_query ë˜ëŠ” web_queryë¥¼ ìƒì„±í•˜ê³ , ë¶ˆí™•ì‹¤í•˜ë©´ clarifyì— ì§§ì€ ì§ˆë¬¸ 1ê°œë¥¼ ë„£ì–´ë¼. JSONë§Œ ì¶œë ¥."
            ),
        },
        {"role": "user", "content": f"[hist]\n{hist[-1500:]}\n\n[input]\n{user_input}"},
    ]
    tmpl = '{"route":"conv|rag|web","rag_query":"","web_query":"","clarify":""}'
    # 1ì°¨: json_object ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ í”„ë¦¬í¼ ì¬ì‹œë„ â†’ ë§ˆì§€ë§‰ìœ¼ë¡œ conv í´ë°±
    try:
        resp1 = await openai_chat_with_retry(
            model=LLM_MODEL,
            messages=msgs,
            response_format={"type": "json_object"},
            max_tokens=REWRITE_MAX_TOKENS,
        )
        content1 = (resp1.choices[0].message.content or "").strip()
        data1 = json.loads(content1)
        if isinstance(data1, dict) and data1.get("route"):
            return data1
    except Exception:
        pass
    try:
        resp2 = await openai_chat_with_retry(
            model=LLM_MODEL, messages=msgs, max_tokens=REWRITE_MAX_TOKENS
        )
        content2 = (resp2.choices[0].message.content or "").strip()
        # ì•ˆì „ JSON ì¶”ì¶œ(ì²« {...} ë¸”ë¡)
        start = content2.find("{")
        end = content2.rfind("}")
        if start != -1 and end != -1 and end > start:
            data2 = json.loads(content2[start : end + 1])
            if isinstance(data2, dict) and data2.get("route"):
                return data2
    except Exception:
        pass
    return {"route": "conv"}


async def route_guard(user_input: str, hist: str) -> dict:
    """
    ìµœì¢… ë¼ìš°íŒ… ê°€ë“œ: í˜„ì¬ ì…ë ¥ê³¼ ìµœê·¼ íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  conv/rag/web ì¤‘ 1ê°œë§Œ ê°•ì œ ì„ íƒí•˜ê±°ë‚˜,
    ë¶ˆí™•ì‹¤í•˜ë©´ clarify ì§ˆë¬¸ì„ ë°˜í™˜í•œë‹¤. JSONë§Œ ë°˜í™˜.
    """
    msgs = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ìµœì¢… ë¼ìš°íŒ… ê°€ë“œë‹¤. í˜„ì¬ ì…ë ¥ì´ ì¸ì‚¿ë§/ì†Œí†µì´ë©´ 'conv'ë¥¼ ë°˜í™˜í•œë‹¤. "
                "ê°œì¸ ê³¼ê±° íšŒìƒ/ì§€ë‚œ ëŒ€í™” ë‚´ìš©ì´ë©´ 'rag', ì™¸ë¶€ ì •ë³´ íƒìƒ‰(ë¡œì»¬/ì‹œí™©/ì›¹ë¬¸ì„œ)ì´ë©´ 'web'ì„ ë°˜í™˜í•œë‹¤. "
                "ë¶ˆí™•ì‹¤í•˜ë©´ clarifyì— ì§§ì€ ì§ˆë¬¸ 1ê°œë§Œ. JSONë§Œ: {route, clarify}."
            ),
        },
        {"role": "user", "content": f"[hist]\n{hist[-1500:]}\n\n[input]\n{user_input}"},
    ]
    try:
        kwargs = {"model": LLM_MODEL, "messages": msgs, "max_tokens": 80}
        if _model_supports_response_format(LLM_MODEL):
            kwargs["response_format"] = {"type": "json_object"}
        resp = await openai_chat_with_retry(**kwargs)
        content = (resp.choices[0].message.content or "").strip()
        data = json.loads(content) if content.startswith("{") else {}
        route = (data.get("route") or "").strip()
        clarify = (data.get("clarify") or "").strip()
        if route in ("conv", "rag", "web") or clarify:
            return {"route": route, "clarify": clarify}
    except Exception:
        pass
    return {"route": "", "clarify": ""}


def rag_answerable(hits, metric="COSINE") -> bool:
    if not hits or len(hits[0]) == 0:
        return False
    sims = []
    for h in hits[0][:3]:
        d = getattr(h, "distance", None)
        s = getattr(h, "score", None)
        if metric == "COSINE":
            sim = 1.0 - float(d if d is not None else 1.0)
        else:
            sim = float(s if s is not None else 0.0)
        sims.append(sim)
    sims.sort(reverse=True)
    max_sim = sims[0]
    mean_sim = sum(sims) / len(sims)
    gap = sims[0] - (sims[1] if len(sims) > 1 else 0.0)
    # ì˜ˆì‹œ ê¸°ì¤€: ì¶©ë¶„íˆ ë†’ì€ ìƒí•œ + ê²©ì°¨ë¡œ ì˜¤íŒ ë°©ì§€
    return (max_sim >= 0.62) or (max_sim >= 0.55 and gap >= 0.08 and mean_sim >= 0.48)


async def async_search_web(query: str, display: int = 5) -> str:
    # ê¸°ë³¸ timeout í•˜ë‚˜ë§Œ ì§€ì •í•˜ì—¬ ValueError ë°©ì§€
    timeout = httpx.Timeout(min(TIMEOUT_WEB, 2.2))
    headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    params = {"query": query, "display": display}
    async with httpx.AsyncClient(timeout=timeout) as client_http:
        try:
            r = await client_http.get(
                "https://openapi.naver.com/v1/search/local.json",
                headers=headers,
                params=params,
            )
            data = r.json() if r.status_code == 200 else {}
        except Exception:
            data = {}
    items = data.get("items", []) or []
    return "\n".join(
        f"{i.get('title','').replace('<b>','').replace('</b>','')} â€” {i.get('roadAddress', i.get('address',''))}"
        for i in items[:5]
    )


# ë°±ì—”ë“œ ì‹¤í–‰
# uvicorn app:app --host 0.0.0.0 --port 8000 --reload

# ì›¹ì†Œìº£
# wscat -c ws://localhost:8000/ws/my-session


# ------------------------------
# ìˆ˜ë™ íŠ¸ë¦¬ê±°(í…ŒìŠ¤íŠ¸ìš©)
# ------------------------------
@app.post("/proactive/trigger/{user_id}")
async def trigger_proactive(user_id: str):
    try:
        from backend.proactive.agent import select_and_send

        sent = await select_and_send(user_id, max_send=1)
        return {"status": "ok", "sent": len(sent)}
    except Exception as e:
        return {"status": "error", "error": repr(e)}
