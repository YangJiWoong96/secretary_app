{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845f3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\1903131679.py:19: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  llm = OpenAI(\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: 환경 변수 및 라이브러리 불러오기 (이전 코드와 동일)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")  # .env에서 OPENAI_API_KEY 로드\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "# LangChain의 메모리 모듈에서 ConversationBufferMemory 임포트\n",
    "from langchain.memory import ConversationBufferMemory  # <--- 이 부분을 추가합니다.\n",
    "\n",
    "# Redis URL 설정 (기본값은 localhost:6379, 필요시 .env 파일에 REDIS_URL 정의)\n",
    "redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = OpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, temperature=0.7\n",
    ")  # temperature는 창의성 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72ed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\2150304891.py:22: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Redis를 활용한 멀티챗 시뮬레이션 ---\n",
      "세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\n",
      "종료: 'exit' 또는 'quit'\n",
      "\n",
      "현재 활성 세션: default_session\n",
      "AI 응답:  안녕하세요, 웅이님! 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  그렇군요, 사용자님. 제가 조금 더 자세한 정보를 알면 더 도움이 될 수 있을 것 같아요. 직장에서 무엇이 어려우신가요? 어떤 분위기인가요? 제가 도와드릴 수 있는 건 없을까요?\n",
      "AI 응답:  이해합니다, 웅이님. 인간관계는 정말 어려운 일이죠. 늘 그렇지는 않지만, 종종 충돌이 있을 수 있죠. 직장에서도 그런 문제가 있나요? 어떤 일을 하고 계신가요?\n",
      "AI 응답:  그렇군요, 웅이님. IT 개발은 정말 어려운 일이죠. 많은 도전과 압박이 있을 수 있습니다. 하지만 그만큼 보람도 있을 것 같아요. 어떤 기술을 다루시나요? 저도 기술적인 부분에서 도움이 필요하다면 제가 최선을 다해 도와드릴 수 있을 거예요.\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_session_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] 당신의 질문: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m대화를 종료합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# --- 멀티챗 구현 함수 ---\n",
    "\n",
    "\n",
    "def get_conversation_chain(session_id: str):\n",
    "    \"\"\"\n",
    "    주어진 session_id에 따라 Redis에 연결된 ConversationChain을 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. RedisChatMessageHistory 인스턴스 생성\n",
    "    redis_chat_history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=redis_url,\n",
    "    )\n",
    "\n",
    "    # 2. ConversationBufferMemory에 RedisChatMessageHistory를 chat_memory로 전달\n",
    "    #    return_messages=True로 설정하면 메시지 객체 리스트를 반환합니다.\n",
    "    memory = ConversationBufferMemory(\n",
    "        chat_memory=redis_chat_history,\n",
    "        return_messages=True,  # LLM에 메시지 객체 형태로 전달 (더 유연함)\n",
    "    )\n",
    "\n",
    "    # 3. ConversationChain에 수정된 memory 객체 전달\n",
    "    conversation = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=memory,  # <--- 수정된 부분입니다.\n",
    "        verbose=False,  # 대화 과정을 너무 자세히 보여주지 않기 위해 False로 설정\n",
    "        # 필요하다면 True로 변경하여 디버깅에 활용\n",
    "    )\n",
    "    return conversation\n",
    "\n",
    "\n",
    "# --- 메인 대화 루프 (이하 동일) ---\n",
    "\n",
    "print(\"--- Redis를 활용한 멀티챗 시뮬레이션 ---\")\n",
    "print(\"세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"  # 초기 세션 ID\n",
    "current_conversation = get_conversation_chain(current_session_id)\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # 'change <session_id>' 명령 처리\n",
    "            parts = user_input.split(\" \")\n",
    "            if len(parts) == 2:\n",
    "                new_session_id = parts[1]\n",
    "                if new_session_id == current_session_id:\n",
    "                    print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "                else:\n",
    "                    current_session_id = new_session_id\n",
    "                    current_conversation = get_conversation_chain(current_session_id)\n",
    "                    print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(\"잘못된 'change' 명령입니다. 사용법: change <session_id>\")\n",
    "            continue  # 다음 루프\n",
    "\n",
    "        # LLM에게 질문하고 응답 받기\n",
    "        ai_response = current_conversation.predict(input=user_input)\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        print(\"API 키를 확인하거나, Redis 서버가 실행 중인지 확인해주세요.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- 모든 세션의 최종 대화 기록 (Redis에서 직접 확인) ---\")\n",
    "# 실험에 사용된 모든 세션 ID를 여기에 추가하여 실제 Redis에 저장된 내용을 확인할 수 있습니다.\n",
    "# 위 예시에서 사용한 'default_session', 'user_a', 'session_1' 등을 넣어보세요.\n",
    "test_session_ids = [\"default_session\", \"user_a\", \"session_1\", \"user_b_test\"]\n",
    "for sid in test_session_ids:\n",
    "    try:\n",
    "        history_check = RedisChatMessageHistory(session_id=sid, url=redis_url)\n",
    "        if history_check.messages:  # 메시지가 있는 세션만 표시\n",
    "            print(f\"\\n--- 세션 '{sid}' 기록 ---\")\n",
    "            for msg in history_check.messages:\n",
    "                print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"세션 '{sid}' 기록을 가져오는 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9873c4d",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 1: 패키지 설치 (한 번만 실행)\n",
    "# !pip install langchain-community redis python-dotenv ipykernel\n",
    "\n",
    "# 셀 2: 환경 변수 로드 & 임포트\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 셀 3: LLM 및 Redis 메모리 초기화\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "memory = RedisChatMessageHistory(\n",
    "    session_id=\"default_session\", url=\"redis://localhost:6379/0\"\n",
    ")\n",
    "\n",
    "\n",
    "# 셀 4: 대화 함수 정의 (수정된 부분)\n",
    "def chat(input_text: str) -> str:\n",
    "    # 1) Redis에서 이전 메시지 리스트 불러오기\n",
    "    history = memory.messages  # ❌ get_messages()가 아니라 messages 속성 사용\n",
    "    # 2) HumanMessage 추가하여 LLM 호출\n",
    "    messages = history + [HumanMessage(content=input_text)]\n",
    "    response = llm(messages)\n",
    "    # 3) Redis에 대화 저장\n",
    "    memory.add_user_message(input_text)\n",
    "    memory.add_ai_message(response.content)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0281c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_10992\\68044283.py:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "# 셀 5: 멀티턴 대화 테스트\n",
    "print(chat(\"안녕.난 웅이라고 해.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd871b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송해요, 제가 이름을 잘못 말했네요. 사용자님의 이름은 웅이군요. 다시 한번 반가워요, 웅이님! 혼란을 드려 죄송합니다. 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"내 이름이 뭐라고?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960c890",
   "metadata": {},
   "source": [
    "### Gemini의 Redis + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a41625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# --- 환경 변수 로드 ---\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# --- 상수 정의 ---\n",
    "EMBEDDING_DIM = 1536  # OpenAI 'text-embedding-ada-002' 기준\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# --- LLM 및 임베딩 모델 초기화 ---\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# --- 프로필 DB 시뮬레이션 (실제로는 MongoDB 등 별도 DB 사용 추천) ---\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    \"\"\"Milvus 서버에 연결하고 연결 상태를 반환합니다.\"\"\"\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(collection_name, description):\n",
    "    \"\"\"지정된 스키마로 Milvus 컬렉션을 생성합니다.\"\"\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        return Collection(collection_name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\n",
    "            name=\"user_id\",\n",
    "            dtype=DataType.VARCHAR,\n",
    "            max_length=256,\n",
    "            is_partition_key=False,\n",
    "        ),\n",
    "        FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"created_at\", dtype=DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description, enable_dynamic_field=False)\n",
    "    collection = Collection(collection_name, schema)\n",
    "\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "    collection.create_index(\"embedding\", index_params)\n",
    "    print(f\"Milvus collection '{collection_name}' created successfully.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억 관리 (RAG & 프로필) 핵심 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    \"\"\"대화 세션 종료 후 장기 기억(프로필, 로그)을 업데이트합니다.\"\"\"\n",
    "    print(f\"\\n[{session_id}] 장기 기억 업데이트 시작...\")\n",
    "\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"업데이트할 대화 내용이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 최근 10개 메시지를 요약 대상으로 함 (조절 가능)\n",
    "    recent_messages = history.messages[-10:]\n",
    "    conversation_text = \"\\n\".join(\n",
    "        [f\"{msg.type}: {msg.content}\" for msg in recent_messages]\n",
    "    )\n",
    "\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화 내용을 바탕으로, 대화의 핵심 흐름과 뉘앙스를 상세히 요약해줘.\\n\\n{conversation}\"\n",
    "    )\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "    flow_summary = summary_chain.run(conversation=conversation_text)\n",
    "    print(f\"  - 흐름 요약 완료: {flow_summary[:50]}...\")\n",
    "\n",
    "    old_profile_str = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "\n",
    "    update_prompt_str = \"\"\"You are a profile manager AI. Based on the [Existing Profile] and [Latest Conversation Summary] below, update the [Existing Profile] with the latest information.\n",
    "1. Add new key information.\n",
    "2. If new information contradicts existing information, boldly modify or delete the old information.\n",
    "3. Ignore trivial content like simple greetings.\n",
    "4. Keep the profile concise and maintain the overall core.\n",
    "5. You MUST return the final result in JSON format only.\n",
    "\n",
    "[Existing Profile]:\n",
    "{old_profile}\n",
    "\n",
    "[Latest Conversation Summary]:\n",
    "{summary}\n",
    "\"\"\"\n",
    "    profile_update_prompt = PromptTemplate.from_template(update_prompt_str)\n",
    "    profile_update_chain = LLMChain(llm=llm, prompt=profile_update_prompt)\n",
    "\n",
    "    try:\n",
    "        new_profile_str = profile_update_chain.run(\n",
    "            old_profile=old_profile_str, summary=flow_summary\n",
    "        )\n",
    "        new_profile = json.loads(new_profile_str)\n",
    "        PROFILE_DB[session_id] = new_profile\n",
    "        print(\"  - 프로필 업데이트 완료.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\n",
    "            f\"  - 프로필 업데이트 실패: LLM이 유효한 JSON을 반환하지 않았습니다. 응답: {new_profile_str}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    get_milvus_connection()\n",
    "    profile_collection = create_milvus_collection(\n",
    "        PROFILE_COLLECTION_NAME, \"User Profiles\"\n",
    "    )\n",
    "    log_collection = create_milvus_collection(\n",
    "        LOG_COLLECTION_NAME, \"Conversation Log Summaries\"\n",
    "    )\n",
    "\n",
    "    # Track 1 (프로필 덮어쓰기)\n",
    "    profile_text = json.dumps(new_profile, ensure_ascii=False)\n",
    "    profile_embedding = embeddings.embed_query(profile_text)\n",
    "    profile_data = [\n",
    "        {\n",
    "            \"id\": session_id,\n",
    "            \"embedding\": profile_embedding,\n",
    "            \"text\": profile_text,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"profile\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    profile_collection.upsert(profile_data)\n",
    "    print(\"  - RAG: 프로필 벡터 덮어쓰기 완료.\")\n",
    "\n",
    "    # Track 2 (아카이브 추가)\n",
    "    log_embedding = embeddings.embed_query(flow_summary)\n",
    "    log_id = str(uuid.uuid4())\n",
    "    log_data = [\n",
    "        {\n",
    "            \"id\": log_id,\n",
    "            \"embedding\": log_embedding,\n",
    "            \"text\": flow_summary,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"log_summary\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    log_collection.insert(log_data)\n",
    "    print(\"  - RAG: 대화 흐름 아카이브 추가 완료.\")\n",
    "    print(f\"[{session_id}] 장기 기억 업데이트 종료.\")\n",
    "\n",
    "\n",
    "def retrieve_from_rag(session_id, query):\n",
    "    \"\"\"주어진 쿼리로 RAG DB에서 관련 정보를 검색합니다.\"\"\"\n",
    "    get_milvus_connection()\n",
    "    if not utility.has_collection(\n",
    "        PROFILE_COLLECTION_NAME\n",
    "    ) or not utility.has_collection(LOG_COLLECTION_NAME):\n",
    "        return \"아직 RAG DB에 저장된 정보가 없습니다.\"\n",
    "\n",
    "    profile_collection = Collection(PROFILE_COLLECTION_NAME)\n",
    "    log_collection = Collection(LOG_COLLECTION_NAME)\n",
    "    profile_collection.load()\n",
    "    log_collection.load()\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    profile_results = profile_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=1,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "    log_results = log_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=2,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "\n",
    "    rag_context = \"[프로필 정보]\\n\"\n",
    "    rag_context += (\n",
    "        profile_results[0][0].entity.get(\"text\")\n",
    "        if profile_results and profile_results[0]\n",
    "        else \"저장된 프로필 정보 없음\"\n",
    "    )\n",
    "    rag_context += \"\\n\\n[과거 대화 기록 요약]\\n\"\n",
    "    if log_results and log_results[0]:\n",
    "        for hit in log_results[0]:\n",
    "            rag_context += f\"- {hit.entity.get('text')}\\n\"\n",
    "    else:\n",
    "        rag_context += \"저장된 과거 대화 없음\"\n",
    "\n",
    "    return rag_context\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: 단기 기억 및 대화 체인 구성\n",
    "# ----------------------------------------------------------------------\n",
    "def get_short_term_memory(session_id: str):\n",
    "    \"\"\"단기 기억(Redis)을 위한 메모리 객체를 반환합니다.\"\"\"\n",
    "    redis_chat_history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_chat_history,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"input\",\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful and friendly AI assistant.\n",
    "Please answer the user's question based on the [Long-Term Memory] and [Recent Conversation History] provided below.\n",
    "\n",
    "[Long-Term Memory - Everything you know about this user]:\n",
    "{rag_context}\n",
    "\n",
    "[Recent Conversation History]:\n",
    "{chat_history}\n",
    "\n",
    "User's Question: {input}\n",
    "AI Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"input\"], template=prompt_template\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n--- 최종 아키텍처 시뮬레이션 (Milvus v2.4.0 호환) ---\")\n",
    "print(\"세션 변경: 'change <session_id>'\")\n",
    "print(\"장기 기억 저장: 'save'\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower() == \"save\":\n",
    "            update_long_term_memory(current_session_id)\n",
    "            continue\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # ... (세션 변경 로직은 이전과 동일) ...\n",
    "            parts = user_input.split(\" \", 1)\n",
    "            new_session_id = parts[1]\n",
    "            if new_session_id != current_session_id:\n",
    "                current_session_id = new_session_id\n",
    "                print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "            continue\n",
    "\n",
    "        # 1. RAG에서 장기 기억 검색\n",
    "        rag_context = retrieve_from_rag(current_session_id, user_input)\n",
    "\n",
    "        # 2. Redis에서 단기 기억 로드\n",
    "        short_term_memory = get_short_term_memory(current_session_id)\n",
    "        # memory.load_memory_variables()는 비동기 호출 등이 필요할 수 있어, 여기서는 수동으로 context를 구성합니다.\n",
    "        # 체인에 직접 전달하기 위해 대화 기록을 문자열로 포맷팅합니다.\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"{msg.type}: {msg.content}\"\n",
    "                for msg in short_term_memory.chat_memory.messages\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. LLMChain으로 최종 프롬프트 실행\n",
    "        conversation_chain = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "        ai_response = conversation_chain.predict(\n",
    "            rag_context=rag_context, chat_history=chat_history_str, input=user_input\n",
    "        )\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "        # 4. 대화 기록을 단기 기억(Redis)에 수동으로 저장\n",
    "        short_term_memory.save_context({\"input\": user_input}, {\"output\": ai_response})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e095c",
   "metadata": {},
   "source": [
    "### GPT의 Redis + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# 상수\n",
    "EMBEDDING_DIM = 1536  # text-embedding-ada-002\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# 모델 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억 관리 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    print(f\"\\n[{session_id}] 장기 기억 업데이트 시작...\")\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"  - 저장된 대화 없음.\")\n",
    "        return\n",
    "\n",
    "    # 최근 10개 요약\n",
    "    recent = history.messages[-10:]\n",
    "    conv = \"\\n\".join(f\"{m.type}: {m.content}\" for m in recent)\n",
    "    summary_tpl = PromptTemplate.from_template(\"다음 대화를 요약해줘:\\n{conversation}\")\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_tpl)\n",
    "    flow_summary = summary_chain.run(conversation=conv)\n",
    "    print(\"  - 흐름 요약 완료.\")\n",
    "\n",
    "    # 프로필 업데이트\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    update_prompt = PromptTemplate.from_template(\n",
    "        \"[Existing Profile]:\\n{old}\\n[Summary]:\\n{sum}\\n\"\n",
    "        + \"업데이트된 프로필 JSON만 출력해줘.\"\n",
    "    )\n",
    "    update_chain = LLMChain(llm=llm, prompt=update_prompt)\n",
    "    new_prof_str = update_chain.run(old=old_prof, sum=flow_summary)\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "        print(\"  - 프로필 업데이트 완료.\")\n",
    "    except Exception:\n",
    "        print(f\"  - JSON 파싱 에러: {new_prof_str}\")\n",
    "        return\n",
    "\n",
    "    # Milvus에 저장\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    # Track1: 프로필 업서트\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - 프로필 벡터 업서트 완료.\")\n",
    "\n",
    "    # Track2: 로그 추가\n",
    "    log_emb = embeddings.embed_query(flow_summary)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": flow_summary,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"[{session_id}] 장기 기억 업데이트 완료.\")\n",
    "\n",
    "\n",
    "def retrieve_from_rag(session_id: str, query: str) -> str:\n",
    "    get_milvus_connection()\n",
    "    if not (\n",
    "        utility.has_collection(PROFILE_COLLECTION_NAME)\n",
    "        and utility.has_collection(LOG_COLLECTION_NAME)\n",
    "    ):\n",
    "        return \"저장된 RAG 정보 없음.\"\n",
    "\n",
    "    prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "    log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "    prof_coll.load()\n",
    "    log_coll.load()\n",
    "\n",
    "    q_emb = embeddings.embed_query(query)\n",
    "    params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "    prof_res = prof_coll.search(\n",
    "        [q_emb], \"embedding\", params, limit=1, expr=f\"user_id=='{session_id}'\"\n",
    "    )\n",
    "    log_res = log_coll.search(\n",
    "        [q_emb], \"embedding\", params, limit=2, expr=f\"user_id=='{session_id}'\"\n",
    "    )\n",
    "\n",
    "    context = \"[프로필]\\n\"\n",
    "    context += prof_res[0][0].entity.get(\"text\") if prof_res and prof_res[0] else \"없음\"\n",
    "    context += \"\\n[과거 대화 요약]\\n\"\n",
    "    if log_res and log_res[0]:\n",
    "        for hit in log_res[0]:\n",
    "            context += f\"- {hit.entity.get('text')}\\n\"\n",
    "    else:\n",
    "        context += \"없음\"\n",
    "    return context\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: 단기 기억 및 체인 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def get_short_term_memory(session_id: str):\n",
    "    hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"input\"],\n",
    "    template=(\n",
    "        \"You are an AI assistant.\\n\"\n",
    "        \"[Long-Term Memory]\\n{rag_context}\\n\"\n",
    "        \"[Recent]\\n{chat_history}\\n\"\n",
    "        \"Q: {input}\\nA:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- 챗봇 시작: 'change <id>', 'save', 'exit' 사용 ---\")\n",
    "    session_id = \"default_session\"\n",
    "    while True:\n",
    "        cmd = input(f\"[{session_id}]> \")\n",
    "        if cmd.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        if cmd.lower() == \"save\":\n",
    "            update_long_term_memory(session_id)\n",
    "            continue\n",
    "        if cmd.lower().startswith(\"change \"):\n",
    "            new = cmd.split(\" \", 1)[1]\n",
    "            if new != session_id:\n",
    "                session_id = new\n",
    "                print(f\"세션 변경: {session_id}\")\n",
    "            else:\n",
    "                print(\"이미 같은 세션입니다.\")\n",
    "            continue\n",
    "\n",
    "        # 일반 질문 처리\n",
    "        rag_ctx = retrieve_from_rag(session_id, cmd)\n",
    "        stm = get_short_term_memory(session_id)\n",
    "        hist_str = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "        chain = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "        ans = chain.predict(rag_context=rag_ctx, chat_history=hist_str, input=cmd)\n",
    "        print(f\"Bot: {ans}\")\n",
    "        stm.save_context({\"input\": cmd}, {\"output\": ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250730\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken  # 토큰 계산용\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# 차원 설정\n",
    "EMBEDDING_DIM = 384 if EMBEDDING_MODEL.endswith(\"3-small\") else 1536\n",
    "\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# LLM 및 임베딩 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    # 로그용 IVF+PQ 인덱스\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128, \"m\": 8}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억(RAG) 업데이트\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    print(f\"\\n[{session_id}] RAG 업데이트 트리거: 3000토큰 도달\")\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"  - 저장된 대화 없음\")\n",
    "        return\n",
    "\n",
    "    # 전체 대화 및 토큰 수\n",
    "    conv_all = \"\\n\".join(f\"{m.type}: {m.content}\" for m in history.messages)\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    count = len(enc.encode(conv_all))\n",
    "\n",
    "    # 요약: 인사/잡담 제거, 핵심 정보만\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화에서 인사말과 불필요한 잡담을 제거하고, 사용자 프로필에 유의미한 핵심 정보만 요약해줘.\\n{conversation}\"\n",
    "    )\n",
    "    summary_text = LLMChain(llm=llm, prompt=summary_prompt).run(conversation=conv_all)\n",
    "    print(f\"  - 요약 완료 (토큰 수: {count})\")\n",
    "\n",
    "    # 프로필 업데이트\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    profile_update_tpl = PromptTemplate.from_template(\n",
    "        \"[기존 프로필]\\n{old}\\n[요약된 최신 대화]\\n{sum}\\n\"\n",
    "        + \"위 내용을 반영하여 간결한 JSON 프로필로 반환해줘.\"\n",
    "    )\n",
    "    new_prof_str = LLMChain(llm=llm, prompt=profile_update_tpl).run(\n",
    "        old=old_prof, sum=summary_text\n",
    "    )\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "        print(\"  - 프로필 업데이트 완료\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  - 프로필 JSON 파싱 오류: {new_prof_str}\")\n",
    "        return\n",
    "\n",
    "    # Milvus 저장\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    # 프로필 벡터 upsert\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 프로필 저장 완료\")\n",
    "\n",
    "    # 로그 아카이브 insert\n",
    "    log_emb = embeddings.embed_query(summary_text)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": summary_text,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 대화 로그 저장 완료\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: RAG 검색 및 단기 기억 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_from_rag(session_id: str, query: str, top_k: int = 2):\n",
    "    \"\"\"Milvus에서 프로필과 로그를 검색하여 RAG 컨텍스트를 생성합니다.\"\"\"\n",
    "    try:\n",
    "        get_milvus_connection()\n",
    "        prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "        log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "        prof_coll.load()\n",
    "        log_coll.load()\n",
    "\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "        prof_res = prof_coll.search(\n",
    "            [query_emb], \"embedding\", params, limit=1, expr=f\"user_id == '{session_id}'\"\n",
    "        )\n",
    "        log_res = log_coll.search(\n",
    "            [query_emb],\n",
    "            \"embedding\",\n",
    "            params,\n",
    "            limit=top_k,\n",
    "            expr=f\"user_id == '{session_id}'\",\n",
    "        )\n",
    "\n",
    "        context = \"[프로필]\\n\"\n",
    "        if prof_res and prof_res[0]:\n",
    "            context += f\"- {prof_res[0][0].entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 프로필 없음\\n\"\n",
    "\n",
    "        context += \"\\n[과거 대화 기록]\\n\"\n",
    "        if log_res and log_res[0]:\n",
    "            for hit in log_res[0]:\n",
    "                context += f\"- {hit.entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 대화 기록 없음\"\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"  - RAG 검색 오류: {e}\")\n",
    "        return \"[장기 기억 정보 없음]\"\n",
    "\n",
    "\n",
    "def get_short_term_memory(session_id: str):\n",
    "    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"question\"],\n",
    "    template=(\n",
    "        \"당신은 전문적이면서도 친근한 개인 비서 AI입니다.\\n\"\n",
    "        \"[장기 기억]\\n{rag_context}\\n\"\n",
    "        \"[단기 기억]\\n{chat_history}\\n\"\n",
    "        \"사용자 질문: {question}\\n\"\n",
    "        \"→ 위 정보를 참고하여 상세히 답변해 주세요.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프 (change/save/exit)\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- 챗봇 시작 ---\")\n",
    "    session_id = \"default_session\"\n",
    "    while True:\n",
    "        user_input = input(f\"[{session_id}]> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        if user_input.lower() == \"save\":\n",
    "            update_long_term_memory(session_id)\n",
    "            continue\n",
    "        if user_input.lower().startswith(\"change \"):\n",
    "            parts = user_input.split(\" \", 1)\n",
    "            if len(parts) > 1 and parts[1].strip():\n",
    "                new_id = parts[1].strip()\n",
    "                if new_id != session_id:\n",
    "                    session_id = new_id\n",
    "                    print(f\"세션 변경: {session_id}\")\n",
    "            else:\n",
    "                print(\"사용법: change <새로운 세션 ID>\")\n",
    "            continue\n",
    "\n",
    "        # 자동 RAG 트리거\n",
    "        stm = get_short_term_memory(session_id)\n",
    "        rag_ctx = retrieve_from_rag(session_id, user_input)\n",
    "        hist_str = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        if len(enc.encode(hist_str)) >= 3000:\n",
    "            update_long_term_memory(session_id)\n",
    "\n",
    "        # 응답 생성\n",
    "        ans = LLMChain(llm=llm, prompt=FINAL_PROMPT, verbose=False).predict(\n",
    "            rag_context=rag_ctx, chat_history=hist_str, question=user_input\n",
    "        )\n",
    "        print(f\"AI: {ans}\")\n",
    "        stm.save_context({\"input\": user_input}, {\"output\": ans})\n",
    "    print(\"챗봇 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 1~3: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "EMBEDDING_DIM = 384 if EMBEDDING_MODEL.endswith(\"3-small\") else 1536\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# LangChain LLM 및 임베딩 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128, \"m\": 8}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억(RAG) 업데이트\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    print(f\"\\n[{session_id}] RAG 업데이트 트리거: 3000토큰 도달\")\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"  - 저장된 대화 없음\")\n",
    "        return\n",
    "\n",
    "    # 전체 대화 및 토큰 수\n",
    "    conv_all = \"\\n\".join(f\"{m.type}: {m.content}\" for m in history.messages)\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    count = len(enc.encode(conv_all))\n",
    "\n",
    "    # 요약: 인사/잡담 제거, 핵심 정보만\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화에서 인사말과 불필요한 잡담을 제거하고, 사용자 프로필에 유의미한 핵심 정보만 요약해줘.\\n{conversation}\"\n",
    "    )\n",
    "    summary_text = LLMChain(llm=llm, prompt=summary_prompt).run(conversation=conv_all)\n",
    "    print(f\"  - 요약 완료 (토큰 수: {count})\")\n",
    "\n",
    "    # 프로필 업데이트\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    profile_update_tpl = PromptTemplate.from_template(\n",
    "        \"[기존 프로필]\\n{old}\\n[요약된 최신 대화]\\n{sum}\\n\"\n",
    "        \"위 내용을 반영하여 간결한 JSON 프로필로 반환해줘.\"\n",
    "    )\n",
    "    new_prof_str = LLMChain(llm=llm, prompt=profile_update_tpl).run(\n",
    "        old=old_prof, sum=summary_text\n",
    "    )\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "        print(\"  - 프로필 업데이트 완료\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  - 프로필 JSON 파싱 오류: {new_prof_str}\")\n",
    "        return\n",
    "\n",
    "    # Milvus 저장\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    # 프로필 벡터 upsert\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 프로필 저장 완료\")\n",
    "\n",
    "    # 로그 아카이브 insert\n",
    "    log_emb = embeddings.embed_query(summary_text)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": summary_text,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 대화 로그 저장 완료\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: RAG 검색 및 단기 기억 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_from_rag(session_id: str, query: str, top_k: int = 2):\n",
    "    try:\n",
    "        get_milvus_connection()\n",
    "        prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "        log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "        prof_coll.load()\n",
    "        log_coll.load()\n",
    "\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "        prof_res = prof_coll.search(\n",
    "            [query_emb], \"embedding\", params, limit=1, expr=f\"user_id == '{session_id}'\"\n",
    "        )\n",
    "        log_res = log_coll.search(\n",
    "            [query_emb],\n",
    "            \"embedding\",\n",
    "            params,\n",
    "            limit=top_k,\n",
    "            expr=f\"user_id == '{session_id}'\",\n",
    "        )\n",
    "\n",
    "        context = \"[프로필]\\n\"\n",
    "        if prof_res and prof_res[0]:\n",
    "            context += f\"- {prof_res[0][0].entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 프로필 없음\\n\"\n",
    "\n",
    "        context += \"\\n[과거 대화 기록]\\n\"\n",
    "        if log_res and log_res[0]:\n",
    "            for hit in log_res[0]:\n",
    "                context += f\"- {hit.entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 대화 기록 없음\"\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"  - RAG 검색 오류: {e}\")\n",
    "        return \"[장기 기억 정보 없음]\"\n",
    "\n",
    "\n",
    "def get_short_term_memory(session_id: str):\n",
    "    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"question\"],\n",
    "    template=(\n",
    "        \"당신은 전문적이면서도 친근한 개인 비서 AI입니다.\\n\"\n",
    "        \"[장기 기억]\\n{rag_context}\\n\"\n",
    "        \"[단기 기억]\\n{chat_history}\\n\"\n",
    "        \"사용자 질문: {question}\\n\"\n",
    "        \"→ 위 정보를 참고하여 상세히 답변해 주세요.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FastAPI + WebSocket 서버 (셀 6 변경)\n",
    "# ----------------------------------------------------------------------\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def background_rag_update(session_id: str):\n",
    "    await asyncio.to_thread(update_long_term_memory, session_id)\n",
    "\n",
    "\n",
    "async def stream_llm_response(session_id: str, user_input: str, websocket: WebSocket):\n",
    "    rag_ctx = retrieve_from_rag(session_id, user_input)\n",
    "    stm = get_short_term_memory(session_id)\n",
    "    hist_str = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "\n",
    "    prompt = FINAL_PROMPT.format(\n",
    "        rag_context=rag_ctx, chat_history=hist_str, question=user_input\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"개인 비서 AI이며, 아래 지침에 따라 답하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL, messages=messages, stream=True, temperature=0.7\n",
    "    )\n",
    "    full_answer = \"\"\n",
    "    async for chunk in response:\n",
    "        token = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "        if token:\n",
    "            full_answer += token\n",
    "            await websocket.send_text(token)\n",
    "\n",
    "    stm.save_context({\"input\": user_input}, {\"output\": full_answer})\n",
    "\n",
    "\n",
    "@app.websocket(\"/ws/{session_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, session_id: str):\n",
    "    await websocket.accept()\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = await websocket.receive_text()\n",
    "            # 백그라운드 RAG 업데이트\n",
    "            asyncio.create_task(background_rag_update(session_id))\n",
    "            # 스트리밍 응답\n",
    "            await stream_llm_response(session_id, user_input, websocket)\n",
    "    except WebSocketDisconnect:\n",
    "        print(f\"[{session_id}] WebSocket disconnected\")\n",
    "\n",
    "\n",
    "# 실행:\n",
    "# uvicorn app:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 1~3: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "EMBEDDING_DIM = 384 if EMBEDDING_MODEL.endswith(\"3-small\") else 1536\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# LangChain LLM 및 임베딩 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128, \"m\": 8}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억(RAG) 업데이트\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    print(f\"\\n[{session_id}] RAG 업데이트 트리거: 3000토큰 도달\")\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"  - 저장된 대화 없음\")\n",
    "        return\n",
    "\n",
    "    # 전체 대화 및 토큰 수\n",
    "    conv_all = \"\\n\".join(f\"{m.type}: {m.content}\" for m in history.messages)\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    count = len(enc.encode(conv_all))\n",
    "\n",
    "    # 요약: 인사/잡담 제거, 핵심 정보만\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화에서 인사말과 불필요한 잡담을 제거하고, 사용자 프로필에 유의미한 핵심 정보만 요약해줘.\\n{conversation}\"\n",
    "    )\n",
    "    summary_text = LLMChain(llm=llm, prompt=summary_prompt).run(conversation=conv_all)\n",
    "    print(f\"  - 요약 완료 (토큰 수: {count})\")\n",
    "\n",
    "    # 프로필 업데이트\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    profile_update_tpl = PromptTemplate.from_template(\n",
    "        \"[기존 프로필]\\n{old}\\n[요약된 최신 대화]\\n{sum}\\n\"\n",
    "        \"위 내용을 반영하여 간결한 JSON 프로필로 반환해줘.\"\n",
    "    )\n",
    "    new_prof_str = LLMChain(llm=llm, prompt=profile_update_tpl).run(\n",
    "        old=old_prof, sum=summary_text\n",
    "    )\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "        print(\"  - 프로필 업데이트 완료\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  - 프로필 JSON 파싱 오류: {new_prof_str}\")\n",
    "        return\n",
    "\n",
    "    # Milvus 저장\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    # 프로필 벡터 upsert\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 프로필 저장 완료\")\n",
    "\n",
    "    # 로그 아카이브 insert\n",
    "    log_emb = embeddings.embed_query(summary_text)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": summary_text,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - RAG: 대화 로그 저장 완료\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: RAG 검색 및 단기 기억 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_from_rag(session_id: str, query: str, top_k: int = 2):\n",
    "    try:\n",
    "        get_milvus_connection()\n",
    "        prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "        log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "        prof_coll.load()\n",
    "        log_coll.load()\n",
    "\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "        prof_res = prof_coll.search(\n",
    "            [query_emb], \"embedding\", params, limit=1, expr=f\"user_id == '{session_id}'\"\n",
    "        )\n",
    "        log_res = log_coll.search(\n",
    "            [query_emb],\n",
    "            \"embedding\",\n",
    "            params,\n",
    "            limit=top_k,\n",
    "            expr=f\"user_id == '{session_id}'\",\n",
    "        )\n",
    "\n",
    "        context = \"[프로필]\\n\"\n",
    "        if prof_res and prof_res[0]:\n",
    "            context += f\"- {prof_res[0][0].entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 프로필 없음\\n\"\n",
    "\n",
    "        context += \"\\n[과거 대화 기록]\\n\"\n",
    "        if log_res and log_res[0]:\n",
    "            for hit in log_res[0]:\n",
    "                context += f\"- {hit.entity.get('text')}\\n\"\n",
    "        else:\n",
    "            context += \"- 저장된 대화 기록 없음\"\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"  - RAG 검색 오류: {e}\")\n",
    "        return \"[장기 기억 정보 없음]\"\n",
    "\n",
    "\n",
    "def get_short_term_memory(session_id: str):\n",
    "    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"question\"],\n",
    "    template=(\n",
    "        \"당신은 전문적이면서도 친근한 개인 비서 AI입니다.\\n\"\n",
    "        \"[장기 기억]\\n{rag_context}\\n\"\n",
    "        \"[단기 기억]\\n{chat_history}\\n\"\n",
    "        \"사용자 질문: {question}\\n\"\n",
    "        \"→ 위 정보를 참고하여 상세히 답변해 주세요.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FastAPI + WebSocket 서버 (셀 6 변경)\n",
    "# ----------------------------------------------------------------------\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def background_rag_update(session_id: str):\n",
    "    await asyncio.to_thread(update_long_term_memory, session_id)\n",
    "\n",
    "\n",
    "async def stream_llm_response(\n",
    "    session_id: str,\n",
    "    user_input: str,\n",
    "    websocket: WebSocket,\n",
    "    stm: ConversationSummaryBufferMemory,\n",
    "):\n",
    "    # 단기 메모리에서 히스토리 문자열 생성\n",
    "    hist_str = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "\n",
    "    # RAG 컨텍스트 생성\n",
    "    rag_ctx = retrieve_from_rag(session_id, user_input)\n",
    "\n",
    "    # 프롬프트 구성\n",
    "    prompt = FINAL_PROMPT.format(\n",
    "        rag_context=rag_ctx, chat_history=hist_str, question=user_input\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"개인 비서 AI이며, 아래 지침에 따라 답하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # OpenAI 스트리밍 호출\n",
    "    response = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL, messages=messages, stream=True, temperature=0.7\n",
    "    )\n",
    "    full_answer = \"\"\n",
    "    async for chunk in response:\n",
    "        token = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "        if token:\n",
    "            full_answer += token\n",
    "            await websocket.send_text(token)\n",
    "\n",
    "    # 단기 메모리 저장\n",
    "    stm.save_context({\"input\": user_input}, {\"output\": full_answer})\n",
    "\n",
    "\n",
    "@app.websocket(\"/ws/{session_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, session_id: str):\n",
    "    await websocket.accept()\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = await websocket.receive_text()\n",
    "\n",
    "            # 1. 단기 기억 로드 및 토큰 수 체크\n",
    "            stm = get_short_term_memory(session_id)\n",
    "            hist_str = \"\\n\".join(\n",
    "                f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages\n",
    "            )\n",
    "            enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            if len(enc.encode(hist_str)) >= 3000:\n",
    "                print(\n",
    "                    f\"[{session_id}] 토큰 수 임계점 도달. 백그라운드 RAG 업데이트 실행.\"\n",
    "                )\n",
    "                asyncio.create_task(background_rag_update(session_id))\n",
    "\n",
    "            # 2. 스트리밍 응답 (단기 메모리 stm 전달)\n",
    "            await stream_llm_response(session_id, user_input, websocket, stm)\n",
    "\n",
    "    except WebSocketDisconnect:\n",
    "        print(f\"[{session_id}] WebSocket disconnected\")\n",
    "\n",
    "\n",
    "# 실행:\n",
    "# uvicorn app:app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928577a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 1~3: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "EMBEDDING_DIM = 384 if EMBEDDING_MODEL.endswith(\"3-small\") else 1536\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# LangChain LLM 및 임베딩 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128, \"m\": 8}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억(RAG) 업데이트\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        return\n",
    "\n",
    "    conv_all = \"\\n\".join(f\"{m.type}: {m.content}\" for m in history.messages)\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화에서 인사말과 불필요한 잡담을 제거하고, \"\n",
    "        \"사용자 프로필에 유의미한 핵심 정보만 요약해줘.\\n{conversation}\"\n",
    "    )\n",
    "    summary_text = LLMChain(llm=llm, prompt=summary_prompt).run(conversation=conv_all)\n",
    "\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    profile_update_tpl = PromptTemplate.from_template(\n",
    "        \"[기존 프로필]\\n{old}\\n[요약된 최신 대화]\\n{sum}\\n\"\n",
    "        \"위 내용을 반영하여 간결한 JSON 프로필로 반환해줘.\"\n",
    "    )\n",
    "    new_prof_str = LLMChain(llm=llm, prompt=profile_update_tpl).run(\n",
    "        old=old_prof, sum=summary_text\n",
    "    )\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "    except json.JSONDecodeError:\n",
    "        return\n",
    "\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    log_emb = embeddings.embed_query(summary_text)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": summary_text,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: RAG 검색 및 단기 기억 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_from_rag(session_id: str, query: str, top_k: int = 2) -> str:\n",
    "    try:\n",
    "        get_milvus_connection()\n",
    "        prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "        log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "        prof_coll.load()\n",
    "        log_coll.load()\n",
    "\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "        prof_res = prof_coll.search(\n",
    "            [query_emb], \"embedding\", params, limit=1, expr=f\"user_id == '{session_id}'\"\n",
    "        )\n",
    "        log_res = log_coll.search(\n",
    "            [query_emb],\n",
    "            \"embedding\",\n",
    "            params,\n",
    "            limit=top_k,\n",
    "            expr=f\"user_id == '{session_id}'\",\n",
    "        )\n",
    "\n",
    "        context = \"\"\n",
    "        if prof_res and prof_res[0]:\n",
    "            context += f\"[RAG 프로필]\\n{prof_res[0][0].entity.get('text')}\\n\"\n",
    "        if log_res and log_res[0]:\n",
    "            for hit in log_res[0]:\n",
    "                context += f\"[RAG 로그]\\n{hit.entity.get('text')}\\n\"\n",
    "        return context or \"RAG 결과 없음\"\n",
    "    except:\n",
    "        return \"RAG 에러\"\n",
    "\n",
    "\n",
    "def get_short_term_memory(session_id: str) -> ConversationSummaryBufferMemory:\n",
    "    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 7: Naver Search Chain 구현\n",
    "# ----------------------------------------------------------------------\n",
    "def naver_search(query: str, display: int = 5) -> dict:\n",
    "    url = \"https://openapi.naver.com/v1/search/local.json\"\n",
    "    headers = {\"X-Naver-Client-Id\": CLIENT_ID, \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "    params = {\"query\": query, \"display\": display}\n",
    "    res = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "    return res.json() if res.status_code == 200 else {}\n",
    "\n",
    "\n",
    "def search_web(query: str) -> str:\n",
    "    data = naver_search(query)\n",
    "    items = data.get(\"items\", [])\n",
    "    if not items:\n",
    "        return \"검색 결과가 없습니다.\"\n",
    "    snippets = []\n",
    "    for item in items:\n",
    "        title = item.get(\"title\", \"\").replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n",
    "        address = item.get(\"roadAddress\", item.get(\"address\", \"\"))\n",
    "        snippets.append(f\"{title} — {address}\")\n",
    "    return \"\\n\".join(snippets[:5])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 8: Conversation Chain 구현\n",
    "# ----------------------------------------------------------------------\n",
    "async def conversation_chain(\n",
    "    session_id: str, user_input: str, stm: ConversationSummaryBufferMemory\n",
    ") -> str:\n",
    "    hist = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "    prompt = (\n",
    "        \"당신은 친근한 소통 전문가 팀입니다. 인사나 짧은 반응을 자연스럽게 처리하세요.\\n\"\n",
    "        f\"[대화 히스토리]\\n{hist}\\n\"\n",
    "        f\"[최신 입력]\\n{user_input}\"\n",
    "    )\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 9: Router LLM 구현\n",
    "# ----------------------------------------------------------------------\n",
    "async def call_router(session_id: str, user_input: str) -> list[str]:\n",
    "    system = (\n",
    "        \"당신은 비서실장(라우터)입니다. 아래 세 팀 중 어떤 팀이 필요할지 간단히 결정하세요:\\n\"\n",
    "        \"1) 기억 전문가 팀 (RAG)\\n\"\n",
    "        \"2) 정보 분석가 팀 (Web Search)\\n\"\n",
    "        \"3) 소통 전문가 팀 (Conversation)\\n\"\n",
    "        '출력 양식: JSON 리스트, 예: [\"RAG\",\"WebSearch\"]'\n",
    "    )\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    text = resp.choices[0].message.content\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        teams = []\n",
    "        if \"RAG\" in text:\n",
    "            teams.append(\"RAG\")\n",
    "        if \"WebSearch\" in text or \"검색\" in text:\n",
    "            teams.append(\"WebSearch\")\n",
    "        if \"Conversation\" in text or \"반응\" in text:\n",
    "            teams.append(\"Conversation\")\n",
    "        return teams or [\"Conversation\"]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 10: Main LLM 최종 응답 템플릿\n",
    "# ----------------------------------------------------------------------\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_ctx\", \"web_ctx\", \"conv_ctx\", \"question\"],\n",
    "    template=(\n",
    "        \"당신은 전문적이면서도 친근한 개인 비서 AI입니다.\\n\\n\"\n",
    "        \"[RAG 결과]\\n{rag_ctx}\\n\\n\"\n",
    "        \"[Web 검색 결과]\\n{web_ctx}\\n\\n\"\n",
    "        \"[소통 체인 결과]\\n{conv_ctx}\\n\\n\"\n",
    "        \"사용자 질문: {question}\\n\"\n",
    "        \"→ 위 모든 정보를 참고하여 완전한 답변을 제공하세요.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 11: FastAPI + WebSocket 서버\n",
    "# ----------------------------------------------------------------------\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def background_rag_update(session_id: str):\n",
    "    await asyncio.to_thread(update_long_term_memory, session_id)\n",
    "\n",
    "\n",
    "async def main_response(\n",
    "    session_id: str,\n",
    "    user_input: str,\n",
    "    websocket: WebSocket,\n",
    "    rag_ctx: str,\n",
    "    web_ctx: str,\n",
    "    conv_ctx: str,\n",
    ") -> str:\n",
    "    prompt = FINAL_PROMPT.format(\n",
    "        rag_ctx=rag_ctx, web_ctx=web_ctx, conv_ctx=conv_ctx, question=user_input\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"개인 비서 AI이며, 아래 지침에 따라 답하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL, messages=messages, stream=True, temperature=0.7\n",
    "    )\n",
    "\n",
    "    full_answer = \"\"\n",
    "    async for chunk in resp:\n",
    "        token = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "        if token:\n",
    "            full_answer += token\n",
    "            await websocket.send_text(token)\n",
    "\n",
    "    return full_answer\n",
    "\n",
    "\n",
    "@app.websocket(\"/ws/{session_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, session_id: str):\n",
    "    await websocket.accept()\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = await websocket.receive_text()\n",
    "\n",
    "            # 단기 기억 불러오기 + 토큰 체크 (백그라운드 RAG 업데이트)\n",
    "            stm = get_short_term_memory(session_id)\n",
    "            hist = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "            if len(enc.encode(hist)) >= 3000:\n",
    "                asyncio.create_task(background_rag_update(session_id))\n",
    "\n",
    "            # 1) 라우터 호출\n",
    "            teams = await call_router(session_id, user_input)\n",
    "\n",
    "            # 2) 전문가 팀 병렬 실행\n",
    "            tasks = {}\n",
    "            if \"RAG\" in teams:\n",
    "                tasks[\"rag\"] = asyncio.to_thread(\n",
    "                    retrieve_from_rag, session_id, user_input\n",
    "                )\n",
    "            if \"WebSearch\" in teams:\n",
    "                tasks[\"web\"] = asyncio.to_thread(search_web, user_input)\n",
    "            if \"Conversation\" in teams:\n",
    "                tasks[\"conv\"] = asyncio.create_task(\n",
    "                    conversation_chain(session_id, user_input, stm)\n",
    "                )\n",
    "\n",
    "            results = await asyncio.gather(*tasks.values())\n",
    "            rag_ctx = results[list(tasks).index(\"rag\")] if \"rag\" in tasks else \"\"\n",
    "            web_ctx = results[list(tasks).index(\"web\")] if \"web\" in tasks else \"\"\n",
    "            conv_ctx = results[list(tasks).index(\"conv\")] if \"conv\" in tasks else \"\"\n",
    "\n",
    "            # 3) 최종 메인 LLM 스트리밍 응답 및 full_answer 수집\n",
    "            full_answer = await main_response(\n",
    "                session_id, user_input, websocket, rag_ctx, web_ctx, conv_ctx\n",
    "            )\n",
    "\n",
    "            # 4) 완성된 답변으로 대화 저장\n",
    "            stm.save_context({\"input\": user_input}, {\"output\": full_answer})\n",
    "\n",
    "    except WebSocketDisconnect:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 실행:\n",
    "# uvicorn app:app --host 0.0.0.0 --port 8000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretary_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
