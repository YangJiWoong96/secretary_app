{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845f3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\1903131679.py:19: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  llm = OpenAI(\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: 환경 변수 및 라이브러리 불러오기 (이전 코드와 동일)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")  # .env에서 OPENAI_API_KEY 로드\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "# LangChain의 메모리 모듈에서 ConversationBufferMemory 임포트\n",
    "from langchain.memory import ConversationBufferMemory  # <--- 이 부분을 추가합니다.\n",
    "\n",
    "# Redis URL 설정 (기본값은 localhost:6379, 필요시 .env 파일에 REDIS_URL 정의)\n",
    "redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = OpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, temperature=0.7\n",
    ")  # temperature는 창의성 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72ed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\2150304891.py:22: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Redis를 활용한 멀티챗 시뮬레이션 ---\n",
      "세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\n",
      "종료: 'exit' 또는 'quit'\n",
      "\n",
      "현재 활성 세션: default_session\n",
      "AI 응답:  안녕하세요, 웅이님! 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  그렇군요, 사용자님. 제가 조금 더 자세한 정보를 알면 더 도움이 될 수 있을 것 같아요. 직장에서 무엇이 어려우신가요? 어떤 분위기인가요? 제가 도와드릴 수 있는 건 없을까요?\n",
      "AI 응답:  이해합니다, 웅이님. 인간관계는 정말 어려운 일이죠. 늘 그렇지는 않지만, 종종 충돌이 있을 수 있죠. 직장에서도 그런 문제가 있나요? 어떤 일을 하고 계신가요?\n",
      "AI 응답:  그렇군요, 웅이님. IT 개발은 정말 어려운 일이죠. 많은 도전과 압박이 있을 수 있습니다. 하지만 그만큼 보람도 있을 것 같아요. 어떤 기술을 다루시나요? 저도 기술적인 부분에서 도움이 필요하다면 제가 최선을 다해 도와드릴 수 있을 거예요.\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_session_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] 당신의 질문: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m대화를 종료합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# --- 멀티챗 구현 함수 ---\n",
    "\n",
    "\n",
    "def get_conversation_chain(session_id: str):\n",
    "    \"\"\"\n",
    "    주어진 session_id에 따라 Redis에 연결된 ConversationChain을 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. RedisChatMessageHistory 인스턴스 생성\n",
    "    redis_chat_history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=redis_url,\n",
    "    )\n",
    "\n",
    "    # 2. ConversationBufferMemory에 RedisChatMessageHistory를 chat_memory로 전달\n",
    "    #    return_messages=True로 설정하면 메시지 객체 리스트를 반환합니다.\n",
    "    memory = ConversationBufferMemory(\n",
    "        chat_memory=redis_chat_history,\n",
    "        return_messages=True,  # LLM에 메시지 객체 형태로 전달 (더 유연함)\n",
    "    )\n",
    "\n",
    "    # 3. ConversationChain에 수정된 memory 객체 전달\n",
    "    conversation = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=memory,  # <--- 수정된 부분입니다.\n",
    "        verbose=False,  # 대화 과정을 너무 자세히 보여주지 않기 위해 False로 설정\n",
    "        # 필요하다면 True로 변경하여 디버깅에 활용\n",
    "    )\n",
    "    return conversation\n",
    "\n",
    "\n",
    "# --- 메인 대화 루프 (이하 동일) ---\n",
    "\n",
    "print(\"--- Redis를 활용한 멀티챗 시뮬레이션 ---\")\n",
    "print(\"세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"  # 초기 세션 ID\n",
    "current_conversation = get_conversation_chain(current_session_id)\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # 'change <session_id>' 명령 처리\n",
    "            parts = user_input.split(\" \")\n",
    "            if len(parts) == 2:\n",
    "                new_session_id = parts[1]\n",
    "                if new_session_id == current_session_id:\n",
    "                    print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "                else:\n",
    "                    current_session_id = new_session_id\n",
    "                    current_conversation = get_conversation_chain(current_session_id)\n",
    "                    print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(\"잘못된 'change' 명령입니다. 사용법: change <session_id>\")\n",
    "            continue  # 다음 루프\n",
    "\n",
    "        # LLM에게 질문하고 응답 받기\n",
    "        ai_response = current_conversation.predict(input=user_input)\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        print(\"API 키를 확인하거나, Redis 서버가 실행 중인지 확인해주세요.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- 모든 세션의 최종 대화 기록 (Redis에서 직접 확인) ---\")\n",
    "# 실험에 사용된 모든 세션 ID를 여기에 추가하여 실제 Redis에 저장된 내용을 확인할 수 있습니다.\n",
    "# 위 예시에서 사용한 'default_session', 'user_a', 'session_1' 등을 넣어보세요.\n",
    "test_session_ids = [\"default_session\", \"user_a\", \"session_1\", \"user_b_test\"]\n",
    "for sid in test_session_ids:\n",
    "    try:\n",
    "        history_check = RedisChatMessageHistory(session_id=sid, url=redis_url)\n",
    "        if history_check.messages:  # 메시지가 있는 세션만 표시\n",
    "            print(f\"\\n--- 세션 '{sid}' 기록 ---\")\n",
    "            for msg in history_check.messages:\n",
    "                print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"세션 '{sid}' 기록을 가져오는 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9873c4d",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 1: 패키지 설치 (한 번만 실행)\n",
    "# !pip install langchain-community redis python-dotenv ipykernel\n",
    "\n",
    "# 셀 2: 환경 변수 로드 & 임포트\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 셀 3: LLM 및 Redis 메모리 초기화\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "memory = RedisChatMessageHistory(\n",
    "    session_id=\"default_session\", url=\"redis://localhost:6379/0\"\n",
    ")\n",
    "\n",
    "\n",
    "# 셀 4: 대화 함수 정의 (수정된 부분)\n",
    "def chat(input_text: str) -> str:\n",
    "    # 1) Redis에서 이전 메시지 리스트 불러오기\n",
    "    history = memory.messages  # ❌ get_messages()가 아니라 messages 속성 사용\n",
    "    # 2) HumanMessage 추가하여 LLM 호출\n",
    "    messages = history + [HumanMessage(content=input_text)]\n",
    "    response = llm(messages)\n",
    "    # 3) Redis에 대화 저장\n",
    "    memory.add_user_message(input_text)\n",
    "    memory.add_ai_message(response.content)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0281c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_10992\\68044283.py:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "# 셀 5: 멀티턴 대화 테스트\n",
    "print(chat(\"안녕.난 웅이라고 해.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd871b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송해요, 제가 이름을 잘못 말했네요. 사용자님의 이름은 웅이군요. 다시 한번 반가워요, 웅이님! 혼란을 드려 죄송합니다. 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"내 이름이 뭐라고?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960c890",
   "metadata": {},
   "source": [
    "### Gemini의 Redis + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a41625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# --- 환경 변수 로드 ---\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# --- 상수 정의 ---\n",
    "EMBEDDING_DIM = 1536  # OpenAI 'text-embedding-ada-002' 기준\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# --- LLM 및 임베딩 모델 초기화 ---\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# --- 프로필 DB 시뮬레이션 (실제로는 MongoDB 등 별도 DB 사용 추천) ---\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    \"\"\"Milvus 서버에 연결하고 연결 상태를 반환합니다.\"\"\"\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(collection_name, description):\n",
    "    \"\"\"지정된 스키마로 Milvus 컬렉션을 생성합니다.\"\"\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        return Collection(collection_name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\n",
    "            name=\"user_id\",\n",
    "            dtype=DataType.VARCHAR,\n",
    "            max_length=256,\n",
    "            is_partition_key=False,\n",
    "        ),\n",
    "        FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"created_at\", dtype=DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description, enable_dynamic_field=False)\n",
    "    collection = Collection(collection_name, schema)\n",
    "\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "    collection.create_index(\"embedding\", index_params)\n",
    "    print(f\"Milvus collection '{collection_name}' created successfully.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억 관리 (RAG & 프로필) 핵심 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    \"\"\"대화 세션 종료 후 장기 기억(프로필, 로그)을 업데이트합니다.\"\"\"\n",
    "    print(f\"\\n[{session_id}] 장기 기억 업데이트 시작...\")\n",
    "\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"업데이트할 대화 내용이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 최근 10개 메시지를 요약 대상으로 함 (조절 가능)\n",
    "    recent_messages = history.messages[-10:]\n",
    "    conversation_text = \"\\n\".join(\n",
    "        [f\"{msg.type}: {msg.content}\" for msg in recent_messages]\n",
    "    )\n",
    "\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화 내용을 바탕으로, 대화의 핵심 흐름과 뉘앙스를 상세히 요약해줘.\\n\\n{conversation}\"\n",
    "    )\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "    flow_summary = summary_chain.run(conversation=conversation_text)\n",
    "    print(f\"  - 흐름 요약 완료: {flow_summary[:50]}...\")\n",
    "\n",
    "    old_profile_str = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "\n",
    "    update_prompt_str = \"\"\"You are a profile manager AI. Based on the [Existing Profile] and [Latest Conversation Summary] below, update the [Existing Profile] with the latest information.\n",
    "1. Add new key information.\n",
    "2. If new information contradicts existing information, boldly modify or delete the old information.\n",
    "3. Ignore trivial content like simple greetings.\n",
    "4. Keep the profile concise and maintain the overall core.\n",
    "5. You MUST return the final result in JSON format only.\n",
    "\n",
    "[Existing Profile]:\n",
    "{old_profile}\n",
    "\n",
    "[Latest Conversation Summary]:\n",
    "{summary}\n",
    "\"\"\"\n",
    "    profile_update_prompt = PromptTemplate.from_template(update_prompt_str)\n",
    "    profile_update_chain = LLMChain(llm=llm, prompt=profile_update_prompt)\n",
    "\n",
    "    try:\n",
    "        new_profile_str = profile_update_chain.run(\n",
    "            old_profile=old_profile_str, summary=flow_summary\n",
    "        )\n",
    "        new_profile = json.loads(new_profile_str)\n",
    "        PROFILE_DB[session_id] = new_profile\n",
    "        print(\"  - 프로필 업데이트 완료.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\n",
    "            f\"  - 프로필 업데이트 실패: LLM이 유효한 JSON을 반환하지 않았습니다. 응답: {new_profile_str}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    get_milvus_connection()\n",
    "    profile_collection = create_milvus_collection(\n",
    "        PROFILE_COLLECTION_NAME, \"User Profiles\"\n",
    "    )\n",
    "    log_collection = create_milvus_collection(\n",
    "        LOG_COLLECTION_NAME, \"Conversation Log Summaries\"\n",
    "    )\n",
    "\n",
    "    # Track 1 (프로필 덮어쓰기)\n",
    "    profile_text = json.dumps(new_profile, ensure_ascii=False)\n",
    "    profile_embedding = embeddings.embed_query(profile_text)\n",
    "    profile_data = [\n",
    "        {\n",
    "            \"id\": session_id,\n",
    "            \"embedding\": profile_embedding,\n",
    "            \"text\": profile_text,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"profile\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    profile_collection.upsert(profile_data)\n",
    "    print(\"  - RAG: 프로필 벡터 덮어쓰기 완료.\")\n",
    "\n",
    "    # Track 2 (아카이브 추가)\n",
    "    log_embedding = embeddings.embed_query(flow_summary)\n",
    "    log_id = str(uuid.uuid4())\n",
    "    log_data = [\n",
    "        {\n",
    "            \"id\": log_id,\n",
    "            \"embedding\": log_embedding,\n",
    "            \"text\": flow_summary,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"log_summary\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    log_collection.insert(log_data)\n",
    "    print(\"  - RAG: 대화 흐름 아카이브 추가 완료.\")\n",
    "    print(f\"[{session_id}] 장기 기억 업데이트 종료.\")\n",
    "\n",
    "\n",
    "def retrieve_from_rag(session_id, query):\n",
    "    \"\"\"주어진 쿼리로 RAG DB에서 관련 정보를 검색합니다.\"\"\"\n",
    "    get_milvus_connection()\n",
    "    if not utility.has_collection(\n",
    "        PROFILE_COLLECTION_NAME\n",
    "    ) or not utility.has_collection(LOG_COLLECTION_NAME):\n",
    "        return \"아직 RAG DB에 저장된 정보가 없습니다.\"\n",
    "\n",
    "    profile_collection = Collection(PROFILE_COLLECTION_NAME)\n",
    "    log_collection = Collection(LOG_COLLECTION_NAME)\n",
    "    profile_collection.load()\n",
    "    log_collection.load()\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    profile_results = profile_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=1,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "    log_results = log_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=2,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "\n",
    "    rag_context = \"[프로필 정보]\\n\"\n",
    "    rag_context += (\n",
    "        profile_results[0][0].entity.get(\"text\")\n",
    "        if profile_results and profile_results[0]\n",
    "        else \"저장된 프로필 정보 없음\"\n",
    "    )\n",
    "    rag_context += \"\\n\\n[과거 대화 기록 요약]\\n\"\n",
    "    if log_results and log_results[0]:\n",
    "        for hit in log_results[0]:\n",
    "            rag_context += f\"- {hit.entity.get('text')}\\n\"\n",
    "    else:\n",
    "        rag_context += \"저장된 과거 대화 없음\"\n",
    "\n",
    "    return rag_context\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: 단기 기억 및 대화 체인 구성\n",
    "# ----------------------------------------------------------------------\n",
    "def get_short_term_memory(session_id: str):\n",
    "    \"\"\"단기 기억(Redis)을 위한 메모리 객체를 반환합니다.\"\"\"\n",
    "    redis_chat_history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_chat_history,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"input\",\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful and friendly AI assistant.\n",
    "Please answer the user's question based on the [Long-Term Memory] and [Recent Conversation History] provided below.\n",
    "\n",
    "[Long-Term Memory - Everything you know about this user]:\n",
    "{rag_context}\n",
    "\n",
    "[Recent Conversation History]:\n",
    "{chat_history}\n",
    "\n",
    "User's Question: {input}\n",
    "AI Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"input\"], template=prompt_template\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n--- 최종 아키텍처 시뮬레이션 (Milvus v2.4.0 호환) ---\")\n",
    "print(\"세션 변경: 'change <session_id>'\")\n",
    "print(\"장기 기억 저장: 'save'\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower() == \"save\":\n",
    "            update_long_term_memory(current_session_id)\n",
    "            continue\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # ... (세션 변경 로직은 이전과 동일) ...\n",
    "            parts = user_input.split(\" \", 1)\n",
    "            new_session_id = parts[1]\n",
    "            if new_session_id != current_session_id:\n",
    "                current_session_id = new_session_id\n",
    "                print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "            continue\n",
    "\n",
    "        # 1. RAG에서 장기 기억 검색\n",
    "        rag_context = retrieve_from_rag(current_session_id, user_input)\n",
    "\n",
    "        # 2. Redis에서 단기 기억 로드\n",
    "        short_term_memory = get_short_term_memory(current_session_id)\n",
    "        # memory.load_memory_variables()는 비동기 호출 등이 필요할 수 있어, 여기서는 수동으로 context를 구성합니다.\n",
    "        # 체인에 직접 전달하기 위해 대화 기록을 문자열로 포맷팅합니다.\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"{msg.type}: {msg.content}\"\n",
    "                for msg in short_term_memory.chat_memory.messages\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. LLMChain으로 최종 프롬프트 실행\n",
    "        conversation_chain = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "        ai_response = conversation_chain.predict(\n",
    "            rag_context=rag_context, chat_history=chat_history_str, input=user_input\n",
    "        )\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "        # 4. 대화 기록을 단기 기억(Redis)에 수동으로 저장\n",
    "        short_term_memory.save_context({\"input\": user_input}, {\"output\": ai_response})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e095c",
   "metadata": {},
   "source": [
    "### GPT의 Redis + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# 상수\n",
    "EMBEDDING_DIM = 1536  # text-embedding-ada-002\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# 모델 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# 간이 프로필 DB\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억 관리 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    print(f\"\\n[{session_id}] 장기 기억 업데이트 시작...\")\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"  - 저장된 대화 없음.\")\n",
    "        return\n",
    "\n",
    "    # 최근 10개 요약\n",
    "    recent = history.messages[-10:]\n",
    "    conv = \"\\n\".join(f\"{m.type}: {m.content}\" for m in recent)\n",
    "    summary_tpl = PromptTemplate.from_template(\"다음 대화를 요약해줘:\\n{conversation}\")\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_tpl)\n",
    "    flow_summary = summary_chain.run(conversation=conv)\n",
    "    print(\"  - 흐름 요약 완료.\")\n",
    "\n",
    "    # 프로필 업데이트\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    update_prompt = PromptTemplate.from_template(\n",
    "        \"[Existing Profile]:\\n{old}\\n[Summary]:\\n{sum}\\n\"\n",
    "        + \"업데이트된 프로필 JSON만 출력해줘.\"\n",
    "    )\n",
    "    update_chain = LLMChain(llm=llm, prompt=update_prompt)\n",
    "    new_prof_str = update_chain.run(old=old_prof, sum=flow_summary)\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "        print(\"  - 프로필 업데이트 완료.\")\n",
    "    except Exception:\n",
    "        print(f\"  - JSON 파싱 에러: {new_prof_str}\")\n",
    "        return\n",
    "\n",
    "    # Milvus에 저장\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "\n",
    "    # Track1: 프로필 업서트\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"  - 프로필 벡터 업서트 완료.\")\n",
    "\n",
    "    # Track2: 로그 추가\n",
    "    log_emb = embeddings.embed_query(flow_summary)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": flow_summary,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(f\"[{session_id}] 장기 기억 업데이트 완료.\")\n",
    "\n",
    "\n",
    "def retrieve_from_rag(session_id: str, query: str) -> str:\n",
    "    get_milvus_connection()\n",
    "    if not (\n",
    "        utility.has_collection(PROFILE_COLLECTION_NAME)\n",
    "        and utility.has_collection(LOG_COLLECTION_NAME)\n",
    "    ):\n",
    "        return \"저장된 RAG 정보 없음.\"\n",
    "\n",
    "    prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "    log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "    prof_coll.load()\n",
    "    log_coll.load()\n",
    "\n",
    "    q_emb = embeddings.embed_query(query)\n",
    "    params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "\n",
    "    prof_res = prof_coll.search(\n",
    "        [q_emb], \"embedding\", params, limit=1, expr=f\"user_id=='{session_id}'\"\n",
    "    )\n",
    "    log_res = log_coll.search(\n",
    "        [q_emb], \"embedding\", params, limit=2, expr=f\"user_id=='{session_id}'\"\n",
    "    )\n",
    "\n",
    "    context = \"[프로필]\\n\"\n",
    "    context += prof_res[0][0].entity.get(\"text\") if prof_res and prof_res[0] else \"없음\"\n",
    "    context += \"\\n[과거 대화 요약]\\n\"\n",
    "    if log_res and log_res[0]:\n",
    "        for hit in log_res[0]:\n",
    "            context += f\"- {hit.entity.get('text')}\\n\"\n",
    "    else:\n",
    "        context += \"없음\"\n",
    "    return context\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: 단기 기억 및 체인 설정\n",
    "# ----------------------------------------------------------------------\n",
    "def get_short_term_memory(session_id: str):\n",
    "    hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"input\"],\n",
    "    template=(\n",
    "        \"You are an AI assistant.\\n\"\n",
    "        \"[Long-Term Memory]\\n{rag_context}\\n\"\n",
    "        \"[Recent]\\n{chat_history}\\n\"\n",
    "        \"Q: {input}\\nA:\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- 챗봇 시작: 'change <id>', 'save', 'exit' 사용 ---\")\n",
    "    session_id = \"default_session\"\n",
    "    while True:\n",
    "        cmd = input(f\"[{session_id}]> \")\n",
    "        if cmd.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        if cmd.lower() == \"save\":\n",
    "            update_long_term_memory(session_id)\n",
    "            continue\n",
    "        if cmd.lower().startswith(\"change \"):\n",
    "            new = cmd.split(\" \", 1)[1]\n",
    "            if new != session_id:\n",
    "                session_id = new\n",
    "                print(f\"세션 변경: {session_id}\")\n",
    "            else:\n",
    "                print(\"이미 같은 세션입니다.\")\n",
    "            continue\n",
    "\n",
    "        # 일반 질문 처리\n",
    "        rag_ctx = retrieve_from_rag(session_id, cmd)\n",
    "        stm = get_short_term_memory(session_id)\n",
    "        hist_str = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "        chain = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "        ans = chain.predict(rag_context=rag_ctx, chat_history=hist_str, input=cmd)\n",
    "        print(f\"Bot: {ans}\")\n",
    "        stm.save_context({\"input\": cmd}, {\"output\": ans})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretary_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
