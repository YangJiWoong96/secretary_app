{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\1903131679.py:19: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  llm = OpenAI(\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: 환경 변수 및 라이브러리 불러오기\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "# LangChain의 메모리 모듈에서 ConversationBufferMemory 임포트 - redis 활용 측면\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Redis URL 설정 (기본값은 localhost:6379, 필요시 .env 파일에 REDIS_URL 따로 재정의)\n",
    "redis_url = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72ed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_5088\\2150304891.py:22: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Redis를 활용한 멀티챗 시뮬레이션 ---\n",
      "세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\n",
      "종료: 'exit' 또는 'quit'\n",
      "\n",
      "현재 활성 세션: default_session\n",
      "AI 응답:  안녕하세요, 웅이님! 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  그렇군요, 사용자님. 제가 조금 더 자세한 정보를 알면 더 도움이 될 수 있을 것 같아요. 직장에서 무엇이 어려우신가요? 어떤 분위기인가요? 제가 도와드릴 수 있는 건 없을까요?\n",
      "AI 응답:  이해합니다, 웅이님. 인간관계는 정말 어려운 일이죠. 늘 그렇지는 않지만, 종종 충돌이 있을 수 있죠. 직장에서도 그런 문제가 있나요? 어떤 일을 하고 계신가요?\n",
      "AI 응답:  그렇군요, 웅이님. IT 개발은 정말 어려운 일이죠. 많은 도전과 압박이 있을 수 있습니다. 하지만 그만큼 보람도 있을 것 같아요. 어떤 기술을 다루시나요? 저도 기술적인 부분에서 도움이 필요하다면 제가 최선을 다해 도와드릴 수 있을 거예요.\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n",
      "AI 응답:  안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent_session_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m] 당신의 질문: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m대화를 종료합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\computer\\anaconda3\\envs\\secretary_app\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 멀티챗 구현\n",
    "\n",
    "\n",
    "def get_conversation_chain(session_id: str):\n",
    "    \"\"\"\n",
    "    주어진 session_id에 따라 Redis에 연결된 ConversationChain을 반환\n",
    "    \"\"\"\n",
    "    # 1. RedisChatMessageHistory\n",
    "    redis_chat_history = RedisChatMessageHistory(\n",
    "        session_id=session_id,\n",
    "        url=redis_url,\n",
    "    )\n",
    "\n",
    "    # 2. ConversationBufferMemory에 RedisChatMessageHistory를 chat_memory로 전달\n",
    "    #    return_messages=True로 설정하면 메시지 객체 리스트를 반환\n",
    "    memory = ConversationBufferMemory(\n",
    "        chat_memory=redis_chat_history,\n",
    "        return_messages=True,  # LLM에 메시지 객체 형태로 전달 - 더 유연한 방식\n",
    "    )\n",
    "\n",
    "    # 3. ConversationChain에 수정된 memory 객체 전달\n",
    "    conversation = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False,  # 대화 과정 X 추후 True로 변경하여 디버깅에 활용\n",
    "    )\n",
    "    return conversation\n",
    "\n",
    "\n",
    "# --- 메인 대화 루프 ---\n",
    "\n",
    "print(\"--- Redis를 활용한 단기기억 멀티챗 test ---\")\n",
    "print(\"세션 변경: 'change <session_id>' (예: 'change user_a' 또는 'change session_1')\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"  # 초기 세션 ID\n",
    "current_conversation = get_conversation_chain(current_session_id)\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # 'change <session_id>' 명령 처리\n",
    "            parts = user_input.split(\" \")\n",
    "            if len(parts) == 2:\n",
    "                new_session_id = parts[1]\n",
    "                if new_session_id == current_session_id:\n",
    "                    print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "                else:\n",
    "                    current_session_id = new_session_id\n",
    "                    current_conversation = get_conversation_chain(current_session_id)\n",
    "                    print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(\"잘못된 'change' 명령입니다. 사용법: change <session_id>\")\n",
    "            continue  # 다음 루프\n",
    "\n",
    "        # LLM에게 질문하고 응답 받기\n",
    "        ai_response = current_conversation.predict(input=user_input)\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        print(\"API 키를 확인하거나, Redis 서버가 실행 중인지 확인해주세요.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- 모든 세션의 최종 대화 기록 (Redis에서 직접 확인) ---\")\n",
    "# 실험에 사용된 모든 세션 ID를 추가하여 실제 Redis에 저장된 내용을 확인 가능\n",
    "# 예시로 'default_session', 'user_a', 'session_1' 등 활용\n",
    "test_session_ids = [\"default_session\", \"user_a\", \"session_1\", \"user_b_test\"]\n",
    "for sid in test_session_ids:\n",
    "    try:\n",
    "        history_check = RedisChatMessageHistory(session_id=sid, url=redis_url)\n",
    "        if history_check.messages:  # 메시지가 있는 세션만 표시\n",
    "            print(f\"\\n--- 세션 '{sid}' 기록 ---\")\n",
    "            for msg in history_check.messages:\n",
    "                print(f\"  {msg.type.upper()}: {msg.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"세션 '{sid}' 기록을 가져오는 중 오류: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9873c4d",
   "metadata": {},
   "source": [
    "### 또다른 버전 g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 셀 3: LLM 및 Redis 메모리 초기화\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "memory = RedisChatMessageHistory(\n",
    "    session_id=\"default_session\", url=\"redis://localhost:6379/0\"\n",
    ")\n",
    "\n",
    "\n",
    "# 셀 4: 대화 함수 정의 (수정된 부분)\n",
    "def chat(input_text: str) -> str:\n",
    "    # 1) Redis에서 이전 메시지 리스트 불러오기\n",
    "    history = memory.messages  # ❌ get_messages()가 아니라 messages 속성 사용\n",
    "    # 2) HumanMessage 추가하여 LLM 호출\n",
    "    messages = history + [HumanMessage(content=input_text)]\n",
    "    response = llm(messages)\n",
    "    # 3) Redis에 대화 저장\n",
    "    memory.add_user_message(input_text)\n",
    "    memory.add_ai_message(response.content)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0281c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer\\AppData\\Local\\Temp\\ipykernel_10992\\68044283.py:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 웅이님. 만나서 반가워요! 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "# 셀 5: 멀티턴 대화 테스트\n",
    "print(chat(\"안녕.난 웅이라고 해.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd871b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송해요, 제가 이름을 잘못 말했네요. 사용자님의 이름은 웅이군요. 다시 한번 반가워요, 웅이님! 혼란을 드려 죄송합니다. 어떻게 도와드릴까요? :)\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"내 이름이 뭐라고?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960c890",
   "metadata": {},
   "source": [
    "### 기본 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a41625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 셀 1: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ChatMessageHistory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 2: 기본 설정 및 상수 정의\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# --- 환경 변수 로드 ---\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "# --- 상수 정의 ---\n",
    "EMBEDDING_DIM = 1536  # OpenAI 'text-embedding-ada-002' 기준\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "# --- LLM 및 임베딩 모델 초기화 ---\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# --- 프로필 DB 시뮬레이션 (실제로는 MongoDB 등 별도 DB 사용 추천) ---\n",
    "PROFILE_DB = {}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    \"\"\"Milvus 서버에 연결하고 연결 상태를 반환합니다.\"\"\"\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(collection_name, description):\n",
    "    \"\"\"지정된 스키마로 Milvus 컬렉션을 생성합니다.\"\"\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        return Collection(collection_name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\n",
    "            name=\"user_id\",\n",
    "            dtype=DataType.VARCHAR,\n",
    "            max_length=256,\n",
    "            is_partition_key=False,\n",
    "        ),\n",
    "        FieldSchema(name=\"type\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"created_at\", dtype=DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, description, enable_dynamic_field=False)\n",
    "    collection = Collection(collection_name, schema)\n",
    "\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "    collection.create_index(\"embedding\", index_params)\n",
    "    print(f\"Milvus collection '{collection_name}' created successfully.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억 관리 (RAG & 프로필) 핵심 함수\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    \"\"\"대화 세션 종료 후 장기 기억(프로필, 로그)을 업데이트합니다.\"\"\"\n",
    "    print(f\"\\n[{session_id}] 장기 기억 업데이트 시작...\")\n",
    "\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        print(\"업데이트할 대화 내용이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 최근 10개 메시지를 요약 대상으로 함 (조절 가능)\n",
    "    recent_messages = history.messages[-10:]\n",
    "    conversation_text = \"\\n\".join(\n",
    "        [f\"{msg.type}: {msg.content}\" for msg in recent_messages]\n",
    "    )\n",
    "\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화 내용을 바탕으로, 대화의 핵심 흐름과 뉘앙스를 상세히 요약해줘.\\n\\n{conversation}\"\n",
    "    )\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "    flow_summary = summary_chain.run(conversation=conversation_text)\n",
    "    print(f\"  - 흐름 요약 완료: {flow_summary[:50]}...\")\n",
    "\n",
    "    old_profile_str = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "\n",
    "    update_prompt_str = \"\"\"You are a profile manager AI. Based on the [Existing Profile] and [Latest Conversation Summary] below, update the [Existing Profile] with the latest information.\n",
    "1. Add new key information.\n",
    "2. If new information contradicts existing information, boldly modify or delete the old information.\n",
    "3. Ignore trivial content like simple greetings.\n",
    "4. Keep the profile concise and maintain the overall core.\n",
    "5. You MUST return the final result in JSON format only.\n",
    "\n",
    "[Existing Profile]:\n",
    "{old_profile}\n",
    "\n",
    "[Latest Conversation Summary]:\n",
    "{summary}\n",
    "\"\"\"\n",
    "    profile_update_prompt = PromptTemplate.from_template(update_prompt_str)\n",
    "    profile_update_chain = LLMChain(llm=llm, prompt=profile_update_prompt)\n",
    "\n",
    "    try:\n",
    "        new_profile_str = profile_update_chain.run(\n",
    "            old_profile=old_profile_str, summary=flow_summary\n",
    "        )\n",
    "        new_profile = json.loads(new_profile_str)\n",
    "        PROFILE_DB[session_id] = new_profile\n",
    "        print(\"  - 프로필 업데이트 완료.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\n",
    "            f\"  - 프로필 업데이트 실패: LLM이 유효한 JSON을 반환하지 않았습니다. 응답: {new_profile_str}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    get_milvus_connection()\n",
    "    profile_collection = create_milvus_collection(\n",
    "        PROFILE_COLLECTION_NAME, \"User Profiles\"\n",
    "    )\n",
    "    log_collection = create_milvus_collection(\n",
    "        LOG_COLLECTION_NAME, \"Conversation Log Summaries\"\n",
    "    )\n",
    "\n",
    "    # Track 1 (프로필 덮어쓰기)\n",
    "    profile_text = json.dumps(new_profile, ensure_ascii=False)\n",
    "    profile_embedding = embeddings.embed_query(profile_text)\n",
    "    profile_data = [\n",
    "        {\n",
    "            \"id\": session_id,\n",
    "            \"embedding\": profile_embedding,\n",
    "            \"text\": profile_text,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"profile\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    profile_collection.upsert(profile_data)\n",
    "    print(\"  - RAG: 프로필 벡터 덮어쓰기 완료.\")\n",
    "\n",
    "    # Track 2 (아카이브 추가)\n",
    "    log_embedding = embeddings.embed_query(flow_summary)\n",
    "    log_id = str(uuid.uuid4())\n",
    "    log_data = [\n",
    "        {\n",
    "            \"id\": log_id,\n",
    "            \"embedding\": log_embedding,\n",
    "            \"text\": flow_summary,\n",
    "            \"user_id\": session_id,\n",
    "            \"type\": \"log_summary\",\n",
    "            \"created_at\": int(os.times().user),\n",
    "        }\n",
    "    ]\n",
    "    log_collection.insert(log_data)\n",
    "    print(\"  - RAG: 대화 흐름 아카이브 추가 완료.\")\n",
    "    print(f\"[{session_id}] 장기 기억 업데이트 종료.\")\n",
    "\n",
    "\n",
    "def retrieve_from_rag(session_id, query):\n",
    "    \"\"\"주어진 쿼리로 RAG DB에서 관련 정보를 검색합니다.\"\"\"\n",
    "    get_milvus_connection()\n",
    "    if not utility.has_collection(\n",
    "        PROFILE_COLLECTION_NAME\n",
    "    ) or not utility.has_collection(LOG_COLLECTION_NAME):\n",
    "        return \"아직 RAG DB에 저장된 정보가 없습니다.\"\n",
    "\n",
    "    profile_collection = Collection(PROFILE_COLLECTION_NAME)\n",
    "    log_collection = Collection(LOG_COLLECTION_NAME)\n",
    "    profile_collection.load()\n",
    "    log_collection.load()\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    profile_results = profile_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=1,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "    log_results = log_collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=2,\n",
    "        expr=f\"user_id == '{session_id}'\",\n",
    "    )\n",
    "\n",
    "    rag_context = \"[프로필 정보]\\n\"\n",
    "    rag_context += (\n",
    "        profile_results[0][0].entity.get(\"text\")\n",
    "        if profile_results and profile_results[0]\n",
    "        else \"저장된 프로필 정보 없음\"\n",
    "    )\n",
    "    rag_context += \"\\n\\n[과거 대화 기록 요약]\\n\"\n",
    "    if log_results and log_results[0]:\n",
    "        for hit in log_results[0]:\n",
    "            rag_context += f\"- {hit.entity.get('text')}\\n\"\n",
    "    else:\n",
    "        rag_context += \"저장된 과거 대화 없음\"\n",
    "\n",
    "    return rag_context\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: 단기 기억 및 대화 체인 구성\n",
    "# ----------------------------------------------------------------------\n",
    "def get_short_term_memory(session_id: str):\n",
    "    \"\"\"단기 기억(Redis)을 위한 메모리 객체를 반환합니다.\"\"\"\n",
    "    redis_chat_history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_chat_history,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"input\",\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful and friendly AI assistant.\n",
    "Please answer the user's question based on the [Long-Term Memory] and [Recent Conversation History] provided below.\n",
    "\n",
    "[Long-Term Memory - Everything you know about this user]:\n",
    "{rag_context}\n",
    "\n",
    "[Recent Conversation History]:\n",
    "{chat_history}\n",
    "\n",
    "User's Question: {input}\n",
    "AI Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_context\", \"chat_history\", \"input\"], template=prompt_template\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 6: 메인 대화 루프\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n--- 최종 아키텍처 시뮬레이션 (Milvus v2.4.0 호환) ---\")\n",
    "print(\"세션 변경: 'change <session_id>'\")\n",
    "print(\"장기 기억 저장: 'save'\")\n",
    "print(\"종료: 'exit' 또는 'quit'\")\n",
    "\n",
    "current_session_id = \"default_session\"\n",
    "print(f\"\\n현재 활성 세션: {current_session_id}\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(f\"[{current_session_id}] 당신의 질문: \")\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"대화를 종료합니다.\")\n",
    "            break\n",
    "        elif user_input.lower() == \"save\":\n",
    "            update_long_term_memory(current_session_id)\n",
    "            continue\n",
    "        elif user_input.lower().startswith(\"change \"):\n",
    "            # ... (세션 변경 로직은 이전과 동일) ...\n",
    "            parts = user_input.split(\" \", 1)\n",
    "            new_session_id = parts[1]\n",
    "            if new_session_id != current_session_id:\n",
    "                current_session_id = new_session_id\n",
    "                print(f\"\\n세션이 '{current_session_id}'(으)로 변경되었습니다.\")\n",
    "            else:\n",
    "                print(f\"이미 '{new_session_id}' 세션에 있습니다.\")\n",
    "            continue\n",
    "\n",
    "        # 1. RAG에서 장기 기억 검색\n",
    "        rag_context = retrieve_from_rag(current_session_id, user_input)\n",
    "\n",
    "        # 2. Redis에서 단기 기억 로드\n",
    "        short_term_memory = get_short_term_memory(current_session_id)\n",
    "        # memory.load_memory_variables()는 비동기 호출 등이 필요할 수 있어, 여기서는 수동으로 context를 구성합니다.\n",
    "        # 체인에 직접 전달하기 위해 대화 기록을 문자열로 포맷팅합니다.\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [\n",
    "                f\"{msg.type}: {msg.content}\"\n",
    "                for msg in short_term_memory.chat_memory.messages\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. LLMChain으로 최종 프롬프트 실행\n",
    "        conversation_chain = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "        ai_response = conversation_chain.predict(\n",
    "            rag_context=rag_context, chat_history=chat_history_str, input=user_input\n",
    "        )\n",
    "        print(f\"AI 응답: {ai_response}\")\n",
    "\n",
    "        # 4. 대화 기록을 단기 기억(Redis)에 수동으로 저장\n",
    "        short_term_memory.save_context({\"input\": user_input}, {\"output\": ai_response})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e095c",
   "metadata": {},
   "source": [
    "### Redis + RAG + + 라우팅 + 임베딩 분류 및 스트리밍, 비동기 방식 웹 소캣 test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import asyncio\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_message_histories.redis import RedisChatMessageHistory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 1~3: 환경 변수 및 라이브러리 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379/0\")\n",
    "\n",
    "EMBEDDING_DIM = 384 if EMBEDDING_MODEL.endswith(\"3-small\") else 1536\n",
    "PROFILE_COLLECTION_NAME = \"user_profiles_v2\"\n",
    "LOG_COLLECTION_NAME = \"conversation_logs_v2\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, model=LLM_MODEL, temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=EMBEDDING_MODEL)\n",
    "\n",
    "PROFILE_DB = {}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Embedding-based Intent Router 설정\n",
    "# ----------------------------------------------------------------------\n",
    "INTENT_EXAMPLES = {\n",
    "    \"rag\": [\n",
    "        \"내가 설정한 목표 다시 알려줘.\",\n",
    "        \"우리 지난주에 무슨 얘기까지 했지?\",\n",
    "        \"내 프로젝트 이름 기억나?\",\n",
    "    ],\n",
    "    \"web\": [\n",
    "        \"오늘 서울 날씨 어때?\",\n",
    "        \"가장 가까운 스타벅스 어디야?\",\n",
    "        \"엔비디아의 최신 GPU 모델 이름이 뭐야?\",\n",
    "    ],\n",
    "    \"conv\": [\n",
    "        \"고마워!\",\n",
    "        \"ㅋㅋㅋㅋㅋ\",\n",
    "        \"재밌는 농담 하나 해줘.\",\n",
    "        \"대한민국의 수도는 어디야?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# 각 intent별 대표 embedding을 평균으로 계산\n",
    "INTENT_EMBEDDINGS = {}\n",
    "for label, texts in INTENT_EXAMPLES.items():\n",
    "    vecs = [embeddings.embed_query(t) for t in texts]\n",
    "    avg = [sum(x) / len(x) for x in zip(*vecs)]\n",
    "    INTENT_EMBEDDINGS[label] = np.array(avg)\n",
    "\n",
    "\n",
    "def embedding_router(query: str, threshold: float = 0.8) -> str | None:\n",
    "    q_emb = np.array(embeddings.embed_query(query))\n",
    "    sims = {}\n",
    "    for label, emb in INTENT_EMBEDDINGS.items():\n",
    "        sims[label] = float(\n",
    "            np.dot(q_emb, emb) / (np.linalg.norm(q_emb) * np.linalg.norm(emb))\n",
    "        )\n",
    "    best = max(sims, key=sims.get)\n",
    "    return best if sims[best] >= threshold else None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 3: Milvus DB 헬퍼 함수 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "def get_milvus_connection():\n",
    "    alias = \"default\"\n",
    "    if not connections.has_connection(alias):\n",
    "        connections.connect(alias, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "    return connections.get_connection(alias)\n",
    "\n",
    "\n",
    "def create_milvus_collection(name: str, desc: str):\n",
    "    if utility.has_collection(name):\n",
    "        return Collection(name)\n",
    "    fields = [\n",
    "        FieldSchema(\"id\", DataType.VARCHAR, is_primary=True, max_length=256),\n",
    "        FieldSchema(\"embedding\", DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),\n",
    "        FieldSchema(\"text\", DataType.VARCHAR, max_length=65535),\n",
    "        FieldSchema(\"user_id\", DataType.VARCHAR, max_length=256),\n",
    "        FieldSchema(\"type\", DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(\"created_at\", DataType.INT64),\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, desc)\n",
    "    coll = Collection(name, schema)\n",
    "    coll.create_index(\n",
    "        \"embedding\",\n",
    "        {\"index_type\": \"IVF_PQ\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128, \"m\": 8}},\n",
    "    )\n",
    "    return coll\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 4: 장기 기억(RAG) 업데이트 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "def update_long_term_memory(session_id: str):\n",
    "    history = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    if not history.messages:\n",
    "        return\n",
    "    conv_all = \"\\n\".join(f\"{m.type}: {m.content}\" for m in history.messages)\n",
    "    summary_prompt = PromptTemplate.from_template(\n",
    "        \"다음 대화에서 인사말 등 불필요한 잡담을 모두 제거하고, \"\n",
    "        \"사용자 프로필에 유의미한 핵심 정보만 요약해라.\\n{conversation}\"\n",
    "    )\n",
    "    summary_text = LLMChain(llm=llm, prompt=summary_prompt).run(conversation=conv_all)\n",
    "    old_prof = json.dumps(PROFILE_DB.get(session_id, {}), ensure_ascii=False)\n",
    "    profile_update_tpl = PromptTemplate.from_template(\n",
    "        \"[기존 프로필]\\n{old}\\n[요약된 최신 대화]\\n{sum}\\n\"\n",
    "        \"위 내용을 반영하여 사용자 개인회를 위한 JSON 프로필로 반환해줘.\"\n",
    "    )\n",
    "    new_prof_str = LLMChain(llm=llm, prompt=profile_update_tpl).run(\n",
    "        old=old_prof, sum=summary_text\n",
    "    )\n",
    "    try:\n",
    "        new_prof = json.loads(new_prof_str)\n",
    "        PROFILE_DB[session_id] = new_prof\n",
    "    except json.JSONDecodeError:\n",
    "        return\n",
    "    get_milvus_connection()\n",
    "    prof_coll = create_milvus_collection(PROFILE_COLLECTION_NAME, \"User Profiles\")\n",
    "    log_coll = create_milvus_collection(LOG_COLLECTION_NAME, \"Conversation Logs\")\n",
    "    prof_emb = embeddings.embed_query(json.dumps(new_prof, ensure_ascii=False))\n",
    "    prof_coll.upsert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": session_id,\n",
    "                \"embedding\": prof_emb,\n",
    "                \"text\": json.dumps(new_prof, ensure_ascii=False),\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"profile\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    log_emb = embeddings.embed_query(summary_text)\n",
    "    log_coll.insert(\n",
    "        [\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"embedding\": log_emb,\n",
    "                \"text\": summary_text,\n",
    "                \"user_id\": session_id,\n",
    "                \"type\": \"log\",\n",
    "                \"created_at\": int(os.times().user),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 5: RAG 검색 및 단기 기억 설정 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_from_rag(session_id: str, query: str, top_k: int = 2) -> str:\n",
    "    try:\n",
    "        get_milvus_connection()\n",
    "        prof_coll = Collection(PROFILE_COLLECTION_NAME)\n",
    "        log_coll = Collection(LOG_COLLECTION_NAME)\n",
    "        prof_coll.load()\n",
    "        log_coll.load()\n",
    "        query_emb = embeddings.embed_query(query)\n",
    "        params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "        prof_res = prof_coll.search(\n",
    "            [query_emb], \"embedding\", params, limit=1, expr=f\"user_id == '{session_id}'\"\n",
    "        )\n",
    "        log_res = log_coll.search(\n",
    "            [query_emb],\n",
    "            \"embedding\",\n",
    "            params,\n",
    "            limit=top_k,\n",
    "            expr=f\"user_id == '{session_id}'\",\n",
    "        )\n",
    "        context = \"\"\n",
    "        if prof_res and prof_res[0]:\n",
    "            context += f\"[RAG 프로필]\\n{prof_res[0][0].entity.get('text')}\\n\"\n",
    "        if log_res and log_res[0]:\n",
    "            for hit in log_res[0]:\n",
    "                context += f\"[RAG 로그]\\n{hit.entity.get('text')}\\n\"\n",
    "        return context or \"RAG 결과 없음\"\n",
    "    except:\n",
    "        return \"RAG 에러\"\n",
    "\n",
    "\n",
    "def get_short_term_memory(session_id: str) -> ConversationSummaryBufferMemory:\n",
    "    redis_hist = RedisChatMessageHistory(session_id=session_id, url=REDIS_URL)\n",
    "    return ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        chat_memory=redis_hist,\n",
    "        max_token_limit=3000,\n",
    "        return_messages=True,\n",
    "        memory_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 7: Naver Search Chain 구현 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "def naver_search(query: str, display: int = 5) -> dict:\n",
    "    url = \"https://openapi.naver.com/v1/search/local.json\"\n",
    "    headers = {\"X-Naver-Client-Id\": CLIENT_ID, \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "    params = {\"query\": query, \"display\": display}\n",
    "    res = requests.get(url, headers=headers, params=params, timeout=5)\n",
    "    return res.json() if res.status_code == 200 else {}\n",
    "\n",
    "\n",
    "def search_web(query: str) -> str:\n",
    "    data = naver_search(query)\n",
    "    items = data.get(\"items\", [])\n",
    "    if not items:\n",
    "        return \"검색 결과가 없습니다.\"\n",
    "    snippets = []\n",
    "    for item in items:\n",
    "        title = item.get(\"title\", \"\").replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n",
    "        address = item.get(\"roadAddress\", item.get(\"address\", \"\"))\n",
    "        snippets.append(f\"{title} — {address}\")\n",
    "    return \"\\n\".join(snippets[:5])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 8: Conversation Chain 구현 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "async def conversation_chain(\n",
    "    session_id: str, user_input: str, stm: ConversationSummaryBufferMemory\n",
    ") -> str:\n",
    "    hist = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "    prompt = (\n",
    "        \"너는 소통 전문가다. 사용자의 감정과 상황에 기반하여 질문에 답변하라.\\n\"\n",
    "        f\"[대화 히스토리]\\n{hist}\\n\"\n",
    "        f\"[최신 입력]\\n{user_input}\"\n",
    "    )\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 9: Function Calling 기반 Router 구현\n",
    "# ----------------------------------------------------------------------\n",
    "ROUTER_FUNCTION = {\n",
    "    \"name\": \"route_tools\",\n",
    "    \"description\": \"Decide which experts (RAG, WebSearch, Conversation) to invoke\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"tools\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"RAG\", \"WebSearch\", \"Conversation\"],\n",
    "                },\n",
    "                \"description\": \"List of tools to apply\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"tools\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "async def call_router(session_id: str, user_input: str) -> list[str]:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"너는 비서실장(라우터)이다. 아래 세 전문가 중 어떤 전문가가 필요할지 결정하라:\\n\"\n",
    "                \"1) 기억 전문가 (RAG)\\n\"\n",
    "                \"2) 정보 분석가 (WebSearch)\\n\"\n",
    "                \"3) 소통 전문가 (Conversation)\\n\"\n",
    "                \"반드시 함수 호출 형식으로 응답하라.\"\n",
    "            ),\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        functions=[ROUTER_FUNCTION],\n",
    "        function_call={\"name\": \"route_tools\"},\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    msg = resp.choices[0].message\n",
    "    if msg.get(\"function_call\"):\n",
    "        args = json.loads(msg.function_call.arguments)\n",
    "        return args.get(\"tools\", [])\n",
    "    # fallback simple parse\n",
    "    text = msg.content or \"\"\n",
    "    teams = []\n",
    "    if \"RAG\" in text:\n",
    "        teams.append(\"RAG\")\n",
    "    if \"WebSearch\" in text or \"검색\" in text:\n",
    "        teams.append(\"WebSearch\")\n",
    "    if \"Conversation\" in text or \"반응\" in text:\n",
    "        teams.append(\"Conversation\")\n",
    "    return teams or [\"Conversation\"]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 10: Main LLM 최종 응답 템플릿 (이전과 동일)\n",
    "# ----------------------------------------------------------------------\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"rag_ctx\", \"web_ctx\", \"conv_ctx\", \"question\"],\n",
    "    template=(\n",
    "        \"너는 전문적이면서도 친근한 개인 비서 AI이다.\\n\\n\"\n",
    "        \"[RAG 결과]\\n{rag_ctx}\\n\\n\"\n",
    "        \"[Web 검색 결과]\\n{web_ctx}\\n\\n\"\n",
    "        \"[소통 체인 결과]\\n{conv_ctx}\\n\\n\"\n",
    "        \"사용자 질문: {question}\\n\"\n",
    "        \"→ 위 모든 정보를 참고하여 완전한 답변을 제공하라.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 셀 11: FastAPI + WebSocket 서버\n",
    "# ----------------------------------------------------------------------\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def background_rag_update(session_id: str):\n",
    "    await asyncio.to_thread(update_long_term_memory, session_id)\n",
    "\n",
    "\n",
    "async def main_response(\n",
    "    session_id: str,\n",
    "    user_input: str,\n",
    "    websocket: WebSocket,\n",
    "    rag_ctx: str,\n",
    "    web_ctx: str,\n",
    "    conv_ctx: str,\n",
    ") -> str:\n",
    "    prompt = FINAL_PROMPT.format(\n",
    "        rag_ctx=rag_ctx, web_ctx=web_ctx, conv_ctx=conv_ctx, question=user_input\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"개인 비서 AI이며, 아래 지침에 따라 답하라.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    resp = await openai.ChatCompletion.acreate(\n",
    "        model=LLM_MODEL, messages=messages, stream=True, temperature=0.7\n",
    "    )\n",
    "\n",
    "    full_answer = \"\"\n",
    "    async for chunk in resp:\n",
    "        token = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "        if token:\n",
    "            full_answer += token\n",
    "            await websocket.send_text(token)\n",
    "\n",
    "    return full_answer\n",
    "\n",
    "\n",
    "@app.websocket(\"/ws/{session_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, session_id: str):\n",
    "    await websocket.accept()\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = await websocket.receive_text()\n",
    "            stm = get_short_term_memory(session_id)\n",
    "            hist = \"\\n\".join(f\"{m.type}: {m.content}\" for m in stm.chat_memory.messages)\n",
    "            if len(enc.encode(hist)) >= 3000:\n",
    "                asyncio.create_task(background_rag_update(session_id))\n",
    "\n",
    "            # 1차 관문: Embedding 기반 간단 Router\n",
    "            intent = embedding_router(user_input)\n",
    "            if intent == \"conv\":\n",
    "                # simple conversation만 수행\n",
    "                answer = await conversation_chain(session_id, user_input, stm)\n",
    "                await websocket.send_text(answer)\n",
    "                stm.save_context({\"input\": user_input}, {\"output\": answer})\n",
    "                continue\n",
    "\n",
    "            # 2차 관문: Function Calling LLM 기반 Router\n",
    "            teams = await call_router(session_id, user_input)\n",
    "\n",
    "            # 전문가 팀 병렬 실행\n",
    "            tasks = {}\n",
    "            if \"RAG\" in teams:\n",
    "                tasks[\"rag\"] = asyncio.to_thread(\n",
    "                    retrieve_from_rag, session_id, user_input\n",
    "                )\n",
    "            if \"WebSearch\" in teams:\n",
    "                tasks[\"web\"] = asyncio.to_thread(search_web, user_input)\n",
    "            if \"Conversation\" in teams:\n",
    "                tasks[\"conv\"] = asyncio.create_task(\n",
    "                    conversation_chain(session_id, user_input, stm)\n",
    "                )\n",
    "\n",
    "            results = await asyncio.gather(*tasks.values())\n",
    "            rag_ctx = results[list(tasks).index(\"rag\")] if \"rag\" in tasks else \"\"\n",
    "            web_ctx = results[list(tasks).index(\"web\")] if \"web\" in tasks else \"\"\n",
    "            conv_ctx = results[list(tasks).index(\"conv\")] if \"conv\" in tasks else \"\"\n",
    "\n",
    "            # 최종 메인 LLM 스트리밍 응답 및 full_answer 수집\n",
    "            full_answer = await main_response(\n",
    "                session_id, user_input, websocket, rag_ctx, web_ctx, conv_ctx\n",
    "            )\n",
    "\n",
    "            # 대화 저장\n",
    "            stm.save_context({\"input\": user_input}, {\"output\": full_answer})\n",
    "\n",
    "    except WebSocketDisconnect:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 실행:\n",
    "# uvicorn app:app --host 0.0.0.0 --port 8000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secretary_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
